<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>86c57daf3b6a4429993d599c94ebac41</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<section id="algorithms-for-big-data---exercise-7" class="cell markdown" id="ilthBvnZCQto">
<h1>Algorithms for Big Data - Exercise 7</h1>
<p>This lecture is focused on the more advanced examples of the RNN usage for text data anylysis.</p>
<p>We will deal with the sentiment analysis task using Twitter data.</p>
<p>You can download the dataset from <a href="https://github.com/MohamedAfham/Twitter-Sentiment-Analysis-Supervised-Learning/tree/master/Data">this link</a></p>
</section>
<div class="cell markdown" id="Fi2Jwhs35Itq">
<p><a href="https://colab.research.google.com/github/rasvob/2020-21-ARD/blob/master/abd_07.ipynb">Open in Google colab</a> <a href="https://github.com/rasvob/2020-21-ARD/blob/master/abd_07.ipynb">Download from Github</a></p>
</div>
<div class="cell code" data-execution_count="1" data-colab="{&quot;height&quot;:35,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="0UJfkDzeOR3n" data-outputId="6f1094ae-fe35-479e-fecd-574cbfb576d9">
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> __future__ <span class="im">import</span> absolute_import</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> __future__ <span class="im">import</span> division</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> __future__ <span class="im">import</span> print_function</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt <span class="co"># plotting</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.image <span class="im">as</span> mpimg <span class="co"># images</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np <span class="co">#numpy</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow.compat.v2 <span class="im">as</span> tf <span class="co">#use tensorflow v2 as a main </span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow.keras <span class="im">as</span> keras <span class="co"># required for high level applications</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split <span class="co"># split for validation sets</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, f1_score, confusion_matrix, classification_report</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> normalize <span class="co"># normalization of the matrix</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> scipy</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>tf.version.VERSION</span></code></pre></div>
<div class="output execute_result" data-execution_count="1">
<div class="sourceCode" id="cb2"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;type&quot;</span><span class="fu">:</span><span class="st">&quot;string&quot;</span><span class="fu">}</span></span></code></pre></div>
</div>
</div>
<div class="cell code" data-execution_count="2" id="4vhRv4wJOR3p">
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> unicodedata, re, string</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> nltk</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> textblob <span class="im">import</span> TextBlob</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="3" id="XK36kwNIOR3p">
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> show_history(history):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    plt.figure()</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> key <span class="kw">in</span> history.history.keys():</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        plt.plot(history.epoch, history.history[key], label<span class="op">=</span>key)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="4" id="XYLlBbfEOR3q">
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Mish(keras.layers.Activation):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&#39;&#39;&#39;</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Mish Activation Function.</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">    .. math::</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">        mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + e^{x}))</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Shape:</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">        - Input: Arbitrary. Use the keyword argument `input_shape`</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">        (tuple of integers, does not include the samples axis)</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">        when using this layer as the first layer in a model.</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co">        - Output: Same shape as the input.</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Examples:</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co">        &gt;&gt;&gt; X = Activation(&#39;Mish&#39;, name=&quot;conv1_act&quot;)(X_input)</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="co">    &#39;&#39;&#39;</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, activation, <span class="op">**</span>kwargs):</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Mish, <span class="va">self</span>).<span class="fu">__init__</span>(activation, <span class="op">**</span>kwargs)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="va">__name__</span> <span class="op">=</span> <span class="st">&#39;Mish&#39;</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mish(inputs):</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> inputs <span class="op">*</span> tf.math.tanh(tf.math.softplus(inputs))</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>keras.utils.get_custom_objects().update({<span class="st">&#39;mish&#39;</span>: Mish(mish)})</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="5" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="SHnF-PpWOR3r" data-outputId="1bc84349-c80d-4874-84f4-b893cb0c8a8a">
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>nltk.download(<span class="st">&#39;punkt&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>[nltk_data] Downloading package punkt to /root/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
</code></pre>
</div>
<div class="output execute_result" data-execution_count="5">
<pre><code>True</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="6" id="ihYpu4rrOR3s">
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&#39;https://raw.githubusercontent.com/rasvob/2020-21-ARD/master/datasets/train_tweets.csv&#39;</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="7" data-colab="{&quot;height&quot;:206,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="WRT4pex4OR3t" data-outputId="37362202-6d98-4546-f5e7-8386cee2c34d">
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>df.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="7">

  <div id="df-30390cfb-b759-4f8b-9a1a-3fb8c519316d">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>label</th>
      <th>tweet</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>@user when a father is dysfunctional and is s...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0</td>
      <td>@user @user thanks for #lyft credit i can't us...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0</td>
      <td>bihday your majesty</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>0</td>
      <td>#model   i love u take with u all the time in ...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>factsguide: society now    #motivation</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-30390cfb-b759-4f8b-9a1a-3fb8c519316d')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-30390cfb-b759-4f8b-9a1a-3fb8c519316d button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-30390cfb-b759-4f8b-9a1a-3fb8c519316d');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<section id="lets-take-a-look-at-the-data" class="cell markdown" id="B4J0-vTcOR3t">
<h1>Let's take a look at the data</h1>
</section>
<div class="cell code" data-execution_count="8" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="RjI3WiOUOR3u" data-outputId="5f33dcf0-33b2-4c9f-bc0b-32b3b68c3fdc">
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>df.shape</span></code></pre></div>
<div class="output execute_result" data-execution_count="8">
<pre><code>(31962, 3)</code></pre>
</div>
</div>
<section id="we-can-see-that-the-classification-task-is-highly-imbalanced-because-we-have-only-2242-negative-tweets-compared-with-positive-one" class="cell markdown" id="JxK9Bx9WOR3u">
<h2>We can see that the classification task is highly imbalanced, because we have only 2242 negative tweets compared with positive one</h2>
</section>
<div class="cell code" data-execution_count="9" data-colab="{&quot;height&quot;:296,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="AI1vLSriOR3u" data-outputId="a50e88ee-9204-40e1-e76c-8cc01faf3eae">
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>sns.countplot(x<span class="op">=</span><span class="st">&#39;label&#39;</span>, data<span class="op">=</span>df)</span></code></pre></div>
<div class="output execute_result" data-execution_count="9">
<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2256df30d0&gt;</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_5d270abade7b464aacf99cad3478e1a6/0679e717018f2d84cecd99790e35c86b2da6a8bd.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="10" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="OjV43gUOOR3v" data-outputId="0f20e714-2822-493d-bd75-cc7e93e83821">
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>df.label.value_counts()</span></code></pre></div>
<div class="output execute_result" data-execution_count="10">
<pre><code>0    29720
1     2242
Name: label, dtype: int64</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="11" id="agE3Py9dOR3v">
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;length&#39;</span>] <span class="op">=</span> df.tweet.<span class="bu">apply</span>(<span class="bu">len</span>)</span></code></pre></div>
</div>
<section id="we-can-see-that-the-sentences-are-of-similar-lengths" class="cell markdown" id="uE0_dydVOR3v">
<h3>We can see that the sentences are of similar lengths</h3>
</section>
<div class="cell code" data-execution_count="12" data-colab="{&quot;height&quot;:296,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="jzzngN9pOR3w" data-outputId="3aeac1f7-d5a9-43ce-e4b4-961d195db7c5">
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>sns.barplot(x<span class="op">=</span><span class="st">&#39;label&#39;</span>, y<span class="op">=</span><span class="st">&#39;length&#39;</span>, data <span class="op">=</span> df)</span></code></pre></div>
<div class="output execute_result" data-execution_count="12">
<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2256d56dd0&gt;</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_5d270abade7b464aacf99cad3478e1a6/f52b6ff9b06997a1287bcb59da8cc66fe88d5097.png" /></p>
</div>
</div>
<section id="we-can-see-that-the-text-data-are-full-of-noise" class="cell markdown" id="EXz5o-vCOR3w">
<h1>We can see that the text data are full of noise</h1>
<ul>
<li>Social posts suffer the most from this effect</li>
<li>The text is full of hashtags, emojis, @mentions and so on</li>
<li>These parts usually don't influence the sentiment score by much</li>
<li>Although most advanced models usually extract even this features because e.g. emojis can help you with the sarcasm understanding</li>
</ul>
</section>
<div class="cell code" data-execution_count="14" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="hSD9iX2sOR3w" data-outputId="7d52271e-605b-4264-cfac-a83af3e88ee1">
<div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x <span class="kw">in</span> df.loc[:<span class="dv">10</span>, <span class="st">&#39;tweet&#39;</span>]:</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(x)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&#39;---------&#39;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code> @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run
---------
@user @user thanks for #lyft credit i can&#39;t use cause they don&#39;t offer wheelchair vans in pdx.    #disapointed #getthanked
---------
  bihday your majesty
---------
#model   i love u take with u all the time in urð±!!! ððððð¦ð¦ð¦  
---------
 factsguide: society now    #motivation
---------
[2/2] huge fan fare and big talking before they leave. chaos and pay disputes when they get there. #allshowandnogo  
---------
 @user camping tomorrow @user @user @user @user @user @user @user dannyâ¦
---------
the next school year is the year for exams.ð¯ can&#39;t think about that ð­ #school #exams   #hate #imagine #actorslife #revolutionschool #girl
---------
we won!!! love the land!!! #allin #cavs #champions #cleveland #clevelandcavaliers  â¦ 
---------
 @user @user welcome here !  i&#39;m   it&#39;s so #gr8 ! 
---------
 â #ireland consumer price index (mom) climbed from previous 0.2% to 0.5% in may   #blog #silver #gold #forex
---------
</code></pre>
</div>
</div>
<div class="cell markdown" id="Wg4Lzh0BOR3w">
<h2 id="stemming">Stemming</h2>
<p>Stemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. A stemming algorithm reduces the words “chocolates”, “chocolatey”, “choco” to the root word, “chocolate” and “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve”,</p>
<h2 id="lemmatization">Lemmatization</h2>
<p>Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.</p>
<p>Examples of lemmatization:</p>
<ul>
<li>rocks : rock</li>
<li>corpora : corpus</li>
<li>better : good</li>
</ul>
<h2 id="both-techiques-can-be-used-in-the-preprocessing-pipeline">Both techiques can be used in the preprocessing pipeline</h2>
<p>You have to decide if it is beneficial to you, because this steps leads to some generalization of the data by itself. You will definitely lose some pieces of the information. If you use some form of embedding like Word2Vec or Glove, it is better to skip this steps because the embedding vocabulary skipped it as well.</p>
</div>
<div class="cell code" data-execution_count="15" id="vO3ZZv2wOR3x">
<div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> remove_non_ascii(words):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Remove non-ASCII characters from list of tokenized words&quot;&quot;&quot;</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    new_words <span class="op">=</span> []</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> words:</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>        new_word <span class="op">=</span> unicodedata.normalize(<span class="st">&#39;NFKD&#39;</span>, word).encode(<span class="st">&#39;ascii&#39;</span>, <span class="st">&#39;ignore&#39;</span>).decode(<span class="st">&#39;utf-8&#39;</span>, <span class="st">&#39;ignore&#39;</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>        new_words.append(new_word)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_words</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> to_lowercase(words):</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Convert all characters to lowercase from list of tokenized words&quot;&quot;&quot;</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    new_words <span class="op">=</span> []</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> words:</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        new_word <span class="op">=</span> word.lower()</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>        new_words.append(new_word)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_words</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> remove_punctuation(words):</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Remove punctuation from list of tokenized words&quot;&quot;&quot;</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    new_words <span class="op">=</span> []</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> words:</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>        new_word <span class="op">=</span> re.sub(<span class="vs">r&#39;[^\w\s]&#39;</span>, <span class="st">&#39;&#39;</span>, word)</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> new_word <span class="op">!=</span> <span class="st">&#39;&#39;</span>:</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>            new_words.append(new_word)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_words</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> remove_numbers(words):</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Remove all interger occurrences in list of tokenized words with textual representation&quot;&quot;&quot;</span></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>    new_words <span class="op">=</span> []</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> words:</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>        new_word <span class="op">=</span> re.sub(<span class="st">&quot;\d+&quot;</span>, <span class="st">&quot;&quot;</span>, word)</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> new_word <span class="op">!=</span> <span class="st">&#39;&#39;</span>:</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>            new_words.append(new_word)</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_words</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> remove_stopwords(words):</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Remove stop words from list of tokenized words&quot;&quot;&quot;</span></span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>    new_words <span class="op">=</span> []</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> words:</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> stopwords.words(<span class="st">&#39;english&#39;</span>):</span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>            new_words.append(word)</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_words</span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stem_words(words):</span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Stem words in list of tokenized words&quot;&quot;&quot;</span></span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a>    stemmer <span class="op">=</span> LancasterStemmer()</span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a>    stems <span class="op">=</span> []</span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> words:</span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a>        stem <span class="op">=</span> stemmer.stem(word)</span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a>        stems.append(stem)</span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> stems</span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> lemmatize_verbs(words):</span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Lemmatize verbs in list of tokenized words&quot;&quot;&quot;</span></span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a>    lemmatizer <span class="op">=</span> WordNetLemmatizer()</span>
<span id="cb22-55"><a href="#cb22-55" aria-hidden="true" tabindex="-1"></a>    lemmas <span class="op">=</span> []</span>
<span id="cb22-56"><a href="#cb22-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> word <span class="kw">in</span> words:</span>
<span id="cb22-57"><a href="#cb22-57" aria-hidden="true" tabindex="-1"></a>        lemma <span class="op">=</span> lemmatizer.lemmatize(word, pos<span class="op">=</span><span class="st">&#39;v&#39;</span>)</span>
<span id="cb22-58"><a href="#cb22-58" aria-hidden="true" tabindex="-1"></a>        lemmas.append(lemma)</span>
<span id="cb22-59"><a href="#cb22-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lemmas</span>
<span id="cb22-60"><a href="#cb22-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-61"><a href="#cb22-61" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> normalize(words):</span>
<span id="cb22-62"><a href="#cb22-62" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> remove_non_ascii(words)</span>
<span id="cb22-63"><a href="#cb22-63" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> to_lowercase(words)</span>
<span id="cb22-64"><a href="#cb22-64" aria-hidden="true" tabindex="-1"></a><span class="co"># words = remove_punctuation(words)</span></span>
<span id="cb22-65"><a href="#cb22-65" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> remove_numbers(words)</span>
<span id="cb22-66"><a href="#cb22-66" aria-hidden="true" tabindex="-1"></a><span class="co">#    words = remove_stopwords(words)</span></span>
<span id="cb22-67"><a href="#cb22-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> words</span>
<span id="cb22-68"><a href="#cb22-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-69"><a href="#cb22-69" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> form_sentence(tweet):</span>
<span id="cb22-70"><a href="#cb22-70" aria-hidden="true" tabindex="-1"></a>    tweet_blob <span class="op">=</span> TextBlob(tweet)</span>
<span id="cb22-71"><a href="#cb22-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tweet_blob.words</span></code></pre></div>
</div>
<section id="tokenize-sentences-and-remove-puncuation-by-textblob-library" class="cell markdown" id="wlkBWq0gOR3y">
<h1>Tokenize sentences and remove puncuation by TextBlob library</h1>
</section>
<div class="cell code" data-execution_count="16" id="7NHD13mNOR3y">
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;Words&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;tweet&#39;</span>].<span class="bu">apply</span>(form_sentence)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="17" data-colab="{&quot;height&quot;:206,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="WBevn9keOR3y" data-outputId="2f917b22-ae0c-479b-c121-21ec4af7c8ff">
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>df.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="17">

  <div id="df-c271463b-efdc-4d4f-9b6e-204c1afdd2fd">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>label</th>
      <th>tweet</th>
      <th>length</th>
      <th>Words</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>@user when a father is dysfunctional and is s...</td>
      <td>102</td>
      <td>[user, when, a, father, is, dysfunctional, and...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0</td>
      <td>@user @user thanks for #lyft credit i can't us...</td>
      <td>122</td>
      <td>[user, user, thanks, for, lyft, credit, i, ca,...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0</td>
      <td>bihday your majesty</td>
      <td>21</td>
      <td>[bihday, your, majesty]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>0</td>
      <td>#model   i love u take with u all the time in ...</td>
      <td>86</td>
      <td>[model, i, love, u, take, with, u, all, the, t...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>factsguide: society now    #motivation</td>
      <td>39</td>
      <td>[factsguide, society, now, motivation]</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-c271463b-efdc-4d4f-9b6e-204c1afdd2fd')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-c271463b-efdc-4d4f-9b6e-204c1afdd2fd button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-c271463b-efdc-4d4f-9b6e-204c1afdd2fd');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<section id="normalize-sentences" class="cell markdown" id="d5kEeoqoOR3z">
<h1>Normalize sentences</h1>
<ul>
<li>We want only ascii, lowercase and no numbers</li>
</ul>
<h2 id="you-can-experiments-with-different-preprocess-steps">You can experiments with different preprocess steps!</h2>
</section>
<div class="cell code" data-execution_count="18" id="4zRfjiT2OR3z">
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;Words_normalized&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;Words&#39;</span>].<span class="bu">apply</span>(normalize)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="19" data-colab="{&quot;height&quot;:206,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="uCt14iC1OR3z" data-outputId="3041a768-02b1-431a-c06b-de676dc6f688">
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>df.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="19">

  <div id="df-c8b783d3-640e-438b-8510-994dba47db7e">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>label</th>
      <th>tweet</th>
      <th>length</th>
      <th>Words</th>
      <th>Words_normalized</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>@user when a father is dysfunctional and is s...</td>
      <td>102</td>
      <td>[user, when, a, father, is, dysfunctional, and...</td>
      <td>[user, when, a, father, is, dysfunctional, and...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0</td>
      <td>@user @user thanks for #lyft credit i can't us...</td>
      <td>122</td>
      <td>[user, user, thanks, for, lyft, credit, i, ca,...</td>
      <td>[user, user, thanks, for, lyft, credit, i, ca,...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0</td>
      <td>bihday your majesty</td>
      <td>21</td>
      <td>[bihday, your, majesty]</td>
      <td>[bihday, your, majesty]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>0</td>
      <td>#model   i love u take with u all the time in ...</td>
      <td>86</td>
      <td>[model, i, love, u, take, with, u, all, the, t...</td>
      <td>[model, i, love, u, take, with, u, all, the, t...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>factsguide: society now    #motivation</td>
      <td>39</td>
      <td>[factsguide, society, now, motivation]</td>
      <td>[factsguide, society, now, motivation]</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-c8b783d3-640e-438b-8510-994dba47db7e')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-c8b783d3-640e-438b-8510-994dba47db7e button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-c8b783d3-640e-438b-8510-994dba47db7e');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<section id="remove-the-user-word-from-tweets" class="cell markdown" id="YGdnpcKnOR3z">
<h2>Remove the 'user' word from tweets</h2>
</section>
<div class="cell code" data-execution_count="20" id="JDuHVb0GOR3z">
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;Words_normalized_no_user&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;Words_normalized&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: [y <span class="cf">for</span> y <span class="kw">in</span> x <span class="cf">if</span> <span class="st">&#39;user&#39;</span> <span class="kw">not</span> <span class="kw">in</span> y])</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="21" data-colab="{&quot;height&quot;:206,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="qyCvrrBOOR3z" data-outputId="d1e0e301-4b1a-4a7c-909b-9e4873b420c3">
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>df.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="21">

  <div id="df-9a68c3f9-83f5-48f6-90ee-4da188b2f09d">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>label</th>
      <th>tweet</th>
      <th>length</th>
      <th>Words</th>
      <th>Words_normalized</th>
      <th>Words_normalized_no_user</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>@user when a father is dysfunctional and is s...</td>
      <td>102</td>
      <td>[user, when, a, father, is, dysfunctional, and...</td>
      <td>[user, when, a, father, is, dysfunctional, and...</td>
      <td>[when, a, father, is, dysfunctional, and, is, ...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0</td>
      <td>@user @user thanks for #lyft credit i can't us...</td>
      <td>122</td>
      <td>[user, user, thanks, for, lyft, credit, i, ca,...</td>
      <td>[user, user, thanks, for, lyft, credit, i, ca,...</td>
      <td>[thanks, for, lyft, credit, i, ca, n't, use, c...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0</td>
      <td>bihday your majesty</td>
      <td>21</td>
      <td>[bihday, your, majesty]</td>
      <td>[bihday, your, majesty]</td>
      <td>[bihday, your, majesty]</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>0</td>
      <td>#model   i love u take with u all the time in ...</td>
      <td>86</td>
      <td>[model, i, love, u, take, with, u, all, the, t...</td>
      <td>[model, i, love, u, take, with, u, all, the, t...</td>
      <td>[model, i, love, u, take, with, u, all, the, t...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>factsguide: society now    #motivation</td>
      <td>39</td>
      <td>[factsguide, society, now, motivation]</td>
      <td>[factsguide, society, now, motivation]</td>
      <td>[factsguide, society, now, motivation]</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-9a68c3f9-83f5-48f6-90ee-4da188b2f09d')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-9a68c3f9-83f5-48f6-90ee-4da188b2f09d button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-9a68c3f9-83f5-48f6-90ee-4da188b2f09d');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<section id="we-can-see-that-no-pre-processing-is-ideal-and-we-have-to-fix-some-issues-by-ourselves" class="cell markdown" id="Ks8TfCmSOR30">
<h2>We can see that no pre-processing is ideal and we have to fix some issues by ourselves</h2>
<ul>
<li>e.g. n't splitting</li>
</ul>
</section>
<div class="cell code" data-execution_count="22" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="M3t1OmZMOR30" data-outputId="29241801-1b57-4910-ea07-3896709214e6">
<div class="sourceCode" id="cb29"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.tweet.iloc[<span class="dv">1</span>])</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.Words_normalized_no_user.iloc[<span class="dv">1</span>])</span></code></pre></div>
<div class="output stream stdout">
<pre><code>@user @user thanks for #lyft credit i can&#39;t use cause they don&#39;t offer wheelchair vans in pdx.    #disapointed #getthanked
[&#39;thanks&#39;, &#39;for&#39;, &#39;lyft&#39;, &#39;credit&#39;, &#39;i&#39;, &#39;ca&#39;, &quot;n&#39;t&quot;, &#39;use&#39;, &#39;cause&#39;, &#39;they&#39;, &#39;do&#39;, &quot;n&#39;t&quot;, &#39;offer&#39;, &#39;wheelchair&#39;, &#39;vans&#39;, &#39;in&#39;, &#39;pdx&#39;, &#39;disapointed&#39;, &#39;getthanked&#39;]
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="23" id="gn8VZeWmOR30">
<div class="sourceCode" id="cb31"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fix_nt(words):</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    st_res <span class="op">=</span> []</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(words) <span class="op">-</span> <span class="dv">1</span>):</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> words[i<span class="op">+</span><span class="dv">1</span>] <span class="op">==</span> <span class="st">&quot;n&#39;t&quot;</span> <span class="kw">or</span> words[i<span class="op">+</span><span class="dv">1</span>] <span class="op">==</span> <span class="st">&quot;nt&quot;</span>:</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>            st_res.append(words[i]<span class="op">+</span>(<span class="st">&quot;n&#39;t&quot;</span>))</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> words[i] <span class="op">!=</span> <span class="st">&quot;n&#39;t&quot;</span> <span class="kw">and</span> words[i] <span class="op">!=</span> <span class="st">&quot;nt&quot;</span>:</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>                st_res.append(words[i])</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> st_res</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="24" id="XynALVGVOR30">
<div class="sourceCode" id="cb32"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;Words_normalized_no_user_fixed&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;Words_normalized_no_user&#39;</span>].<span class="bu">apply</span>(fix_nt)</span></code></pre></div>
</div>
<section id="the-issue-is-now-fixed" class="cell markdown" id="_BnWq5eOOR30">
<h2>The issue is now fixed</h2>
</section>
<div class="cell code" data-execution_count="25" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="saBdsTyTOR30" data-outputId="289c0ad8-42ad-4cbc-c9de-3443885c90d8">
<div class="sourceCode" id="cb33"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.tweet.iloc[<span class="dv">1</span>])</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.Words_normalized_no_user.iloc[<span class="dv">1</span>])</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.Words_normalized_no_user_fixed.iloc[<span class="dv">1</span>])</span></code></pre></div>
<div class="output stream stdout">
<pre><code>@user @user thanks for #lyft credit i can&#39;t use cause they don&#39;t offer wheelchair vans in pdx.    #disapointed #getthanked
[&#39;thanks&#39;, &#39;for&#39;, &#39;lyft&#39;, &#39;credit&#39;, &#39;i&#39;, &#39;ca&#39;, &quot;n&#39;t&quot;, &#39;use&#39;, &#39;cause&#39;, &#39;they&#39;, &#39;do&#39;, &quot;n&#39;t&quot;, &#39;offer&#39;, &#39;wheelchair&#39;, &#39;vans&#39;, &#39;in&#39;, &#39;pdx&#39;, &#39;disapointed&#39;, &#39;getthanked&#39;]
[&#39;thanks&#39;, &#39;for&#39;, &#39;lyft&#39;, &#39;credit&#39;, &#39;i&#39;, &quot;can&#39;t&quot;, &#39;use&#39;, &#39;cause&#39;, &#39;they&#39;, &quot;don&#39;t&quot;, &#39;offer&#39;, &#39;wheelchair&#39;, &#39;vans&#39;, &#39;in&#39;, &#39;pdx&#39;, &#39;disapointed&#39;]
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="26" id="EbtXTIwVOR31">
<div class="sourceCode" id="cb35"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a>df[<span class="st">&#39;Clean_text&#39;</span>] <span class="op">=</span> df[<span class="st">&#39;Words_normalized_no_user_fixed&#39;</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">&quot; &quot;</span>.join(x))</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="27" data-colab="{&quot;height&quot;:337,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="_5qab517OR31" data-outputId="02336e13-c602-4ad6-f222-821cb7a6ac06">
<div class="sourceCode" id="cb36"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>df.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="27">

  <div id="df-fd8ef68f-6125-4e45-a407-276b4d1e2c91">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>label</th>
      <th>tweet</th>
      <th>length</th>
      <th>Words</th>
      <th>Words_normalized</th>
      <th>Words_normalized_no_user</th>
      <th>Words_normalized_no_user_fixed</th>
      <th>Clean_text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>@user when a father is dysfunctional and is s...</td>
      <td>102</td>
      <td>[user, when, a, father, is, dysfunctional, and...</td>
      <td>[user, when, a, father, is, dysfunctional, and...</td>
      <td>[when, a, father, is, dysfunctional, and, is, ...</td>
      <td>[when, a, father, is, dysfunctional, and, is, ...</td>
      <td>when a father is dysfunctional and is so selfi...</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0</td>
      <td>@user @user thanks for #lyft credit i can't us...</td>
      <td>122</td>
      <td>[user, user, thanks, for, lyft, credit, i, ca,...</td>
      <td>[user, user, thanks, for, lyft, credit, i, ca,...</td>
      <td>[thanks, for, lyft, credit, i, ca, n't, use, c...</td>
      <td>[thanks, for, lyft, credit, i, can't, use, cau...</td>
      <td>thanks for lyft credit i can't use cause they ...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0</td>
      <td>bihday your majesty</td>
      <td>21</td>
      <td>[bihday, your, majesty]</td>
      <td>[bihday, your, majesty]</td>
      <td>[bihday, your, majesty]</td>
      <td>[bihday, your]</td>
      <td>bihday your</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>0</td>
      <td>#model   i love u take with u all the time in ...</td>
      <td>86</td>
      <td>[model, i, love, u, take, with, u, all, the, t...</td>
      <td>[model, i, love, u, take, with, u, all, the, t...</td>
      <td>[model, i, love, u, take, with, u, all, the, t...</td>
      <td>[model, i, love, u, take, with, u, all, the, t...</td>
      <td>model i love u take with u all the time in</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>factsguide: society now    #motivation</td>
      <td>39</td>
      <td>[factsguide, society, now, motivation]</td>
      <td>[factsguide, society, now, motivation]</td>
      <td>[factsguide, society, now, motivation]</td>
      <td>[factsguide, society, now]</td>
      <td>factsguide society now</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-fd8ef68f-6125-4e45-a407-276b4d1e2c91')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-fd8ef68f-6125-4e45-a407-276b4d1e2c91 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-fd8ef68f-6125-4e45-a407-276b4d1e2c91');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<section id="lets-take-a-look-at-the-most-common-words-in-corpus" class="cell markdown" id="vYVa6JgYOR31">
<h1>Let's take a look at the most common words in corpus</h1>
</section>
<div class="cell code" data-execution_count="28" id="Y01Vq6UpOR31">
<div class="sourceCode" id="cb37"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> itertools</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="29" id="yc9QQ9BOOR31">
<div class="sourceCode" id="cb38"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>all_words <span class="op">=</span> <span class="bu">list</span>(itertools.chain(<span class="op">*</span>df.Words_normalized_no_user_fixed))</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="30" id="tMxWNn4BOR31">
<div class="sourceCode" id="cb39"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>dist <span class="op">=</span> nltk.FreqDist(all_words)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="31" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="x9O4aNx6OR32" data-outputId="736d33d2-39a5-43df-ae5c-d7f4dddf23db">
<div class="sourceCode" id="cb40"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>dist</span></code></pre></div>
<div class="output execute_result" data-execution_count="31">
<pre><code>FreqDist({&#39;when&#39;: 1229,
          &#39;a&#39;: 7672,
          &#39;father&#39;: 610,
          &#39;is&#39;: 4045,
          &#39;dysfunctional&#39;: 1,
          &#39;and&#39;: 4865,
          &#39;so&#39;: 1935,
          &#39;selfish&#39;: 12,
          &#39;he&#39;: 627,
          &#39;drags&#39;: 4,
          &#39;his&#39;: 507,
          &#39;kids&#39;: 185,
          &#39;into&#39;: 203,
          &#39;dysfunction&#39;: 2,
          &#39;thanks&#39;: 291,
          &#39;for&#39;: 4461,
          &#39;lyft&#39;: 5,
          &#39;credit&#39;: 13,
          &#39;i&#39;: 7153,
          &quot;can&#39;t&quot;: 742,
          &#39;use&#39;: 104,
          &#39;cause&#39;: 66,
          &#39;they&#39;: 814,
          &quot;don&#39;t&quot;: 645,
          &#39;offer&#39;: 25,
          &#39;wheelchair&#39;: 2,
          &#39;vans&#39;: 2,
          &#39;in&#39;: 4612,
          &#39;pdx&#39;: 3,
          &#39;disapointed&#39;: 1,
          &#39;bihday&#39;: 794,
          &#39;your&#39;: 1694,
          &#39;model&#39;: 373,
          &#39;love&#39;: 2484,
          &#39;u&#39;: 1145,
          &#39;take&#39;: 622,
          &#39;with&#39;: 2485,
          &#39;all&#39;: 1894,
          &#39;the&#39;: 10135,
          &#39;time&#39;: 1077,
          &#39;factsguide&#39;: 12,
          &#39;society&#39;: 22,
          &#39;now&#39;: 899,
          &#39;/&#39;: 69,
          &#39;huge&#39;: 42,
          &#39;fan&#39;: 56,
          &#39;fare&#39;: 4,
          &#39;big&#39;: 193,
          &#39;talking&#39;: 66,
          &#39;before&#39;: 207,
          &#39;leave&#39;: 132,
          &#39;chaos&#39;: 6,
          &#39;pay&#39;: 248,
          &#39;disputes&#39;: 1,
          &#39;get&#39;: 907,
          &#39;there&#39;: 521,
          &#39;camping&#39;: 12,
          &#39;tomorrow&#39;: 376,
          &#39;next&#39;: 336,
          &#39;school&#39;: 178,
          &#39;year&#39;: 299,
          &#39;exams. &#39;: 1,
          &#39;think&#39;: 403,
          &#39;about&#39;: 849,
          &#39;that&#39;: 2028,
          &#39;exams&#39;: 17,
          &#39;hate&#39;: 214,
          &#39;imagine&#39;: 30,
          &#39;actorslife&#39;: 6,
          &#39;revolutionschool&#39;: 1,
          &#39;we&#39;: 1545,
          &#39;won&#39;: 52,
          &#39;land&#39;: 26,
          &#39;allin&#39;: 4,
          &#39;cavs&#39;: 32,
          &#39;champions&#39;: 7,
          &#39;cleveland&#39;: 24,
          &#39;clevelandcavaliers&#39;: 1,
          &#39;welcome&#39;: 61,
          &#39;here&#39;: 478,
          &quot;&#39;m&quot;: 1007,
          &#39;it&#39;: 3433,
          &quot;&#39;s&quot;: 3190,
          &#39;ireland&#39;: 26,
          &#39;consumer&#39;: 13,
          &#39;price&#39;: 28,
          &#39;index&#39;: 20,
          &#39;mom&#39;: 121,
          &#39;climbed&#39;: 2,
          &#39;from&#39;: 1040,
          &#39;previous&#39;: 42,
          &#39;.&#39;: 361,
          &#39;to&#39;: 9798,
          &#39;may&#39;: 213,
          &#39;blog&#39;: 371,
          &#39;silver&#39;: 304,
          &#39;gold&#39;: 315,
          &#39;are&#39;: 1853,
          &#39;orlando&#39;: 334,
          &#39;standwithorlando&#39;: 1,
          &#39;pulseshooting&#39;: 6,
          &#39;orlandoshooting&#39;: 33,
          &#39;biggerproblems&#39;: 1,
          &#39;heabreaking&#39;: 21,
          &#39;values&#39;: 9,
          &#39;see&#39;: 754,
          &#39;my&#39;: 3684,
          &#39;daddy&#39;: 85,
          &#39;today&#39;: 957,
          &#39;days&#39;: 455,
          &#39;cnn&#39;: 22,
          &#39;calls&#39;: 27,
          &#39;michigan&#39;: 3,
          &#39;middle&#39;: 24,
          &quot;&#39;build&quot;: 2,
          &#39;wall&#39;: 23,
          &#39;chant&#39;: 3,
          &#39;no&#39;: 807,
          &#39;comment&#39;: 26,
          &#39;australia&#39;: 32,
          &#39;opkillingbay&#39;: 2,
          &#39;seashepherd&#39;: 5,
          &#39;helpcovedolphins&#39;: 2,
          &#39;thecove&#39;: 4,
          &#39;ouch&#39;: 8,
          &#39;junior&#39;: 8,
          &#39;angry&#39;: 199,
          &#39;got&#39;: 578,
          &#39;yugyoem&#39;: 1,
          &#39;am&#39;: 1183,
          &#39;thankful&#39;: 931,
          &#39;having&#39;: 204,
          &#39;paner&#39;: 23,
          &#39;retweet&#39;: 96,
          &#39;if&#39;: 691,
          &#39;you&#39;: 5677,
          &#39;its&#39;: 252,
          &#39;friday&#39;: 477,
          &#39;smiles&#39;: 90,
          &#39;around&#39;: 187,
          &#39;via&#39;: 143,
          &#39;ig&#39;: 18,
          &#39;cookies&#39;: 14,
          &#39;make&#39;: 542,
          &#39;as&#39;: 931,
          &#39;know&#39;: 417,
          &#39;essential&#39;: 20,
          &#39;oils&#39;: 16,
          &#39;not&#39;: 1269,
          &#39;made&#39;: 228,
          &#39;of&#39;: 4132,
          &#39;euro&#39;: 123,
          &#39;people&#39;: 844,
          &#39;blaming&#39;: 7,
          &#39;ha&#39;: 17,
          &#39;conceded&#39;: 1,
          &#39;goal&#39;: 21,
          &#39;was&#39;: 892,
          &#39;fat&#39;: 27,
          &#39;rooney&#39;: 4,
          &#39;who&#39;: 651,
          &#39;gave&#39;: 49,
          &#39;away&#39;: 155,
          &#39;free&#39;: 258,
          &#39;kick&#39;: 20,
          &#39;knowing&#39;: 29,
          &#39;bale&#39;: 2,
          &#39;can&#39;: 812,
          &#39;hit&#39;: 58,
          &#39;them&#39;: 304,
          &#39;sad&#39;: 395,
          &#39;little&#39;: 282,
          &#39;dude&#39;: 26,
          &#39;badday&#39;: 3,
          &#39;coneofshame&#39;: 1,
          &#39;cats&#39;: 41,
          &#39;pissed&#39;: 12,
          &#39;funny&#39;: 195,
          &#39;product&#39;: 31,
          &#39;day&#39;: 2023,
          &#39;happy&#39;: 1606,
          &#39;man&#39;: 262,
          &#39;wine&#39;: 49,
          &#39;tool&#39;: 27,
          &#39;weekend&#39;: 506,
          &#39;open&#39;: 103,
          &#39;up&#39;: 980,
          &#39;amp&#39;: 1753,
          &#39;drink&#39;: 45,
          &#39;lumpy&#39;: 5,
          &#39;says&#39;: 122,
          &#39;prove&#39;: 9,
          &#39;tgif&#39;: 79,
          &#39;ff&#39;: 61,
          &#39;gamedev&#39;: 25,
          &#39;indiedev&#39;: 24,
          &#39;indiegamedev&#39;: 17,
          &#39;beautiful&#39;: 483,
          &#39;sign&#39;: 39,
          &#39;by&#39;: 773,
          &#39;vendor&#39;: 12,
          &#39;upsideofflorida&#39;: 6,
          &#39;shopalyssas&#39;: 6,
          &#39;media&#39;: 96,
          &#39;pressconference&#39;: 1,
          &#39;antalya&#39;: 2,
          &#39;turkey&#39;: 15,
          &#39;sunday&#39;: 335,
          &#39;throwback&#39;: 23,
          &#39;had&#39;: 354,
          &#39;great&#39;: 494,
          &#39;panel&#39;: 7,
          &#39;on&#39;: 2586,
          &#39;mediatization&#39;: 1,
          &#39;public&#39;: 33,
          &#39;service&#39;: 52,
          &#39;went&#39;: 85,
          &#39;nightclub&#39;: 29,
          &#39;have&#39;: 1627,
          &#39;good&#39;: 844,
          &#39;night&#39;: 346,
          &#39;actions&#39;: 14,
          &#39;means&#39;: 52,
          &#39;those&#39;: 161,
          &#39;lost&#39;: 158,
          &#39;their&#39;: 471,
          &#39;families&#39;: 67,
          &#39;forever&#39;: 82,
          &#39;rip&#39;: 106,
          &#39;never&#39;: 412,
          &#39;chance&#39;: 38,
          &#39;vote&#39;: 52,
          &#39;presidential&#39;: 14,
          &#39;candidate&#39;: 16,
          &#39;excited&#39;: 211,
          &#39;this&#39;: 2606,
          &#39;cycle&#39;: 5,
          &#39;looks&#39;: 128,
          &#39;be&#39;: 2378,
          &#39;alohafriday&#39;: 2,
          &#39;does&#39;: 137,
          &#39;exist&#39;: 14,
          &#39;positivevibes&#39;: 28,
          &#39;fellow&#39;: 16,
          &#39;nohern&#39;: 8,
          &#39;sadley&#39;: 1,
          &#39;passed&#39;: 33,
          &#39;tonight&#39;: 265,
          &#39;gawa&#39;: 5,
          &#39;singing&#39;: 36,
          &#39;cheering&#39;: 6,
          &#39;hard&#39;: 140,
          &#39;monday&#39;: 150,
          &#39;due&#39;: 46,
          &#39;cloudy&#39;: 11,
          &#39;weather&#39;: 44,
          &#39;disabling&#39;: 1,
          &#39;oxygen&#39;: 3,
          &#39;production&#39;: 15,
          &#39;goodnight&#39;: 33,
          &#39;unbelievable&#39;: 12,
          &#39;st&#39;: 115,
          &#39;century&#39;: 8,
          &quot;&#39;d&quot;: 72,
          &#39;need&#39;: 505,
          &#39;something&#39;: 167,
          &#39;like&#39;: 1030,
          &#39;again&#39;: 233,
          &#39;neverump&#39;: 11,
          &#39;taylorswift&#39;: 6,
          &#39;bull&#39;: 504,
          &#39;will&#39;: 1264,
          &#39;dominate&#39;: 175,
          &#39;direct&#39;: 178,
          &#39;whatever&#39;: 227,
          &#39;want&#39;: 639,
          &#39;do&#39;: 1163,
          &#39;morning&#39;: 431,
          &#39;travelingram&#39;: 3,
          &#39;dalat&#39;: 1,
          &#39;once&#39;: 73,
          &#39;more&#39;: 681,
          &#39;only&#39;: 492,
          &#39;one&#39;: 762,
          &#39;word&#39;: 71,
          &#39;tells&#39;: 14,
          &#39;photoshop&#39;: 1,
          &#39;enoughisenough&#39;: 1,
          &#39;dontphotoshopeverything&#39;: 1,
          &#39;oh&#39;: 155,
          &#39;cedarpoint&#39;: 2,
          &#39;waited&#39;: 13,
          &#39;hours&#39;: 115,
          &#39;valravn&#39;: 1,
          &#39;line&#39;: 48,
          &#39;stopped&#39;: 13,
          &#39;working&#39;: 118,
          &#39;were&#39;: 184,
          &#39;sunshine&#39;: 90,
          &#39;finally&#39;: 332,
          &#39;finish&#39;: 32,
          &#39;book&#39;: 132,
          &quot;&#39;ve&quot;: 335,
          &#39;been&#39;: 432,
          &#39;awhile&#39;: 7,
          &#39;bookworm&#39;: 1,
          &#39;yup&#39;: 12,
          &#39;being&#39;: 393,
          &#39;knicks&#39;: 2,
          &#39;easier&#39;: 14,
          &#39;just&#39;: 1349,
          &#39;an&#39;: 576,
          &#39;nba&#39;: 38,
          &#39;playoffs&#39;: 4,
          &#39;roll&#39;: 22,
          &#39;life&#39;: 996,
          &#39;after&#39;: 384,
          &#39;social&#39;: 86,
          &#39;networking&#39;: 6,
          &#39;embrace&#39;: 13,
          &#39;each&#39;: 113,
          &#39;shares&#39;: 6,
          &#39;same&#39;: 124,
          &#39;snake&#39;: 3,
          &#39;lovely&#39;: 149,
          &#39;echeveria&#39;: 1,
          &#39;blooms&#39;: 6,
          &#39;flowers&#39;: 90,
          &#39;grow&#39;: 40,
          &#39;gardening&#39;: 10,
          &#39;iphonesia&#39;: 1,
          &#39;bliss&#39;: 22,
          &#39;amazing&#39;: 309,
          &#39;i_am&#39;: 352,
          &#39;positive&#39;: 493,
          &#39;whenever&#39;: 13,
          &#39;im&#39;: 130,
          &#39;goes&#39;: 89,
          &#39;feeling&#39;: 238,
          &#39;blue&#39;: 82,
          &#39;best&#39;: 502,
          &#39;pa&#39;: 121,
          &#39;ai &#39;: 396,
          &#39;abc&#39;: 8,
          &#39;getting&#39;: 256,
          &#39;ready&#39;: 299,
          &#39;remove&#39;: 12,
          &#39;victums&#39;: 1,
          &#39;frm&#39;: 13,
          &#39;pulseclub&#39;: 2,
          &#39;her&#39;: 388,
          &#39;nose&#39;: 13,
          &#39;job&#39;: 122,
          &#39;petunia&#39;: 1,
          &#39;off&#39;: 369,
          &#39;concelebrate&#39;: 1,
          &#39;at&#39;: 1598,
          &#39;albanpilgrimage&#39;: 1,
          &#39;first&#39;: 473,
          &#39;let&#39;: 277,
          &#39;scum-baggery&#39;: 2,
          &#39;thank&#39;: 307,
          &#39;super&#39;: 103,
          &#39;zpamdelacruz&#39;: 1,
          &#39;wedding&#39;: 181,
          &#39;dolores&#39;: 1,
          &#39;capas&#39;: 1,
          &#39;scourge&#39;: 1,
          &#39;playing&#39;: 157,
          &#39;baroque&#39;: 1,
          &#39;pieces&#39;: 6,
          &#39;piano&#39;: 27,
          &#39;beyond&#39;: 27,
          &#39;lets&#39;: 33,
          &#39;fight&#39;: 35,
          &#39;against&#39;: 124,
          &#39;fatheras&#39;: 9,
          &#39;mr&#39;: 41,
          &#39;rayos&#39;: 2,
          &#39;video&#39;: 169,
          &#39;fathers&#39;: 288,
          &#39;world&#39;: 397,
          &#39;hotvideo&#39;: 3,
          &#39;ascot&#39;: 11,
          &#39;times&#39;: 94,
          &#39;babe&#39;: 24,
          &#39;ai ai &#39;: 23,
          &#39;fashion&#39;: 141,
          &#39;monochrome&#39;: 3,
          &#39;style&#39;: 64,
          &#39;weekend..is&#39;: 1,
          &#39;selfie&#39;: 278,
          &#39;yolo&#39;: 16,
          &#39;xoxo&#39;: 22,
          &#39;work&#39;: 580,
          &#39;conference&#39;: 104,
          &#39;right&#39;: 351,
          &#39;mindset&#39;: 78,
          &#39;leads&#39;: 72,
          &#39;culture-of-development&#39;: 65,
          &#39;organizations&#39;: 67,
          &#39;christina&#39;: 32,
          &#39;grimmie&#39;: 26,
          &#39;last&#39;: 341,
          &#39;performance&#39;: 26,
          &#39;shot&#39;: 50,
          &#39;christinarip&#39;: 1,
          &#39;voice&#39;: 52,
          &#39;dance&#39;: 101,
          &#39;roar&#39;: 4,
          &#39;preschoolers&#39;: 1,
          &#39;students&#39;: 44,
          &#39;really&#39;: 465,
          &#39;hu&#39;: 70,
          &#39;wife&#39;: 50,
          &#39;whom&#39;: 11,
          &#39;adore&#39;: 2,
          &#39;miss&#39;: 168,
          &#39;poland&#39;: 23,
          &#39;show&#39;: 239,
          &#39;because&#39;: 339,
          &#39;she&#39;: 338,
          &#39;surgery&#39;: 4,
          &#39;name&#39;: 56,
          &#39;bridget&#39;: 1,
          &#39;jealous&#39;: 18,
          &#39;celebrate&#39;: 80,
          &#39;every&#39;: 230,
          &#39;has&#39;: 567,
          &#39;played&#39;: 30,
          &#39;fatherly&#39;: 3,
          &#39;role&#39;: 28,
          &#39;sure&#39;: 77,
          &#39;white&#39;: 208,
          &#39;establishment&#39;: 4,
          &#39;blk&#39;: 3,
          &#39;folx&#39;: 3,
          &#39;running&#39;: 75,
          &#39;loving&#39;: 90,
          &#39;themselves&#39;: 31,
          &#39;promoting&#39;: 10,
          &#39;our&#39;: 863,
          &#39;journey&#39;: 29,
          &#39;begins&#39;: 28,
          &#39;travel&#39;: 154,
          &#39;yeah&#39;: 147,
          &#39;thejourneybegins&#39;: 2,
          &#39;luv&#39;: 20,
          &#39;hottweets&#39;: 8,
          &#39;new&#39;: 984,
          &#39;brochures&#39;: 1,
          &#39;arrived&#39;: 68,
          &#39;how&#39;: 859,
          &#39;exciting&#39;: 48,
          &#39;aworks&#39;: 3,
          &#39;much&#39;: 349,
          &#39;stuff&#39;: 53,
          &#39;happening&#39;: 52,
          &#39;florida&#39;: 58,
          &#39;shooting&#39;: 122,
          &#39;disneygatorattack&#39;: 6,
          &#39;two&#39;: 170,
          &#39;old&#39;: 252,
          &#39;ferrari&#39;: 3,
          &#39;ita&#39;: 3,
          &#39;sake&#39;: 9,
          &#39;championship&#39;: 7,
          &#39;gp&#39;: 5,
          &#39;clearly&#39;: 26,
          &#39;turning&#39;: 23,
          &#39;point&#39;: 66,
          &#39;rb&#39;: 2,
          &#39;aced&#39;: 2,
          &#39;test&#39;: 40,
          &#39;seeks&#39;: 13,
          &#39;probe&#39;: 7,
          &#39;udtapunjab&#39;: 42,
          &#39;leak&#39;: 10,
          &#39;points&#39;: 17,
          &#39;finger&#39;: 36,
          &#39;amarinder&#39;: 11,
          &#39;wrapping&#39;: 3,
          &#39;senseaboutmaths&#39;: 2,
          &#39;hey&#39;: 102,
          &#39;call&#39;: 105,
          &quot;&#39;white&quot;: 1,
          &#39;race&#39;: 90,
          &#39;identity&#39;: 5,
          &#39;might&#39;: 223,
          &#39;shown&#39;: 6,
          &#39;regurgitated&#39;: 1,
          &#39;sometimes&#39;: 77,
          &#39;raise&#39;: 14,
          &#39;few&#39;: 124,
          &#39;brows&#39;: 4,
          &#39;bar&#39;: 27,
          &#39;golfstrengthandconditioning&#39;: 1,
          &#39;strong&#39;: 148,
          &#39;greathonour&#39;: 2,
          &#39;designing&#39;: 4,
          &#39;innovative&#39;: 3,
          &#39;learning&#39;: 32,
          &#39;space&#39;: 29,
          &#39;include&#39;: 1,
          &#39;wateringhole&#39;: 1,
          &#39;cave&#39;: 4,
          &#39;mountaintop&#39;: 1,
          &#39;campfire&#39;: 2,
          &#39;altright&#39;: 16,
          &#39;uses&#39;: 8,
          &#39;insecurity&#39;: 2,
          &#39;lure&#39;: 5,
          &#39;men&#39;: 112,
          &#39;carrying&#39;: 8,
          &#39;gun&#39;: 78,
          &quot;wouldn&#39;t&quot;: 30,
          &#39;helped&#39;: 6,
          &#39;control&#39;: 59,
          &quot;won&#39;t&quot;: 111,
          &#39;stop&#39;: 235,
          &#39;black&#39;: 248,
          &#39;market&#39;: 26,
          &#39;terrorism&#39;: 26,
          &#39;power&#39;: 81,
          &#39;mind&#39;: 109,
          &#39;heal&#39;: 58,
          &#39;body&#39;: 114,
          &#39;altwaystoheal&#39;: 261,
          &#39;healthy&#39;: 504,
          &#39;woohoo&#39;: 17,
          &#39;over&#39;: 321,
          &#39;weeks&#39;: 156,
          &#39;far&#39;: 84,
          &#39;place&#39;: 137,
          &#39;where&#39;: 207,
          &#39;family&#39;: 473,
          &#39;members&#39;: 21,
          &#39;rehearse&#39;: 1,
          &#39;music&#39;: 392,
          &#39;videos&#39;: 43,
          &#39;look&#39;: 291,
          &#39;out&#39;: 901,
          &#39;announcement&#39;: 10,
          &#39;midweek&#39;: 6,
          &#39;newmusic&#39;: 10,
          &#39;watchthisspace&#39;: 3,
          &#39;nights&#39;: 22,
          &#39;pm&#39;: 65,
          &#39;channel&#39;: 17,
          &#39;what&#39;: 1158,
          &#39;fuss&#39;: 3,
          &#39;watching&#39;: 169,
          &#39;episodes&#39;: 16,
          &#39;offline&#39;: 3,
          &#39;very&#39;: 413,
          &#39;nice&#39;: 196,
          &#39;long&#39;: 180,
          &#39;snapchat&#39;: 64,
          &#39;redhead&#39;: 7,
          &#39;things&#39;: 293,
          &#39;incredibly&#39;: 16,
          &#39;yes&#39;: 182,
          &#39;received&#39;: 35,
          &#39;acceptance&#39;: 4,
          &#39;letter&#39;: 8,
          &#39;masters&#39;: 8,
          &#39;back&#39;: 417,
          &#39;october&#39;: 7,
          &#39;goodtimes&#39;: 18,
          &#39;daughter&#39;: 71,
          &#39;riding&#39;: 7,
          &#39;bike&#39;: 37,
          &#39;driveway&#39;: 1,
          &#39;son&#39;: 84,
          &#39;guitar&#39;: 17,
          &#39;us&#39;: 494,
          &#39;while&#39;: 113,
          &#39;enjoy&#39;: 227,
          &#39;o&#39;: 118,
          &#39;summeime&#39;: 22,
          &#39;omg&#39;: 111,
          &#39;station&#39;: 15,
          &#39;way&#39;: 481,
          &#39;jam&#39;: 5,
          &#39;done&#39;: 175,
          &#39;course&#39;: 52,
          &quot;&#39;ll&quot;: 251,
          &#39;always&#39;: 334,
          &#39;hope&#39;: 234,
          &#39;hug&#39;: 20,
          &#39;but&#39;: 1085,
          &#39;gon&#39;: 142,
          &#39;na&#39;: 269,
          &#39;happen&#39;: 55,
          &#39;anytime&#39;: 3,
          &#39;couple&#39;: 115,
          &#39;sex&#39;: 120,
          &#39;naked&#39;: 35,
          &#39;japanese&#39;: 15,
          &#39;hump&#39;: 16,
          &#39;humpers&#39;: 1,
          &#39;edwardsville&#39;: 2,
          &#39;personalised&#39;: 16,
          &#39;gbp&#39;: 57,
          &#39;shop&#39;: 103,
          &#39;cool&#39;: 166,
          &#39;home&#39;: 355,
          &#39;some&#39;: 434,
          &#39;truly&#39;: 55,
          &#39;sick&#39;: 77,
          &#39;ppl&#39;: 97,
          &#39;trump&#39;: 352,
          &#39;calling&#39;: 38,
          &#39;obama&#39;: 116,
          &#39;resign&#39;: 8,
          &#39;shootings&#39;: 30,
          &#39;boy&#39;: 134,
          &#39;years&#39;: 192,
          &#39;did&#39;: 199,
          &#39;talk&#39;: 83,
          &#39;nashvilletour&#39;: 1,
          &#39;eur/usd&#39;: 16,
          &#39;clears&#39;: 3,
          &#39;barrier&#39;: 3,
          &#39;jumps&#39;: 2,
          &#39;fresh&#39;: 57,
          &#39;-week&#39;: 4,
          &#39;high&#39;: 81,
          &#39;going&#39;: 479,
          &#39;la&#39;: 45,
          &#39;friends&#39;: 447,
          &#39;still&#39;: 318,
          &#39;wrap&#39;: 7,
          &#39;head&#39;: 120,
          &#39;fact&#39;: 81,
          &#39;christinagrimmie&#39;: 22,
          &#39;gone&#39;: 89,
          &#39;destroyed&#39;: 11,
          &#39;prayfororlando&#39;: 62,
          &#39;dis&#39;: 13,
          &#39;wait&#39;: 435,
          &#39;sta&#39;: 221,
          &#39;baking&#39;: 6,
          &#39;eyelids&#39;: 1,
          &#39;vigilfororlando&#39;: 1,
          &#39;harp&#39;: 1,
          &#39;clonakilty&#39;: 1,
          &#39;ihavenofriends&#39;: 2,
          &#39;relax&#39;: 80,
          &#39;icon&#39;: 4,
          &#39;woman&#39;: 117,
          &#39;sundaymorning&#39;: 14,
          &#39;interested&#39;: 14,
          &#39;linguistics&#39;: 1,
          &quot;doesn&#39;t&quot;: 134,
          &#39;address&#39;: 13,
          &#39;racism&#39;: 68,
          &#39;raciolinguistics&#39;: 1,
          &#39;beloved&#39;: 7,
          &#39;cds&#39;: 3,
          &#39;recovered&#39;: 6,
          &#39;apple&#39;: 37,
          &#39;marvel&#39;: 7,
          &#39;song&#39;: 72,
          &#39;musica&#39;: 3,
          &#39;weed&#39;: 6,
          &#39;ripchristina&#39;: 29,
          &#39;adele&#39;: 10,
          &#39;vine&#39;: 67,
          &#39;why&#39;: 475,
          &#39;mocked&#39;: 3,
          &#39;under&#39;: 78,
          &#39;spell&#39;: 2,
          &#39;brexit&#39;: 64,
          &#39;referendum&#39;: 14,
          &#39;commerzbank&#39;: 5,
          &#39;health&#39;: 120,
          &#39;benefits&#39;: 34,
          &#39;cucumbers&#39;: 15,
          &#39;ofw&#39;: 1,
          &#39;pinoy&#39;: 4,
          &#39;followme&#39;: 146,
          &#39;igers&#39;: 27,
          &#39;instagood&#39;: 209,
          &#39;smile&#39;: 626,
          &#39;toradora&#39;: 1,
          &#39;anime&#39;: 32,
          &#39;animeedit&#39;: 1,
          &#39;breakups&#39;: 3,
          &#39;alone&#39;: 94,
          &#39;laps&#39;: 2,
          &#39;pool&#39;: 42,
          &#39;k&#39;: 67,
          &#39;ride&#39;: 52,
          &#39;picked&#39;: 14,
          &#39;gym&#39;: 87,
          &#39;membership&#39;: 3,
          &#39;cotd&#39;: 5,
          &#39;polar&#39;: 216,
          &#39;bear&#39;: 232,
          &#39;climb&#39;: 245,
          &#39;racing&#39;: 154,
          &#39;living&#39;: 139,
          &#39;cold&#39;: 108,
          &#39;places&#39;: 81,
          &#39;gets&#39;: 77,
          &#39;him&#39;: 235,
          &#39;muslim&#39;: 45,
          &#39;assassinating&#39;: 2,
          &#39;snappy&#39;: 1,
          &#39;waiting&#39;: 169,
          &#39;football&#39;: 59,
          &#39;fringes&#39;: 1,
          &#39;qcbags&#39;: 2,
          &#39;summer&#39;: 513,
          &#39;nochebuena&#39;: 2,
          &#39;lasvegas&#39;: 13,
          &#39;usa&#39;: 79,
          &#39;las&#39;: 12,
          &#39;vegas&#39;: 39,
          &#39;ma&#39;: 15,
          &#39;fleurette&#39;: 1,
          &#39; &#39;: 154,
          &#39;instaboy&#39;: 6,
          &#39;instaman&#39;: 1,
          &#39;sefie&#39;: 1,
          &#39;septum&#39;: 1,
          &#39;friend&#39;: 219,
          &#39;branches&#39;: 2,
          &#39;itas&#39;: 25,
          &#39;rainy&#39;: 17,
          &#39;writing&#39;: 46,
          &#39;tears&#39;: 45,
          &#39;flying&#39;: 29,
          &#39;birds&#39;: 75,
          &#39;haiku&#39;: 5,
          &#39;lines&#39;: 14,
          &#39;buttons&#39;: 3,
          &#39;mail&#39;: 13,
          &#39;me&#39;: 1699,
          &#39;pretty&#39;: 120,
          &#39;jewelrymaking&#39;: 1,
          &#39;driver&#39;: 19,
          &#39;female&#39;: 39,
          &#39;moose&#39;: 5,
          &#39;river&#39;: 15,
          &#39;rd&#39;: 37,
          &#39;weston&#39;: 2,
          &#39;killed&#39;: 68,
          &#39;ok&#39;: 65,
          &#39;crews&#39;: 1,
          &#39;removing&#39;: 3,
          &#39;animal&#39;: 28,
          &#39;afterpas&#39;: 1,
          &#39;japan&#39;: 49,
          &#39;imadeinjapan&#39;: 1,
          &#39;eos&#39;: 1,
          &#39;icute&#39;: 1,
          &#39;fun&#39;: 487,
          &#39;cawaii&#39;: 1,
          &#39;strawberry&#39;: 9,
          &quot;aren&#39;t&quot;: 45,
          &#39;protesting&#39;: 15,
          &#39;republican&#39;: 29,
          &#39;won-they&#39;: 11,
          &#39;fuhered&#39;: 11,
          &#39;find&#39;: 303,
          &#39;spend&#39;: 43,
          &#39;guy&#39;: 105,
          &#39;update&#39;: 65,
          &#39;analytics&#39;: 34,
          &#39;photooftheday&#39;: 153,
          &#39;anyone&#39;: 83,
          &#39;date&#39;: 65,
          &#39;doj&#39;: 4,
          &#39;fbi&#39;: 11,
          &#39;became&#39;: 8,
          &#39;corrupted&#39;: 3,
          &#39;emailgate&#39;: 1,
          &#39;hillary&#39;: 51,
          &#39;shameful&#39;: 12,
          &#39;disgraceful&#39;: 7,
          &#39;stupidity&#39;: 11,
          &#39;makes&#39;: 296,
          &#39;than&#39;: 320,
          &#39;even&#39;: 342,
          &#39;negligence.why&#39;: 1,
          &#39;put&#39;: 100,
          &#39;sachintendulkar&#39;: 1,
          &#39;installation&#39;: 4,
          &#39;a ai &#39;: 6,
          &#39;aaaa&#39;: 16,
          &#39;thbihday&#39;: 2,
          &#39;bestfriend&#39;: 25,
          &#39;d&#39;: 179,
          &#39;most&#39;: 248,
          &#39;impoant&#39;: 74,
          &#39;thing&#39;: 165,
          &#39;matters&#39;: 21,
          &#39;too&#39;: 345,
          &#39;sho&#39;: 47,
          &#39;chris&#39;: 9,
          &#39;evans&#39;: 3,
          &#39;actor&#39;: 46,
          &#39;human&#39;: 47,
          &#39;chrisevans&#39;: 2,
          &#39;heas&#39;: 29,
          &#39;thoughts&#39;: 91,
          &#39;prayers&#39;: 65,
          &#39;go&#39;: 627,
          &#39;murdered&#39;: 19,
          &#39;gay&#39;: 119,
          &#39;demoing&#39;: 1,
          &#39;guitars&#39;: 2,
          &#39;album&#39;: 36,
          &#39;newalbum&#39;: 1,
          &#39;indie&#39;: 12,
          &#39;retweeted&#39;: 11,
          &#39;lion&#39;: 10,
          &#39;pro&#39;: 14,
          &#39;webmareting&#39;: 1,
          &#39;seo&#39;: 2,
          &#39;community&#39;: 51,
          &#39;management&#39;: 9,
          &#39;nzd/usd&#39;: 9,
          &#39;targets&#39;: 3,
          &#39;week&#39;: 378,
          &#39;sma&#39;: 29,
          &#39;bad&#39;: 166,
          &#39;worst&#39;: 56,
          &#39;ever&#39;: 235,
          &#39;bihdayweeksucks&#39;: 2,
          &#39;bithday&#39;: 2,
          &#39;blessed&#39;: 190,
          &#39;worked&#39;: 28,
          &#39;sa&#39;: 13,
          &#39;leading&#39;: 11,
          &#39;happiest&#39;: 25,
          &#39;eah&#39;: 34,
          &#39;disneysmagickingdom&#39;: 1,
          &#39;disney&#39;: 66,
          &#39;magickingdom&#39;: 3,
          &#39;disneyland&#39;: 12,
          &#39;kinda&#39;: 22,
          &#39;among&#39;: 13,
          &#39;humans&#39;: 25,
          &#39;found&#39;: 186,
          &#39;exclaiming&#39;: 1,
          &#39;gr&#39;: 10,
          &#39;blain&#39;: 1,
          &#39;hair&#39;: 145,
          &#39;other&#39;: 168,
          &#39;lt&#39;: 151,
          &#39;listen&#39;: 121,
          &#39;n&#39;: 176,
          &#39;freedom&#39;: 55,
          &#39;effo&#39;: 17,
          &#39;present&#39;: 53,
          &#39;merely&#39;: 4,
          &#39;stored&#39;: 2,
          &#39;past&#39;: 51,
          &#39;theodore&#39;: 1,
          &#39;yall&#39;: 11,
          &#39;aint&#39;: 6,
          &#39;commitment&#39;: 5,
          &#39;trust&#39;: 38,
          &#39;michelleobama&#39;: 5,
          &#39;gorilla&#39;: 124,
          &#39;racists&#39;: 8,
          &#39;thought&#39;: 119,
          &#39;newarkfestival&#39;: 1,
          &#39;internet&#39;: 42,
          &#39;broken&#39;: 56,
          &#39;watch&#39;: 236,
          &#39;netflix&#39;: 23,
          &#39;mochithepug&#39;: 1,
          &#39;then&#39;: 281,
          &#39;what..business&#39;: 1,
          &#39;hbd&#39;: 6,
          &#39;dick&#39;: 10,
          &#39;suckin&#39;: 2,
          &#39;tequila&#39;: 2,
          &#39;lovin&#39;: 3,
          &#39;slut&#39;: 55,
          &#39;wouldnt&#39;: 5,
          &#39;any&#39;: 204,
          &#39;justalillate&#39;: 2,
          &#39;smaller&#39;: 5,
          &#39;hands&#39;: 39,
          &#39;barry&#39;: 3,
          &#39;probably&#39;: 41,
          &#39;lied&#39;: 12,
          &#39;game&#39;: 229,
          &#39;sucked&#39;: 6,
          &#39;everyone&#39;: 259,
          &#39;doing&#39;: 153,
          &#39;goodmorning&#39;: 78,
          &#39;girls&#39;: 243,
          &#39;challenges&#39;: 14,
          &#39;aap&#39;: 15,
          &#39;claim&#39;: 15,
          &#39;punjabis&#39;: 4,
          &#39;drugaddicts&#39;: 3,
          &#39;dares&#39;: 4,
          &#39;tracerequest&#39;: 1,
          &#39;sending&#39;: 19,
          &#39;deepest&#39;: 3,
          &#39;condolences&#39;: 12,
          &#39;zimbabwe&#39;: 1,
          &#39;hea&#39;: 177,
          &#39;a.a&#39;: 2,
          &#39;mato&#39;: 1,
          &#39;saturday&#39;: 202,
          &#39;afternoon&#39;: 55,
          &#39;chi&#39;: 2,
          &#39;meet&#39;: 100,
          &#39;soed&#39;: 8,
          &#39;nut&#39;: 5,
          &#39;bolts&#39;: 2,
          &#39;aww&#39;: 87,
          &#39;bing&#39;: 152,
          &#39;bong&#39;: 75,
          &#39;dawned&#39;: 2,
          &#39;months&#39;: 91,
          &#39;seeing&#39;: 101,
          &#39;live&#39;: 322,
          &#39;vfest&#39;: 1,
          &#39;th&#39;: 204,
          &#39;shane&#39;: 8,
          &#39;robe&#39;: 12,
          &#39;watson&#39;: 4,
          &#39;millions&#39;: 14,
          &#39;pointed&#39;: 2,
          &#39;hear&#39;: 103,
          &#39;announcers&#39;: 2,
          &#39;say&#39;: 233,
          &#39;moved&#39;: 18,
          &#39;player&#39;: 17,
          &#39;or&#39;: 539,
          &#39;lead&#39;: 24,
          &#39;radio&#39;: 24,
          &#39;coldplay&#39;: 17,
          &#39;god&#39;: 234,
          &#39;coming&#39;: 200,
          &#39;through&#39;: 106,
          &#39;keeping&#39;: 29,
          &#39;terrorist&#39;: 26,
          &#39;constitutional&#39;: 6,
          &#39;rights&#39;: 18,
          &#39;another&#39;: 263,
          &#39;excuse&#39;: 15,
          &#39;republicans&#39;: 15,
          &#39;appease&#39;: 1,
          &#39;nra&#39;: 6,
          &#39;everything&#39;: 138,
          &#39;together&#39;: 86,
          &#39;down&#39;: 246,
          &#39;fine&#39;: 21,
          &#39;dandy&#39;: 2,
          &#39;lonely&#39;: 43,
          &#39;exuberant&#39;: 1,
          &#39;depressed&#39;: 60,
          &#39;halffull&#39;: 1,
          &#39;mikeashley&#39;: 1,
          &#39;sposdirectshame&#39;: 1,
          &#39;perhapse&#39;: 1,
          &#39;example&#39;: 11,
          &#39;protect&#39;: 16,
          &#39;workersrights&#39;: 1,
          &#39;within&#39;: 28,
          &#39;europe&#39;: 28,
          &#39;euref&#39;: 8,
          &#39;stella&#39;: 8,
          &#39;princess&#39;: 25,
          &#39;birdsstellabadprincess&#39;: 1,
          &#39;post&#39;: 97,
          &#39;check&#39;: 150,
          &#39;memories&#39;: 52,
          &#39;saw&#39;: 87,
          &#39;t&#39;: 45,
          &#39;shi&#39;: 21,
          &#39;said&#39;: 121,
          &#39;cared&#39;: 8,
          &#39;hilarious&#39;: 12,
          &#39;already&#39;: 113,
          &#39;blocked&#39;: 21,
          &#39;asking&#39;: 20,
          &#39;paicularly&#39;: 5,
          &#39;difficult&#39;: 24,
          &#39;devastating&#39;: 3,
          &#39;news&#39;: 208,
          &#39;victims&#39;: 73,
          &#39;happiness&#39;: 345,
          &#39;state&#39;: 62,
          &#39;arrive&#39;: 22,
          &#39;manner&#39;: 3,
          &#39;traveling&#39;: 20,
          &#39;margaret&#39;: 1,
          &#39;lee&#39;: 13,
          &#39;runbeck&#39;: 1,
          ...})</code></pre>
</div>
</div>
<section id="we-have-34289-unique-words" class="cell markdown" id="epnG4KkvOR32">
<h3>We have 34289 unique words</h3>
</section>
<div class="cell code" data-execution_count="32" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="kKWHKSRkOR32" data-outputId="61fdaa34-8901-4b34-ec30-f8820250c787">
<div class="sourceCode" id="cb42"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(dist)</span></code></pre></div>
<div class="output execute_result" data-execution_count="32">
<pre><code>34407</code></pre>
</div>
</div>
<section id="the-longest-tweet-has-42-words" class="cell markdown" id="2mkwTxsJOR32">
<h3>The longest tweet has 42 words</h3>
</section>
<div class="cell code" data-execution_count="33" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="tYQwjCw_OR32" data-outputId="aba4c0b6-4e04-4f33-fc7f-2f33b08234e2">
<div class="sourceCode" id="cb44"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="bu">max</span>(df.Words_normalized_no_user_fixed.<span class="bu">apply</span>(<span class="bu">len</span>))</span></code></pre></div>
<div class="output execute_result" data-execution_count="33">
<pre><code>42</code></pre>
</div>
</div>
<section id="we-will-use-new-textvectorization-layer-for-creating-vector-model-from-our-text-data" class="cell markdown" id="Fnhyl4d4OR32">
<h1>We will use new TextVectorization layer for creating vector model from our text data</h1>
<p>For those of you who are interested in the topic there is very good <a href="https://towardsdatascience.com/you-should-try-the-new-tensorflows-textvectorization-layer-a80b3c6b00ee">article on Medium</a> about the layer and its parameters.</p>
<p>There is of course a <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization">documentation page</a> about the layer.</p>
</section>
<div class="cell code" data-execution_count="34" id="NuatO_UtOR33">
<div class="sourceCode" id="cb46"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow <span class="im">import</span> string <span class="im">as</span> tf_string</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers.experimental.preprocessing <span class="im">import</span> TextVectorization</span></code></pre></div>
</div>
<div class="cell code" id="mzDmTd-JOR33">
<div class="sourceCode" id="cb47"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">128</span> <span class="co"># Dimension of embedded representation - this is already part of latent space, there is captured some dependecy among words, we are learning this vectors in ANN</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="dv">10000</span> <span class="co"># Number of unique tokens in vocabulary</span></span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>sequence_length <span class="op">=</span> <span class="dv">30</span> <span class="co"># Output dimension after vectorizing - words in vectorited representation are independent</span></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>vect_layer <span class="op">=</span> TextVectorization(max_tokens<span class="op">=</span>vocab_size, output_mode<span class="op">=</span><span class="st">&#39;int&#39;</span>, output_sequence_length<span class="op">=</span>sequence_length)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>vect_layer.adapt(df.Clean_text.values)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a></span></code></pre></div>
</div>
<section id="we-will-split-our-dataset-to-train-and-test-parts-with-stratification" class="cell markdown" id="XYhx_lrQOR33">
<h3>We will split our dataset to train and test parts with stratification</h3>
</section>
<div class="cell code" id="qQNPYpH5OR33">
<div class="sourceCode" id="cb48"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(df.Clean_text, df.label, test_size<span class="op">=</span><span class="fl">0.20</span>, random_state<span class="op">=</span><span class="dv">13</span>, stratify<span class="op">=</span>df.label)</span></code></pre></div>
</div>
<div class="cell code" id="lolUX74oOR33">
<div class="sourceCode" id="cb49"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>X_train, X_valid, y_train, y_valid <span class="op">=</span> train_test_split(X_train, y_train, test_size<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">13</span>, stratify<span class="op">=</span>y_train)</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="xfMORXmkOR33" data-outputId="1bfc0694-ebfb-455e-9dbf-bdcd2d6e115e">
<div class="sourceCode" id="cb50"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_train.shape, X_test.shape)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>(23012,) (6393,)
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="wVUhHwu2OR34" data-outputId="042931fb-919a-461e-df9d-f0504ee6d49c">
<div class="sourceCode" id="cb52"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Train&#39;</span>)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y_train.value_counts())</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Test&#39;</span>)</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(y_test.value_counts())</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Train
0    21397
1     1615
Name: label, dtype: int64
Test
0    5945
1     448
Name: label, dtype: int64
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="NFW9CRzGOR34" data-outputId="d229ca0c-ea1a-4861-d887-e00a0592e184">
<div class="sourceCode" id="cb54"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Vocabulary example: &#39;</span>, vect_layer.get_vocabulary()[:<span class="dv">10</span>])</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&#39;Vocabulary shape: &#39;</span>, <span class="bu">len</span>(vect_layer.get_vocabulary()))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Vocabulary example:  [&#39;&#39;, &#39;[UNK]&#39;, &#39;the&#39;, &#39;to&#39;, &#39;a&#39;, &#39;i&#39;, &#39;you&#39;, &#39;and&#39;, &#39;in&#39;, &#39;for&#39;]
Vocabulary shape:  10000
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="35" id="HiixwQAFOR34">
<div class="sourceCode" id="cb56"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.compat.v1.keras.layers <span class="im">import</span> CuDNNGRU, CuDNNLSTM</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> LSTM, GRU, Bidirectional</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="se0btowQOR34" data-outputId="dcfce318-6b74-4b37-868c-7753ebf58a93">
<div class="sourceCode" id="cb57"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>input_layer <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span>tf_string)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>x_v <span class="op">=</span> vect_layer(input_layer)</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>emb <span class="op">=</span> keras.layers.Embedding(vocab_size, embedding_dim)(x_v)</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> LSTM(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, return_sequences<span class="op">=</span><span class="va">True</span>)(emb)</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> GRU(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, return_sequences<span class="op">=</span><span class="va">True</span>)(x)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Flatten()(x)</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">64</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">32</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.2</span>)(x)</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>output_layer <span class="op">=</span> keras.layers.Dense(<span class="dv">1</span>, <span class="st">&#39;sigmoid&#39;</span>)(x)</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.Model(input_layer, output_layer)</span>
<span id="cb57-13"><a href="#cb57-13" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb57-14"><a href="#cb57-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-15"><a href="#cb57-15" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;rmsprop&#39;</span>, loss<span class="op">=</span>keras.losses.BinaryCrossentropy(), metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span></code></pre></div>
<div class="output stream stdout">
<pre><code>WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Model: &quot;model&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1)]               0         
                                                                 
 text_vectorization (TextVec  (None, 30)               0         
 torization)                                                     
                                                                 
 embedding (Embedding)       (None, 30, 128)           1280000   
                                                                 
 lstm (LSTM)                 (None, 30, 64)            49408     
                                                                 
 gru (GRU)                   (None, 30, 64)            24960     
                                                                 
 flatten (Flatten)           (None, 1920)              0         
                                                                 
 dense (Dense)               (None, 64)                122944    
                                                                 
 dense_1 (Dense)             (None, 32)                2080      
                                                                 
 dropout (Dropout)           (None, 32)                0         
                                                                 
 dense_2 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 1,479,425
Trainable params: 1,479,425
Non-trainable params: 0
_________________________________________________________________
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;height&quot;:200,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="hVSmoztPOR35" data-outputId="c5c28b44-1277-4b0a-a988-a82eb92545c8">
<div class="sourceCode" id="cb59"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>es <span class="op">=</span> keras.callbacks.EarlyStopping(monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, min_delta<span class="op">=</span><span class="dv">0</span>, patience<span class="op">=</span><span class="dv">70</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train.values, y_train.values, validation_data<span class="op">=</span>(X_valid.values, y_valid.values), callbacks<span class="op">=</span>[es], epochs<span class="op">=</span>epochs, batch_size<span class="op">=</span>batch_size)</span></code></pre></div>
<div class="output error" data-ename="NameError" data-evalue="ignored">
<pre><code>---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
&lt;ipython-input-44-664c6e9048a6&gt; in &lt;module&gt;()
      3 batch_size = 128
      4 epochs = 5
----&gt; 5 history = model.fit(X_train.values, y_train.values, validation_data=(X_valid.values, y_valid.values), callbacks=[es], epochs=epochs, batch_size=batch_size)

NameError: name &#39;model&#39; is not defined
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;height&quot;:297,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="KmQLoAgCOR35" data-outputId="a1883f0a-58c1-4787-977d-621971e960ea">
<div class="sourceCode" id="cb61"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>show_history(history)</span></code></pre></div>
<div class="output display_data">
<p><img src="vertopal_5d270abade7b464aacf99cad3478e1a6/ea71e7dad2d4e617f267be53f46921f209a1775e.png" /></p>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="khyN11ElOR35" data-outputId="207d3532-6392-471c-c897-b4eacff57fd7">
<div class="sourceCode" id="cb62"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>y_test_loss, accuracy <span class="op">=</span> model.evaluate(X_test, y_test)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>200/200 [==============================] - 4s 19ms/step - loss: 0.1326 - accuracy: 0.9595
</code></pre>
</div>
</div>
<div class="cell code" id="eBjivEMdOR35">
<div class="sourceCode" id="cb64"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test).ravel()</span></code></pre></div>
</div>
<div class="cell markdown" id="71kQDIkAOR36">
<h4 id="sigmoid-function-gives-us-real-number-in-range-0-1">Sigmoid function gives us real number in range &lt;0, 1&gt;.</h4>
<h4 id="we-need-to-map-this-valus-to-classes">We need to map this valus to classes</h4>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="KcYEGdhFOR36" data-outputId="f822f79e-366b-4230-ef46-68c19d693935">
<div class="sourceCode" id="cb65"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>y_pred</span></code></pre></div>
<div class="output execute_result" data-execution_count="48">
<pre><code>array([5.8227992e-03, 1.5153906e-04, 4.9275317e-04, ..., 9.9427795e-01,
       3.3160646e-05, 3.6857808e-03], dtype=float32)</code></pre>
</div>
</div>
<div class="cell code" id="0qcQ9AIyOR36">
<div class="sourceCode" id="cb67"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> [<span class="dv">1</span> <span class="cf">if</span> x <span class="op">&gt;=</span> <span class="fl">0.5</span> <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> x <span class="kw">in</span> y_pred]</span></code></pre></div>
</div>
<section id="we-can-see-that-accuracy-is-not-the-best-metric-in-the-imbalanced-situation---why" class="cell markdown" id="HEZTrxkIOR36">
<h1>We can see that accuracy is not the best metric in the imbalanced situation - why?</h1>
<p>There are many more metrics we can use and one of the most common in this situation is the F1 Score, see <a href="https://en.wikipedia.org/wiki/F-score">this</a> and <a href="https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/">this</a> for more info</p>
</section>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="cFdxD859OR37" data-outputId="d1d5e0b9-7bf6-406e-f5f5-db04bd24a607">
<div class="sourceCode" id="cb68"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a>accuracy_score(y_true<span class="op">=</span>y_test, y_pred<span class="op">=</span>y_pred)</span></code></pre></div>
<div class="output execute_result" data-execution_count="50">
<pre><code>0.9594869388393555</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="NE1mDvRHOR37" data-outputId="aceff9bf-bf17-4b4e-fd3c-f3e6b10bdf39">
<div class="sourceCode" id="cb70"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>f1_score(y_true<span class="op">=</span>y_test, y_pred<span class="op">=</span>y_pred)</span></code></pre></div>
<div class="output execute_result" data-execution_count="51">
<pre><code>0.6852976913730254</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="M9JZwMDeOR38" data-outputId="cca830fa-73fc-46b7-ecbd-32176b1bec94">
<div class="sourceCode" id="cb72"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_true<span class="op">=</span>y_test, y_pred<span class="op">=</span>y_pred))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>              precision    recall  f1-score   support

           0       0.97      0.98      0.98      5945
           1       0.75      0.63      0.69       448

    accuracy                           0.96      6393
   macro avg       0.86      0.81      0.83      6393
weighted avg       0.96      0.96      0.96      6393

</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="5TaDq2t2OR38" data-outputId="0bf56c40-ec07-4e02-b92f-5b147884cf41">
<div class="sourceCode" id="cb74"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_true<span class="op">=</span>y_test, y_pred<span class="op">=</span>y_pred))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>[[5852   93]
 [ 166  282]]
</code></pre>
</div>
</div>
<section id="we-dont-have-to-train-our-own-embedding" class="cell markdown" id="A9p1JujMOR38">
<h1>We don't have to train our own embedding</h1>
<p>There are multiple embeddings available online which were trained on very large corpuses e.g. Wikipedia. Good examples are Word2Vec, Glove or FastText. These embeddings contains fixed length vectors for words in the vocabulary.</p>
<p>We will use GloVe embedding with 50 dimensional embedding vectors. For more details see <a href="https://nlp.stanford.edu/projects/glove/">this</a>. You can download zip with vectors from <a href="http://nlp.stanford.edu/data/glove.6B.zip">http://nlp.stanford.edu/data/glove.6B.zip</a> ~ 800 MB</p>
<h4 id="beware-that-the-original-text-corpus-was-more-general-than-the-specific-social-media-text-data-so-if-you-deal-with-very-specific-domains-it-may-be-beneficial-to-train-your-own-embedding-or-at-least-fine-tune-existing-one">Beware that the original text corpus was more general than the specific social media text data, so if you deal with very specific domains it may be beneficial to train your own embedding or at least fine tune existing one.</h4>
</section>
<section id="we-need-to-download-the-embedding-files" class="cell markdown" id="Gb-rzb8NOR38">
<h1>We need to download the embedding files</h1>
<pre><code>!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip -q glove.6B.zip</code></pre>
<p>50 dims GLOVE is also avaiable here: <a href="https://vsb.ai/downloads/glove.6B.50d.txt" class="uri">https://vsb.ai/downloads/glove.6B.50d.txt</a></p>
</section>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="Ki2u2oL4OYsz" data-outputId="13aaac45-4a88-45be-8482-5f2c2eea99ac">
<div class="sourceCode" id="cb77"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>wget http:<span class="op">//</span>nlp.stanford.edu<span class="op">/</span>data<span class="op">/</span>glove<span class="fl">.6</span>B.<span class="bu">zip</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>--2022-03-12 11:25:40--  http://nlp.stanford.edu/data/glove.6B.zip
Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140
Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://nlp.stanford.edu/data/glove.6B.zip [following]
--2022-03-12 11:25:40--  https://nlp.stanford.edu/data/glove.6B.zip
Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.
HTTP request sent, awaiting response... 301 Moved Permanently
Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]
--2022-03-12 11:25:40--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip
Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22
Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 862182613 (822M) [application/zip]
Saving to: ‘glove.6B.zip’

glove.6B.zip        100%[===================&gt;] 822.24M  5.35MB/s    in 2m 41s  

2022-03-12 11:28:22 (5.10 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]

</code></pre>
</div>
</div>
<div class="cell code" id="slgSXk0mOZ4j">
<div class="sourceCode" id="cb79"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>unzip <span class="op">-</span>q glove<span class="fl">.6</span>B.<span class="bu">zip</span></span></code></pre></div>
</div>
<section id="first-we-need-to-load-the-file-to-memory-and-create-embedding-dictionary" class="cell markdown" id="m_YiHC4rOR38">
<h1>First we need to load the file to memory and create embedding dictionary</h1>
</section>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="zdFnKtCvOR38" data-outputId="17f89064-cd97-402e-ade8-e20df8780951">
<div class="sourceCode" id="cb80"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>path_to_glove_file <span class="op">=</span> <span class="st">&#39;glove.6B.50d.txt&#39;</span></span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>embeddings_index <span class="op">=</span> {}</span>
<span id="cb80-4"><a href="#cb80-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(path_to_glove_file) <span class="im">as</span> f:</span>
<span id="cb80-5"><a href="#cb80-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> line <span class="kw">in</span> f:</span>
<span id="cb80-6"><a href="#cb80-6" aria-hidden="true" tabindex="-1"></a>        word, coefs <span class="op">=</span> line.split(maxsplit<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb80-7"><a href="#cb80-7" aria-hidden="true" tabindex="-1"></a>        coefs <span class="op">=</span> np.fromstring(coefs, <span class="st">&quot;f&quot;</span>, sep<span class="op">=</span><span class="st">&quot; &quot;</span>)</span>
<span id="cb80-8"><a href="#cb80-8" aria-hidden="true" tabindex="-1"></a>        embeddings_index[word] <span class="op">=</span> coefs</span>
<span id="cb80-9"><a href="#cb80-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb80-10"><a href="#cb80-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Found </span><span class="sc">%s</span><span class="st"> word vectors.&quot;</span> <span class="op">%</span> <span class="bu">len</span>(embeddings_index))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Found 400000 word vectors.
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="DhPTh3NKOR39" data-outputId="cff696c3-0f43-4688-f396-f41d1ea24c76">
<div class="sourceCode" id="cb82"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>embeddings_index[<span class="st">&#39;analysis&#39;</span>]</span></code></pre></div>
<div class="output execute_result" data-execution_count="48">
<pre><code>array([ 0.47168 ,  0.25095 ,  0.078363,  0.33009 ,  0.32782 ,  0.47325 ,
        0.16805 , -0.99997 ,  0.8607  ,  0.018286, -0.022716, -0.16986 ,
       -0.33896 ,  0.10096 , -0.29278 , -0.18954 ,  0.063336, -0.64536 ,
       -0.098547, -0.46883 ,  0.26338 , -0.063779,  0.51437 , -0.34739 ,
        0.26537 , -0.58654 , -1.2527  , -0.20557 , -0.31604 ,  0.59308 ,
        2.8329  , -1.2396  , -0.23409 , -1.66    ,  0.047692, -0.23141 ,
       -0.14241 ,  0.74129 ,  0.69887 ,  0.14877 ,  0.6547  , -0.29812 ,
        0.10476 ,  0.56112 , -0.015528,  0.099004,  1.3858  ,  1.8129  ,
        0.3804  ,  0.83921 ], dtype=float32)</code></pre>
</div>
</div>
<section id="we-need-to-get-the-voacabulary-from-the-vectorizer-and-the-integer-indexes" class="cell markdown" id="dmY-c7DDOR39">
<h2>We need to get the voacabulary from the Vectorizer and the integer indexes</h2>
</section>
<div class="cell code" id="WzXgSZlFOR39">
<div class="sourceCode" id="cb84"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">50</span> <span class="co"># Dimension of embedded representation - this is already part of latent space, there is captured some dependecy among words, we are learning this vectors in ANN</span></span>
<span id="cb84-2"><a href="#cb84-2" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="dv">10000</span> <span class="co"># Number of unique tokens in vocabulary</span></span>
<span id="cb84-3"><a href="#cb84-3" aria-hidden="true" tabindex="-1"></a>sequence_length <span class="op">=</span> <span class="dv">20</span> <span class="co"># Output dimension after vectorizing - words in vectorited representation are independent</span></span>
<span id="cb84-4"><a href="#cb84-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb84-5"><a href="#cb84-5" aria-hidden="true" tabindex="-1"></a>vect_layer <span class="op">=</span> TextVectorization(max_tokens<span class="op">=</span>vocab_size, output_mode<span class="op">=</span><span class="st">&#39;int&#39;</span>, output_sequence_length<span class="op">=</span>sequence_length)</span>
<span id="cb84-6"><a href="#cb84-6" aria-hidden="true" tabindex="-1"></a>vect_layer.adapt(df.Clean_text.values)</span></code></pre></div>
</div>
<div class="cell code" id="K9e10NsMOR39">
<div class="sourceCode" id="cb85"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>voc <span class="op">=</span> vect_layer.get_vocabulary()</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>word_index <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(voc, <span class="bu">range</span>(<span class="bu">len</span>(voc))))</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="omziOVtVOR39" data-outputId="0a7c9563-fb7a-4069-e2eb-29405624b5a4">
<div class="sourceCode" id="cb86"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1" aria-hidden="true" tabindex="-1"></a>voc[:<span class="dv">10</span>]</span></code></pre></div>
<div class="output execute_result" data-execution_count="51">
<pre><code>[&#39;&#39;, &#39;[UNK]&#39;, &#39;the&#39;, &#39;to&#39;, &#39;a&#39;, &#39;i&#39;, &#39;you&#39;, &#39;and&#39;, &#39;in&#39;, &#39;for&#39;]</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="1-Hs-O8HOR39" data-outputId="a539ca9a-5b76-46ac-9324-c6bbb3ca2d8a">
<div class="sourceCode" id="cb88"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a>word_index[<span class="st">&#39;the&#39;</span>]</span></code></pre></div>
<div class="output execute_result" data-execution_count="52">
<pre><code>2</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="OBK_iOUsOR39" data-outputId="83350739-e3a2-434d-e027-b4d7ddb3826f">
<div class="sourceCode" id="cb90"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>embeddings_index[<span class="st">&#39;the&#39;</span>]</span></code></pre></div>
<div class="output execute_result" data-execution_count="53">
<pre><code>array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,
       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,
        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,
        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,
       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,
       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,
        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,
        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,
       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,
        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],
      dtype=float32)</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="_OMQN2m5OR3-" data-outputId="bc4a7c0b-3fce-459d-cc5f-fff30d241bc7">
<div class="sourceCode" id="cb92"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1" aria-hidden="true" tabindex="-1"></a>num_tokens <span class="op">=</span> <span class="bu">len</span>(voc) <span class="op">+</span> <span class="dv">2</span></span>
<span id="cb92-2"><a href="#cb92-2" aria-hidden="true" tabindex="-1"></a>hits <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb92-3"><a href="#cb92-3" aria-hidden="true" tabindex="-1"></a>misses <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb92-4"><a href="#cb92-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb92-5"><a href="#cb92-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare embedding matrix</span></span>
<span id="cb92-6"><a href="#cb92-6" aria-hidden="true" tabindex="-1"></a>embedding_matrix <span class="op">=</span> np.zeros((num_tokens, embedding_dim))</span>
<span id="cb92-7"><a href="#cb92-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, i <span class="kw">in</span> word_index.items():</span>
<span id="cb92-8"><a href="#cb92-8" aria-hidden="true" tabindex="-1"></a>    embedding_vector <span class="op">=</span> embeddings_index.get(word)</span>
<span id="cb92-9"><a href="#cb92-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> embedding_vector <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb92-10"><a href="#cb92-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Words not found in embedding index will be all-zeros.</span></span>
<span id="cb92-11"><a href="#cb92-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This includes the representation for &quot;padding&quot; and &quot;OOV&quot;</span></span>
<span id="cb92-12"><a href="#cb92-12" aria-hidden="true" tabindex="-1"></a>        embedding_matrix[i] <span class="op">=</span> embedding_vector</span>
<span id="cb92-13"><a href="#cb92-13" aria-hidden="true" tabindex="-1"></a>        hits <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb92-14"><a href="#cb92-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb92-15"><a href="#cb92-15" aria-hidden="true" tabindex="-1"></a>        misses <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb92-16"><a href="#cb92-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Converted </span><span class="sc">%d</span><span class="st"> words (</span><span class="sc">%d</span><span class="st"> misses)&quot;</span> <span class="op">%</span> (hits, misses))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Converted 8500 words (1500 misses)
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="ok_Ex-ZXOR3-" data-outputId="46383c5c-756e-4fdc-bd51-ae48da37f157">
<div class="sourceCode" id="cb94"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1" aria-hidden="true" tabindex="-1"></a>embedding_matrix[<span class="dv">2</span>]</span></code></pre></div>
<div class="output execute_result" data-execution_count="55">
<pre><code>array([ 4.18000013e-01,  2.49679998e-01, -4.12420005e-01,  1.21699996e-01,
        3.45270008e-01, -4.44569997e-02, -4.96879995e-01, -1.78619996e-01,
       -6.60229998e-04, -6.56599998e-01,  2.78430015e-01, -1.47670001e-01,
       -5.56770027e-01,  1.46579996e-01, -9.50950012e-03,  1.16579998e-02,
        1.02040000e-01, -1.27920002e-01, -8.44299972e-01, -1.21809997e-01,
       -1.68009996e-02, -3.32789987e-01, -1.55200005e-01, -2.31309995e-01,
       -1.91809997e-01, -1.88230002e+00, -7.67459989e-01,  9.90509987e-02,
       -4.21249986e-01, -1.95260003e-01,  4.00710011e+00, -1.85939997e-01,
       -5.22870004e-01, -3.16810012e-01,  5.92130003e-04,  7.44489999e-03,
        1.77780002e-01, -1.58969998e-01,  1.20409997e-02, -5.42230010e-02,
       -2.98709989e-01, -1.57490000e-01, -3.47579986e-01, -4.56370004e-02,
       -4.42510009e-01,  1.87849998e-01,  2.78489990e-03, -1.84110001e-01,
       -1.15139998e-01, -7.85809994e-01])</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="0NE0Rsr3OR3-" data-outputId="cc7c040e-4f80-47d0-de66-b2595a5c2498">
<div class="sourceCode" id="cb96"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1" aria-hidden="true" tabindex="-1"></a>show_historyyer <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span>tf_string)</span>
<span id="cb96-2"><a href="#cb96-2" aria-hidden="true" tabindex="-1"></a>x_v <span class="op">=</span> vect_layer(input_layer)</span>
<span id="cb96-3"><a href="#cb96-3" aria-hidden="true" tabindex="-1"></a>emb <span class="op">=</span> keras.layers.Embedding(num_tokens, embedding_dim, embeddings_initializer<span class="op">=</span>keras.initializers.Constant(embedding_matrix), trainable<span class="op">=</span><span class="va">False</span>)(x_v)</span>
<span id="cb96-4"><a href="#cb96-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> LSTM(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, return_sequences<span class="op">=</span><span class="va">True</span>)(emb)</span>
<span id="cb96-5"><a href="#cb96-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> GRU(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, return_sequences<span class="op">=</span><span class="va">False</span>)(x)</span>
<span id="cb96-6"><a href="#cb96-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Flatten()(x)</span>
<span id="cb96-7"><a href="#cb96-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">64</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb96-8"><a href="#cb96-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">32</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb96-9"><a href="#cb96-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.2</span>)(x)</span>
<span id="cb96-10"><a href="#cb96-10" aria-hidden="true" tabindex="-1"></a>output_layer <span class="op">=</span> keras.layers.Dense(<span class="dv">1</span>, <span class="st">&#39;sigmoid&#39;</span>)(x)</span>
<span id="cb96-11"><a href="#cb96-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-12"><a href="#cb96-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.Model(input_layer, output_layer)</span>
<span id="cb96-13"><a href="#cb96-13" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb96-14"><a href="#cb96-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb96-15"><a href="#cb96-15" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;rmsprop&#39;</span>, loss<span class="op">=</span>keras.losses.BinaryCrossentropy(), metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span></code></pre></div>
<div class="output stream stdout">
<pre><code>WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Model: &quot;model_1&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1)]               0         
                                                                 
 text_vectorization_1 (TextV  (None, 20)               0         
 ectorization)                                                   
                                                                 
 embedding_1 (Embedding)     (None, 20, 50)            500100    
                                                                 
 lstm_1 (LSTM)               (None, 20, 64)            29440     
                                                                 
 gru_1 (GRU)                 (None, 64)                24960     
                                                                 
 flatten_1 (Flatten)         (None, 64)                0         
                                                                 
 dense_3 (Dense)             (None, 64)                4160      
                                                                 
 dense_4 (Dense)             (None, 32)                2080      
                                                                 
 dropout_1 (Dropout)         (None, 32)                0         
                                                                 
 dense_5 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 560,773
Trainable params: 60,673
Non-trainable params: 500,100
_________________________________________________________________
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="C4TJdJ3MOR3-" data-outputId="6b3a5dc1-33b0-4548-91b7-fc9cd22ab7fc">
<div class="sourceCode" id="cb98"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1" aria-hidden="true" tabindex="-1"></a>es <span class="op">=</span> keras.callbacks.EarlyStopping(monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, min_delta<span class="op">=</span><span class="dv">0</span>, patience<span class="op">=</span><span class="dv">70</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb98-2"><a href="#cb98-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb98-3"><a href="#cb98-3" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb98-4"><a href="#cb98-4" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb98-5"><a href="#cb98-5" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train.values, y_train.values, validation_data<span class="op">=</span>(X_valid.values, y_valid.values), callbacks<span class="op">=</span>[es], epochs<span class="op">=</span>epochs, batch_size<span class="op">=</span>batch_size)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/5
180/180 [==============================] - 30s 150ms/step - loss: 0.2314 - accuracy: 0.9290 - val_loss: 0.1642 - val_accuracy: 0.9300
Epoch 2/5
180/180 [==============================] - 27s 152ms/step - loss: 0.1787 - accuracy: 0.9375 - val_loss: 0.1683 - val_accuracy: 0.9437
Epoch 3/5
180/180 [==============================] - 28s 154ms/step - loss: 0.1641 - accuracy: 0.9434 - val_loss: 0.1436 - val_accuracy: 0.9480
Epoch 4/5
180/180 [==============================] - 27s 150ms/step - loss: 0.1534 - accuracy: 0.9469 - val_loss: 0.1378 - val_accuracy: 0.9511
Epoch 5/5
180/180 [==============================] - 27s 149ms/step - loss: 0.1443 - accuracy: 0.9505 - val_loss: 0.1338 - val_accuracy: 0.9515
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;height&quot;:297,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="hcHUWUZhOR3-" data-outputId="bca7b6d3-721d-4038-b323-3ad3ff948726">
<div class="sourceCode" id="cb100"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1" aria-hidden="true" tabindex="-1"></a>show_history(history)</span></code></pre></div>
<div class="output display_data">
<p><img src="vertopal_5d270abade7b464aacf99cad3478e1a6/f68df414b9cf6ff18a3a33924a571a37cd489642.png" /></p>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="sVZraLaSOR3-" data-outputId="fc02731c-ba31-47e5-c031-c8e7a7cb12b3">
<div class="sourceCode" id="cb101"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test).ravel()</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> [<span class="dv">1</span> <span class="cf">if</span> x <span class="op">&gt;=</span> <span class="fl">0.5</span> <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> x <span class="kw">in</span> y_pred]</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Accuracy: </span><span class="sc">{</span>accuracy_score(y_true<span class="op">=</span>y_test, y_pred<span class="op">=</span>y_pred)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb101-4"><a href="#cb101-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;F1 Score: </span><span class="sc">{</span>f1_score(y_true<span class="op">=</span>y_test, y_pred<span class="op">=</span>y_pred)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb101-5"><a href="#cb101-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_true<span class="op">=</span>y_test, y_pred<span class="op">=</span>y_pred))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Accuracy: 0.9444705146253715
F1 Score: 0.5383615084525357
[[5831  114]
 [ 241  207]]
</code></pre>
</div>
</div>
<div class="cell markdown" id="chuT6-KkOR3-">
<h1 id="task-for-the-lecture">Task for the lecture</h1>
<ul>
<li>Try to create your own architecture</li>
<li>Experiment a little - try different batch sizes, optimimizers, time lags as features, etc</li>
<li>Send me a link to the Colab notebook with results and description of what you did and your final solution!</li>
</ul>
<h1 id="there-is-a-competition-for-bonus-points-this-week">There is a competition for bonus points this week!</h1>
<ul>
<li>Everyone who will send me a correct solution will be included in the F1 - Score toplist</li>
<li>Deadline for the competition submission is Monday 14th at 23:59</li>
<li>The toplist will be publicly available on Wednesday</li>
<li>There is no limitation in used layers (LSTM, CNN, ...), optimizers, etc.</li>
<li>You can use any model architecture from the internet including transfer learning,</li>
<li>The only limitation is that the model has to be trained/fine-tuned on Colab/Kaggle/Your machine so online sentiment scoring services are forbidden!</li>
</ul>
<h2 id="the-winner-with-the-best-f1---score-on-test-set-will-be-awarded-with-5-bonus-points">The winner with the best F1 - Score on test set will be awarded with 5 bonus points</h2>
<ul>
<li>The test set is the same as we used in the lecture</li>
</ul>
</div>
<section id="task-for-the-lecture" class="cell markdown" id="CBLg9rHKQuq0">
<h1>Task for the lecture</h1>
</section>
<section id="trainable-glove" class="cell markdown" id="G1lKG36aQ1WN">
<h2>Trainable glove</h2>
</section>
<div class="cell code" data-colab="{&quot;height&quot;:1000,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="HEVds-nzQ8WU" data-outputId="c22adac6-206c-4e1d-bd66-ccc00a8869de">
<div class="sourceCode" id="cb103"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>show_historyyer <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span>tf_string)</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a>x_v <span class="op">=</span> vect_layer(input_layer)</span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a>emb <span class="op">=</span> keras.layers.Embedding(num_tokens, embedding_dim, embeddings_initializer<span class="op">=</span>keras.initializers.Constant(embedding_matrix), trainable<span class="op">=</span><span class="va">True</span>)(x_v)</span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> LSTM(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, return_sequences<span class="op">=</span><span class="va">True</span>)(emb)</span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> GRU(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, return_sequences<span class="op">=</span><span class="va">False</span>)(x)</span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Flatten()(x)</span>
<span id="cb103-7"><a href="#cb103-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">64</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb103-8"><a href="#cb103-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">32</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb103-9"><a href="#cb103-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.2</span>)(x)</span>
<span id="cb103-10"><a href="#cb103-10" aria-hidden="true" tabindex="-1"></a>output_layer <span class="op">=</span> keras.layers.Dense(<span class="dv">1</span>, <span class="st">&#39;sigmoid&#39;</span>)(x)</span>
<span id="cb103-11"><a href="#cb103-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-12"><a href="#cb103-12" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.Model(input_layer, output_layer)</span>
<span id="cb103-13"><a href="#cb103-13" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb103-14"><a href="#cb103-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-15"><a href="#cb103-15" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;rmsprop&#39;</span>, loss<span class="op">=</span>keras.losses.BinaryCrossentropy(), metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb103-16"><a href="#cb103-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-17"><a href="#cb103-17" aria-hidden="true" tabindex="-1"></a>es <span class="op">=</span> keras.callbacks.EarlyStopping(monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, min_delta<span class="op">=</span><span class="dv">0</span>, patience<span class="op">=</span><span class="dv">70</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb103-18"><a href="#cb103-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-19"><a href="#cb103-19" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb103-20"><a href="#cb103-20" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb103-21"><a href="#cb103-21" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train.values, y_train.values, validation_data<span class="op">=</span>(X_valid.values, y_valid.values), callbacks<span class="op">=</span>[es], epochs<span class="op">=</span>epochs, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb103-22"><a href="#cb103-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-23"><a href="#cb103-23" aria-hidden="true" tabindex="-1"></a>show_history(history)</span>
<span id="cb103-24"><a href="#cb103-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-25"><a href="#cb103-25" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test).ravel()</span>
<span id="cb103-26"><a href="#cb103-26" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> [<span class="dv">1</span> <span class="cf">if</span> x <span class="op">&gt;=</span> <span class="fl">0.5</span> <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> x <span class="kw">in</span> y_pred]</span>
<span id="cb103-27"><a href="#cb103-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Accuracy: </span><span class="sc">{</span>accuracy_score(y_true<span class="op">=</span>y_test, y_pred<span class="op">=</span>y_pred)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb103-28"><a href="#cb103-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;F1 Score: </span><span class="sc">{</span>f1_score(y_true<span class="op">=</span>y_test, y_pred<span class="op">=</span>y_pred)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb103-29"><a href="#cb103-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_true<span class="op">=</span>y_test, y_pred<span class="op">=</span>y_pred))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_2 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Model: &quot;model_2&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1)]               0         
                                                                 
 text_vectorization_1 (TextV  (None, 20)               0         
 ectorization)                                                   
                                                                 
 embedding_2 (Embedding)     (None, 20, 50)            500100    
                                                                 
 lstm_2 (LSTM)               (None, 20, 64)            29440     
                                                                 
 gru_2 (GRU)                 (None, 64)                24960     
                                                                 
 flatten_2 (Flatten)         (None, 64)                0         
                                                                 
 dense_6 (Dense)             (None, 64)                4160      
                                                                 
 dense_7 (Dense)             (None, 32)                2080      
                                                                 
 dropout_2 (Dropout)         (None, 32)                0         
                                                                 
 dense_8 (Dense)             (None, 1)                 33        
                                                                 
=================================================================
Total params: 560,773
Trainable params: 560,773
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5
180/180 [==============================] - 32s 157ms/step - loss: 0.2137 - accuracy: 0.9319 - val_loss: 0.1526 - val_accuracy: 0.9445
Epoch 2/5
180/180 [==============================] - 28s 154ms/step - loss: 0.1543 - accuracy: 0.9456 - val_loss: 0.1417 - val_accuracy: 0.9511
Epoch 3/5
180/180 [==============================] - 28s 156ms/step - loss: 0.1335 - accuracy: 0.9519 - val_loss: 0.1307 - val_accuracy: 0.9554
Epoch 4/5
180/180 [==============================] - 28s 153ms/step - loss: 0.1167 - accuracy: 0.9588 - val_loss: 0.1162 - val_accuracy: 0.9589
Epoch 5/5
180/180 [==============================] - 28s 154ms/step - loss: 0.1034 - accuracy: 0.9622 - val_loss: 0.1122 - val_accuracy: 0.9609
Accuracy: 0.9558892538714219
F1 Score: 0.643939393939394
[[5856   89]
 [ 193  255]]
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_5d270abade7b464aacf99cad3478e1a6/c4797759171696eba5d9758e832c87c97186edeb.png" /></p>
</div>
</div>
<section id="porovnání-vyšlo" class="cell markdown" id="eowlPY6hSyC6">
<h1>Porovnání vyšlo</h1>
<ul>
<li>Učení ...0.5909943714821764</li>
<li>Neučení ... 0.5351351351351351</li>
</ul>
</section>
<section id="different-embedding-size" class="cell markdown" id="TMpChd67Q3mM">
<h2>Different embedding size</h2>
</section>
<div class="cell code" id="mg3M1K8zRUgs">
<div class="sourceCode" id="cb105"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb105-1"><a href="#cb105-1" aria-hidden="true" tabindex="-1"></a>from_embedding_size <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb105-2"><a href="#cb105-2" aria-hidden="true" tabindex="-1"></a>to_embedding_size <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb105-3"><a href="#cb105-3" aria-hidden="true" tabindex="-1"></a>step <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb105-4"><a href="#cb105-4" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> {}</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="g3eXWEetQ8po" data-outputId="5e60c0a5-0501-4751-fe1e-5ad839a09ce9">
<div class="sourceCode" id="cb106"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> embedding_dim <span class="kw">in</span> <span class="bu">range</span>(from_embedding_size, to_embedding_size, step):</span>
<span id="cb106-2"><a href="#cb106-2" aria-hidden="true" tabindex="-1"></a>  input_layer <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span>tf_string)</span>
<span id="cb106-3"><a href="#cb106-3" aria-hidden="true" tabindex="-1"></a>  x_v <span class="op">=</span> vect_layer(input_layer)</span>
<span id="cb106-4"><a href="#cb106-4" aria-hidden="true" tabindex="-1"></a>  emb <span class="op">=</span> keras.layers.Embedding(vocab_size, embedding_dim)(x_v)</span>
<span id="cb106-5"><a href="#cb106-5" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> LSTM(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, return_sequences<span class="op">=</span><span class="va">True</span>)(emb)</span>
<span id="cb106-6"><a href="#cb106-6" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> GRU(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, return_sequences<span class="op">=</span><span class="va">True</span>)(x)</span>
<span id="cb106-7"><a href="#cb106-7" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> keras.layers.Flatten()(x)</span>
<span id="cb106-8"><a href="#cb106-8" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> keras.layers.Dense(<span class="dv">64</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb106-9"><a href="#cb106-9" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> keras.layers.Dense(<span class="dv">32</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb106-10"><a href="#cb106-10" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.2</span>)(x)</span>
<span id="cb106-11"><a href="#cb106-11" aria-hidden="true" tabindex="-1"></a>  output_layer <span class="op">=</span> keras.layers.Dense(<span class="dv">1</span>, <span class="st">&#39;sigmoid&#39;</span>)(x)</span>
<span id="cb106-12"><a href="#cb106-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-13"><a href="#cb106-13" aria-hidden="true" tabindex="-1"></a>  model <span class="op">=</span> keras.Model(input_layer, output_layer)</span>
<span id="cb106-14"><a href="#cb106-14" aria-hidden="true" tabindex="-1"></a>  model.summary()</span>
<span id="cb106-15"><a href="#cb106-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-16"><a href="#cb106-16" aria-hidden="true" tabindex="-1"></a>  model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;rmsprop&#39;</span>, loss<span class="op">=</span>keras.losses.BinaryCrossentropy(), metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb106-17"><a href="#cb106-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-18"><a href="#cb106-18" aria-hidden="true" tabindex="-1"></a>  es <span class="op">=</span> keras.callbacks.EarlyStopping(monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, min_delta<span class="op">=</span><span class="dv">0</span>, patience<span class="op">=</span><span class="dv">70</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb106-19"><a href="#cb106-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-20"><a href="#cb106-20" aria-hidden="true" tabindex="-1"></a>  batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb106-21"><a href="#cb106-21" aria-hidden="true" tabindex="-1"></a>  epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb106-22"><a href="#cb106-22" aria-hidden="true" tabindex="-1"></a>  history <span class="op">=</span> model.fit(X_train.values, y_train.values, validation_data<span class="op">=</span>(X_valid.values, y_valid.values), callbacks<span class="op">=</span>[es], epochs<span class="op">=</span>epochs, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb106-23"><a href="#cb106-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-24"><a href="#cb106-24" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="op">=</span> model.predict(X_test).ravel()</span>
<span id="cb106-25"><a href="#cb106-25" aria-hidden="true" tabindex="-1"></a>  y_pred <span class="op">=</span> [<span class="dv">1</span> <span class="cf">if</span> x <span class="op">&gt;=</span> <span class="fl">0.5</span> <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> x <span class="kw">in</span> y_pred]</span>
<span id="cb106-26"><a href="#cb106-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-27"><a href="#cb106-27" aria-hidden="true" tabindex="-1"></a>  f1 <span class="op">=</span> f1_score(y_true<span class="op">=</span>y_test, y_pred<span class="op">=</span>y_pred)</span>
<span id="cb106-28"><a href="#cb106-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb106-29"><a href="#cb106-29" aria-hidden="true" tabindex="-1"></a>  res[embedding_dim] <span class="op">=</span> f1</span></code></pre></div>
<div class="output stream stdout">
<pre><code>WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_3 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Model: &quot;model_3&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_4 (InputLayer)        [(None, 1)]               0         
                                                                 
 text_vectorization_1 (TextV  (None, 20)               0         
 ectorization)                                                   
                                                                 
 embedding_3 (Embedding)     (None, 20, 30)            300000    
                                                                 
 lstm_3 (LSTM)               (None, 20, 64)            24320     
                                                                 
 gru_3 (GRU)                 (None, 20, 64)            24960     
                                                                 
 flatten_3 (Flatten)         (None, 1280)              0         
                                                                 
 dense_9 (Dense)             (None, 64)                81984     
                                                                 
 dense_10 (Dense)            (None, 32)                2080      
                                                                 
 dropout_3 (Dropout)         (None, 32)                0         
                                                                 
 dense_11 (Dense)            (None, 1)                 33        
                                                                 
=================================================================
Total params: 433,377
Trainable params: 433,377
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5
180/180 [==============================] - 32s 157ms/step - loss: 0.2099 - accuracy: 0.9351 - val_loss: 0.1366 - val_accuracy: 0.9539
Epoch 2/5
180/180 [==============================] - 28s 153ms/step - loss: 0.1212 - accuracy: 0.9603 - val_loss: 0.1120 - val_accuracy: 0.9648
Epoch 3/5
180/180 [==============================] - 28s 156ms/step - loss: 0.0983 - accuracy: 0.9681 - val_loss: 0.1050 - val_accuracy: 0.9644
Epoch 4/5
180/180 [==============================] - 29s 159ms/step - loss: 0.0868 - accuracy: 0.9720 - val_loss: 0.1187 - val_accuracy: 0.9664
Epoch 5/5
180/180 [==============================] - 29s 159ms/step - loss: 0.0788 - accuracy: 0.9742 - val_loss: 0.1044 - val_accuracy: 0.9671
WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_4 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Model: &quot;model_4&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 1)]               0         
                                                                 
 text_vectorization_1 (TextV  (None, 20)               0         
 ectorization)                                                   
                                                                 
 embedding_4 (Embedding)     (None, 20, 50)            500000    
                                                                 
 lstm_4 (LSTM)               (None, 20, 64)            29440     
                                                                 
 gru_4 (GRU)                 (None, 20, 64)            24960     
                                                                 
 flatten_4 (Flatten)         (None, 1280)              0         
                                                                 
 dense_12 (Dense)            (None, 64)                81984     
                                                                 
 dense_13 (Dense)            (None, 32)                2080      
                                                                 
 dropout_4 (Dropout)         (None, 32)                0         
                                                                 
 dense_14 (Dense)            (None, 1)                 33        
                                                                 
=================================================================
Total params: 638,497
Trainable params: 638,497
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5
180/180 [==============================] - 32s 158ms/step - loss: 0.2040 - accuracy: 0.9323 - val_loss: 0.1271 - val_accuracy: 0.9570
Epoch 2/5
180/180 [==============================] - 28s 154ms/step - loss: 0.1192 - accuracy: 0.9614 - val_loss: 0.1142 - val_accuracy: 0.9632
Epoch 3/5
180/180 [==============================] - 28s 156ms/step - loss: 0.0959 - accuracy: 0.9682 - val_loss: 0.1092 - val_accuracy: 0.9656
Epoch 4/5
180/180 [==============================] - 28s 154ms/step - loss: 0.0861 - accuracy: 0.9726 - val_loss: 0.1050 - val_accuracy: 0.9660
Epoch 5/5
180/180 [==============================] - 36s 203ms/step - loss: 0.0754 - accuracy: 0.9758 - val_loss: 0.1086 - val_accuracy: 0.9656
WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_5 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Model: &quot;model_5&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 1)]               0         
                                                                 
 text_vectorization_1 (TextV  (None, 20)               0         
 ectorization)                                                   
                                                                 
 embedding_5 (Embedding)     (None, 20, 70)            700000    
                                                                 
 lstm_5 (LSTM)               (None, 20, 64)            34560     
                                                                 
 gru_5 (GRU)                 (None, 20, 64)            24960     
                                                                 
 flatten_5 (Flatten)         (None, 1280)              0         
                                                                 
 dense_15 (Dense)            (None, 64)                81984     
                                                                 
 dense_16 (Dense)            (None, 32)                2080      
                                                                 
 dropout_5 (Dropout)         (None, 32)                0         
                                                                 
 dense_17 (Dense)            (None, 1)                 33        
                                                                 
=================================================================
Total params: 843,617
Trainable params: 843,617
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5
180/180 [==============================] - 35s 173ms/step - loss: 0.1944 - accuracy: 0.9363 - val_loss: 0.1196 - val_accuracy: 0.9605
Epoch 2/5
180/180 [==============================] - 31s 170ms/step - loss: 0.1092 - accuracy: 0.9641 - val_loss: 0.1210 - val_accuracy: 0.9613
Epoch 3/5
180/180 [==============================] - 36s 201ms/step - loss: 0.0915 - accuracy: 0.9714 - val_loss: 0.1060 - val_accuracy: 0.9652
Epoch 4/5
180/180 [==============================] - 31s 170ms/step - loss: 0.0791 - accuracy: 0.9748 - val_loss: 0.1069 - val_accuracy: 0.9660
Epoch 5/5
180/180 [==============================] - 31s 170ms/step - loss: 0.0727 - accuracy: 0.9761 - val_loss: 0.1444 - val_accuracy: 0.9523
WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_6 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Model: &quot;model_6&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_7 (InputLayer)        [(None, 1)]               0         
                                                                 
 text_vectorization_1 (TextV  (None, 20)               0         
 ectorization)                                                   
                                                                 
 embedding_6 (Embedding)     (None, 20, 90)            900000    
                                                                 
 lstm_6 (LSTM)               (None, 20, 64)            39680     
                                                                 
 gru_6 (GRU)                 (None, 20, 64)            24960     
                                                                 
 flatten_6 (Flatten)         (None, 1280)              0         
                                                                 
 dense_18 (Dense)            (None, 64)                81984     
                                                                 
 dense_19 (Dense)            (None, 32)                2080      
                                                                 
 dropout_6 (Dropout)         (None, 32)                0         
                                                                 
 dense_20 (Dense)            (None, 1)                 33        
                                                                 
=================================================================
Total params: 1,048,737
Trainable params: 1,048,737
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5
180/180 [==============================] - 36s 176ms/step - loss: 0.1997 - accuracy: 0.9370 - val_loss: 0.1456 - val_accuracy: 0.9578
Epoch 2/5
180/180 [==============================] - 30s 165ms/step - loss: 0.1150 - accuracy: 0.9620 - val_loss: 0.1130 - val_accuracy: 0.9609
Epoch 3/5
180/180 [==============================] - 30s 165ms/step - loss: 0.0921 - accuracy: 0.9703 - val_loss: 0.1165 - val_accuracy: 0.9648
Epoch 4/5
180/180 [==============================] - 30s 164ms/step - loss: 0.0805 - accuracy: 0.9731 - val_loss: 0.1039 - val_accuracy: 0.9660
Epoch 5/5
180/180 [==============================] - 30s 166ms/step - loss: 0.0756 - accuracy: 0.9754 - val_loss: 0.1154 - val_accuracy: 0.9664
WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_7 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Model: &quot;model_7&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_8 (InputLayer)        [(None, 1)]               0         
                                                                 
 text_vectorization_1 (TextV  (None, 20)               0         
 ectorization)                                                   
                                                                 
 embedding_7 (Embedding)     (None, 20, 110)           1100000   
                                                                 
 lstm_7 (LSTM)               (None, 20, 64)            44800     
                                                                 
 gru_7 (GRU)                 (None, 20, 64)            24960     
                                                                 
 flatten_7 (Flatten)         (None, 1280)              0         
                                                                 
 dense_21 (Dense)            (None, 64)                81984     
                                                                 
 dense_22 (Dense)            (None, 32)                2080      
                                                                 
 dropout_7 (Dropout)         (None, 32)                0         
                                                                 
 dense_23 (Dense)            (None, 1)                 33        
                                                                 
=================================================================
Total params: 1,253,857
Trainable params: 1,253,857
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5
180/180 [==============================] - 34s 168ms/step - loss: 0.1883 - accuracy: 0.9415 - val_loss: 0.1188 - val_accuracy: 0.9585
Epoch 2/5
180/180 [==============================] - 29s 160ms/step - loss: 0.1083 - accuracy: 0.9648 - val_loss: 0.1246 - val_accuracy: 0.9636
Epoch 3/5
180/180 [==============================] - 28s 153ms/step - loss: 0.0890 - accuracy: 0.9711 - val_loss: 0.1088 - val_accuracy: 0.9668
Epoch 4/5
180/180 [==============================] - 28s 154ms/step - loss: 0.0775 - accuracy: 0.9748 - val_loss: 0.1040 - val_accuracy: 0.9668
Epoch 5/5
180/180 [==============================] - 28s 154ms/step - loss: 0.0710 - accuracy: 0.9776 - val_loss: 0.1108 - val_accuracy: 0.9648
WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_8 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Model: &quot;model_8&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_9 (InputLayer)        [(None, 1)]               0         
                                                                 
 text_vectorization_1 (TextV  (None, 20)               0         
 ectorization)                                                   
                                                                 
 embedding_8 (Embedding)     (None, 20, 130)           1300000   
                                                                 
 lstm_8 (LSTM)               (None, 20, 64)            49920     
                                                                 
 gru_8 (GRU)                 (None, 20, 64)            24960     
                                                                 
 flatten_8 (Flatten)         (None, 1280)              0         
                                                                 
 dense_24 (Dense)            (None, 64)                81984     
                                                                 
 dense_25 (Dense)            (None, 32)                2080      
                                                                 
 dropout_8 (Dropout)         (None, 32)                0         
                                                                 
 dense_26 (Dense)            (None, 1)                 33        
                                                                 
=================================================================
Total params: 1,458,977
Trainable params: 1,458,977
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5
180/180 [==============================] - 33s 164ms/step - loss: 0.1910 - accuracy: 0.9378 - val_loss: 0.4213 - val_accuracy: 0.7876
Epoch 2/5
180/180 [==============================] - 30s 169ms/step - loss: 0.1109 - accuracy: 0.9631 - val_loss: 0.1125 - val_accuracy: 0.9625
Epoch 3/5
180/180 [==============================] - 30s 166ms/step - loss: 0.0896 - accuracy: 0.9712 - val_loss: 0.1085 - val_accuracy: 0.9660
Epoch 4/5
180/180 [==============================] - 29s 164ms/step - loss: 0.0774 - accuracy: 0.9751 - val_loss: 0.1062 - val_accuracy: 0.9652
Epoch 5/5
180/180 [==============================] - 28s 155ms/step - loss: 0.0696 - accuracy: 0.9775 - val_loss: 0.1076 - val_accuracy: 0.9660
WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_9 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Model: &quot;model_9&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_10 (InputLayer)       [(None, 1)]               0         
                                                                 
 text_vectorization_1 (TextV  (None, 20)               0         
 ectorization)                                                   
                                                                 
 embedding_9 (Embedding)     (None, 20, 150)           1500000   
                                                                 
 lstm_9 (LSTM)               (None, 20, 64)            55040     
                                                                 
 gru_9 (GRU)                 (None, 20, 64)            24960     
                                                                 
 flatten_9 (Flatten)         (None, 1280)              0         
                                                                 
 dense_27 (Dense)            (None, 64)                81984     
                                                                 
 dense_28 (Dense)            (None, 32)                2080      
                                                                 
 dropout_9 (Dropout)         (None, 32)                0         
                                                                 
 dense_29 (Dense)            (None, 1)                 33        
                                                                 
=================================================================
Total params: 1,664,097
Trainable params: 1,664,097
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5
180/180 [==============================] - 33s 157ms/step - loss: 0.1891 - accuracy: 0.9404 - val_loss: 0.1262 - val_accuracy: 0.9570
Epoch 2/5
180/180 [==============================] - 30s 165ms/step - loss: 0.1105 - accuracy: 0.9638 - val_loss: 0.1073 - val_accuracy: 0.9648
Epoch 3/5
180/180 [==============================] - 28s 158ms/step - loss: 0.0879 - accuracy: 0.9713 - val_loss: 0.1166 - val_accuracy: 0.9605
Epoch 4/5
180/180 [==============================] - 31s 171ms/step - loss: 0.0774 - accuracy: 0.9744 - val_loss: 0.1232 - val_accuracy: 0.9671
Epoch 5/5
180/180 [==============================] - 30s 167ms/step - loss: 0.0688 - accuracy: 0.9785 - val_loss: 0.1368 - val_accuracy: 0.9660
WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_10 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Model: &quot;model_10&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_11 (InputLayer)       [(None, 1)]               0         
                                                                 
 text_vectorization_1 (TextV  (None, 20)               0         
 ectorization)                                                   
                                                                 
 embedding_10 (Embedding)    (None, 20, 170)           1700000   
                                                                 
 lstm_10 (LSTM)              (None, 20, 64)            60160     
                                                                 
 gru_10 (GRU)                (None, 20, 64)            24960     
                                                                 
 flatten_10 (Flatten)        (None, 1280)              0         
                                                                 
 dense_30 (Dense)            (None, 64)                81984     
                                                                 
 dense_31 (Dense)            (None, 32)                2080      
                                                                 
 dropout_10 (Dropout)        (None, 32)                0         
                                                                 
 dense_32 (Dense)            (None, 1)                 33        
                                                                 
=================================================================
Total params: 1,869,217
Trainable params: 1,869,217
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5
180/180 [==============================] - 35s 174ms/step - loss: 0.1852 - accuracy: 0.9426 - val_loss: 0.1166 - val_accuracy: 0.9589
Epoch 2/5
180/180 [==============================] - 35s 197ms/step - loss: 0.1064 - accuracy: 0.9648 - val_loss: 0.1111 - val_accuracy: 0.9636
Epoch 3/5
180/180 [==============================] - 31s 174ms/step - loss: 0.0868 - accuracy: 0.9715 - val_loss: 0.1039 - val_accuracy: 0.9664
Epoch 4/5
180/180 [==============================] - 39s 217ms/step - loss: 0.0779 - accuracy: 0.9749 - val_loss: 0.1041 - val_accuracy: 0.9660
Epoch 5/5
180/180 [==============================] - 30s 168ms/step - loss: 0.0680 - accuracy: 0.9787 - val_loss: 0.1126 - val_accuracy: 0.9640
WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_11 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Model: &quot;model_11&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 1)]               0         
                                                                 
 text_vectorization_1 (TextV  (None, 20)               0         
 ectorization)                                                   
                                                                 
 embedding_11 (Embedding)    (None, 20, 190)           1900000   
                                                                 
 lstm_11 (LSTM)              (None, 20, 64)            65280     
                                                                 
 gru_11 (GRU)                (None, 20, 64)            24960     
                                                                 
 flatten_11 (Flatten)        (None, 1280)              0         
                                                                 
 dense_33 (Dense)            (None, 64)                81984     
                                                                 
 dense_34 (Dense)            (None, 32)                2080      
                                                                 
 dropout_11 (Dropout)        (None, 32)                0         
                                                                 
 dense_35 (Dense)            (None, 1)                 33        
                                                                 
=================================================================
Total params: 2,074,337
Trainable params: 2,074,337
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5
180/180 [==============================] - 35s 173ms/step - loss: 0.1960 - accuracy: 0.9379 - val_loss: 0.1185 - val_accuracy: 0.9570
Epoch 2/5
180/180 [==============================] - 29s 160ms/step - loss: 0.1095 - accuracy: 0.9634 - val_loss: 0.1220 - val_accuracy: 0.9632
Epoch 3/5
180/180 [==============================] - 30s 164ms/step - loss: 0.0882 - accuracy: 0.9716 - val_loss: 0.1264 - val_accuracy: 0.9660
Epoch 4/5
180/180 [==============================] - 29s 161ms/step - loss: 0.0776 - accuracy: 0.9751 - val_loss: 0.1196 - val_accuracy: 0.9679
Epoch 5/5
180/180 [==============================] - 31s 170ms/step - loss: 0.0699 - accuracy: 0.9780 - val_loss: 0.1241 - val_accuracy: 0.9585
</code></pre>
</div>
</div>
<div class="cell code" id="zshwnhNeSe8H">
<div class="sourceCode" id="cb108"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1" aria-hidden="true" tabindex="-1"></a>df_res <span class="op">=</span> pd.DataFrame.from_dict(res, orient<span class="op">=</span><span class="st">&#39;index&#39;</span>)</span>
<span id="cb108-2"><a href="#cb108-2" aria-hidden="true" tabindex="-1"></a>df_res.columns <span class="op">=</span> [<span class="st">&#39;f1&#39;</span>]</span>
<span id="cb108-3"><a href="#cb108-3" aria-hidden="true" tabindex="-1"></a>df_res[<span class="st">&#39;emb_size&#39;</span>] <span class="op">=</span> df_res.index</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;height&quot;:298,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="hmyAebfyljXo" data-outputId="887195ef-3baa-497c-92dd-13bcaacf6628">
<div class="sourceCode" id="cb109"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>sns.barplot(x<span class="op">=</span><span class="st">&quot;emb_size&quot;</span>, y<span class="op">=</span><span class="st">&quot;f1&quot;</span>, data<span class="op">=</span>df_res)</span></code></pre></div>
<div class="output execute_result" data-execution_count="80">
<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f05c4598910&gt;</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_5d270abade7b464aacf99cad3478e1a6/bd6da8efaac6edac06918e0a5d5e5234d0851434.png" /></p>
</div>
</div>
<div class="cell code" data-colab="{&quot;height&quot;:206,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="_IhKKmi9Sk_n" data-outputId="b6be12f7-f6b2-4428-ac58-9b32b0890522">
<div class="sourceCode" id="cb111"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb111-1"><a href="#cb111-1" aria-hidden="true" tabindex="-1"></a>df_res.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="79">

  <div id="df-1b43b0dc-5f91-4163-82f4-cffb0b343178">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>f1</th>
      <th>emb_size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>30</th>
      <td>0.682324</td>
      <td>30</td>
    </tr>
    <tr>
      <th>50</th>
      <td>0.677419</td>
      <td>50</td>
    </tr>
    <tr>
      <th>70</th>
      <td>0.628968</td>
      <td>70</td>
    </tr>
    <tr>
      <th>90</th>
      <td>0.666667</td>
      <td>90</td>
    </tr>
    <tr>
      <th>110</th>
      <td>0.686820</td>
      <td>110</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-1b43b0dc-5f91-4163-82f4-cffb0b343178')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-1b43b0dc-5f91-4163-82f4-cffb0b343178 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-1b43b0dc-5f91-4163-82f4-cffb0b343178');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<div class="cell code" id="N0VQZxlPkMfL">
<div class="sourceCode" id="cb112"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1" aria-hidden="true" tabindex="-1"></a>df_res.to_csv(<span class="st">&quot;saved_embeddings.csv&quot;</span>, sep<span class="op">=</span><span class="st">&#39;;&#39;</span>)</span></code></pre></div>
</div>
<section id="different-model" class="cell markdown" id="xjEzcvEvnzZy">
<h2>Different model</h2>
</section>
<div class="cell code" data-colab="{&quot;height&quot;:1000,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="_GKoPDDkn2GS" data-outputId="e33c0019-bb3b-4357-ff79-b8a2cd3f019b">
<div class="sourceCode" id="cb113"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a>show_historyyer <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span>tf_string)</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a>x_v <span class="op">=</span> vect_layer(input_layer)</span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a>emb <span class="op">=</span> keras.layers.Embedding(num_tokens, <span class="dv">50</span>)(x_v)</span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Flatten()(x)</span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">64</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb113-6"><a href="#cb113-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.2</span>)(x)</span>
<span id="cb113-7"><a href="#cb113-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">128</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb113-8"><a href="#cb113-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.3</span>)(x)</span>
<span id="cb113-9"><a href="#cb113-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">64</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb113-10"><a href="#cb113-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.2</span>)(x)</span>
<span id="cb113-11"><a href="#cb113-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">128</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb113-12"><a href="#cb113-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.2</span>)(x)</span>
<span id="cb113-13"><a href="#cb113-13" aria-hidden="true" tabindex="-1"></a>output_layer <span class="op">=</span> keras.layers.Dense(<span class="dv">1</span>, <span class="st">&#39;sigmoid&#39;</span>)(x)</span>
<span id="cb113-14"><a href="#cb113-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-15"><a href="#cb113-15" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.Model(input_layer, output_layer)</span>
<span id="cb113-16"><a href="#cb113-16" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb113-17"><a href="#cb113-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-18"><a href="#cb113-18" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;rmsprop&#39;</span>, loss<span class="op">=</span>keras.losses.BinaryCrossentropy(), metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb113-19"><a href="#cb113-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-20"><a href="#cb113-20" aria-hidden="true" tabindex="-1"></a>es <span class="op">=</span> keras.callbacks.EarlyStopping(monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, min_delta<span class="op">=</span><span class="dv">0</span>, patience<span class="op">=</span><span class="dv">70</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb113-21"><a href="#cb113-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-22"><a href="#cb113-22" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb113-23"><a href="#cb113-23" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb113-24"><a href="#cb113-24" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(X_train.values, y_train.values, validation_data<span class="op">=</span>(X_valid.values, y_valid.values), callbacks<span class="op">=</span>[es], epochs<span class="op">=</span>epochs, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb113-25"><a href="#cb113-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-26"><a href="#cb113-26" aria-hidden="true" tabindex="-1"></a>show_history(history)</span>
<span id="cb113-27"><a href="#cb113-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-28"><a href="#cb113-28" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test).ravel()</span>
<span id="cb113-29"><a href="#cb113-29" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> [<span class="dv">1</span> <span class="cf">if</span> x <span class="op">&gt;=</span> <span class="fl">0.5</span> <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> x <span class="kw">in</span> y_pred]</span>
<span id="cb113-30"><a href="#cb113-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;Accuracy: </span><span class="sc">{</span>accuracy_score(y_true<span class="op">=</span>y_test, y_pred<span class="op">=</span>y_pred)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb113-31"><a href="#cb113-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&#39;F1 Score: </span><span class="sc">{</span>f1_score(y_true<span class="op">=</span>y_test, y_pred<span class="op">=</span>y_pred)<span class="sc">}</span><span class="ss">&#39;</span>)</span>
<span id="cb113-32"><a href="#cb113-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_true<span class="op">=</span>y_test, y_pred<span class="op">=</span>y_pred))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;model_12&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_12 (InputLayer)       [(None, 1)]               0         
                                                                 
 text_vectorization_1 (TextV  (None, 20)               0         
 ectorization)                                                   
                                                                 
 embedding_11 (Embedding)    (None, 20, 190)           1900000   
                                                                 
 lstm_11 (LSTM)              (None, 20, 64)            65280     
                                                                 
 gru_11 (GRU)                (None, 20, 64)            24960     
                                                                 
 flatten_11 (Flatten)        (None, 1280)              0         
                                                                 
 dense_33 (Dense)            (None, 64)                81984     
                                                                 
 dense_34 (Dense)            (None, 32)                2080      
                                                                 
 dropout_11 (Dropout)        (None, 32)                0         
                                                                 
 flatten_12 (Flatten)        (None, 32)                0         
                                                                 
 dense_36 (Dense)            (None, 64)                2112      
                                                                 
 dropout_12 (Dropout)        (None, 64)                0         
                                                                 
 dense_37 (Dense)            (None, 128)               8320      
                                                                 
 dropout_13 (Dropout)        (None, 128)               0         
                                                                 
 dense_38 (Dense)            (None, 64)                8256      
                                                                 
 dropout_14 (Dropout)        (None, 64)                0         
                                                                 
 dense_39 (Dense)            (None, 128)               8320      
                                                                 
 dropout_15 (Dropout)        (None, 128)               0         
                                                                 
 dense_40 (Dense)            (None, 1)                 129       
                                                                 
=================================================================
Total params: 2,101,441
Trainable params: 2,101,441
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5
180/180 [==============================] - 36s 171ms/step - loss: 0.1167 - accuracy: 0.9670 - val_loss: 0.1458 - val_accuracy: 0.9664
Epoch 2/5
180/180 [==============================] - 30s 164ms/step - loss: 0.0655 - accuracy: 0.9798 - val_loss: 0.1117 - val_accuracy: 0.9660
Epoch 3/5
180/180 [==============================] - 31s 170ms/step - loss: 0.0554 - accuracy: 0.9832 - val_loss: 0.1197 - val_accuracy: 0.9632
Epoch 4/5
180/180 [==============================] - 30s 166ms/step - loss: 0.0514 - accuracy: 0.9848 - val_loss: 0.1414 - val_accuracy: 0.9628
Epoch 5/5
180/180 [==============================] - 30s 168ms/step - loss: 0.0420 - accuracy: 0.9875 - val_loss: 0.1659 - val_accuracy: 0.9589
Accuracy: 0.9524479899890506
F1 Score: 0.663716814159292
[[5789  156]
 [ 148  300]]
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_5d270abade7b464aacf99cad3478e1a6/fbf6c0f022a17c9c8be6dea0750fcfa026469669.png" /></p>
</div>
</div>
<div class="cell markdown" id="yrJN7q0Npf41">
<ul>
<li>Embedding s RNN - <strong>68 cca</strong></li>
<li>Embedding s Dense - <strong>66 cca</strong></li>
</ul>
</div>
<section id="there-is-a-competition-for-bonus-points-this-week" class="cell markdown" id="ICtoSUkAQyKf">
<h1>There is a competition for bonus points this week!</h1>
<ul>
<li>Try to create your own architecture task..</li>
</ul>
</section>
<div class="cell code" data-execution_count="36" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="roiYQrxXEEsS" data-outputId="587eaa82-8300-48ef-d080-68c5e54ca669">
<div class="sourceCode" id="cb115"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install gensim</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)
Requirement already satisfied: six&gt;=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)
Requirement already satisfied: smart-open&gt;=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)
Requirement already satisfied: scipy&gt;=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)
Requirement already satisfied: numpy&gt;=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="37" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="jD5l7fLfQ-Qe" data-outputId="c25e7c27-0658-422e-a868-b41115705655">
<div class="sourceCode" id="cb117"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a><span class="op">!</span>pip install transformers</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Collecting transformers
  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)
ent already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)
Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)
Collecting sacremoses
  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)
ent already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)
Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)
Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)
Collecting huggingface-hub&lt;1.0,&gt;=0.1.0
  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)
ent already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)
Collecting tokenizers!=0.11.3,&gt;=0.11.1
  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)
l&gt;=5.1
  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)
ent already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub&lt;1.0,&gt;=0.1.0-&gt;transformers) (3.10.0.2)
Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=20.0-&gt;transformers) (3.0.7)
Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers) (3.7.0)
Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (3.0.4)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (1.24.3)
Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2.10)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers) (2021.10.8)
Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (1.15.0)
Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (7.1.2)
Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers) (1.1.0)
Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers
  Attempting uninstall: pyyaml
    Found existing installation: PyYAML 3.13
    Uninstalling PyYAML-3.13:
      Successfully uninstalled PyYAML-3.13
Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="38" id="e5CeQtNPC7li">
<div class="sourceCode" id="cb119"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>df_transformers <span class="op">=</span> df.copy()</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="39" id="4_5grPzODXZw">
<div class="sourceCode" id="cb120"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1" aria-hidden="true" tabindex="-1"></a>LABEL_KEY <span class="op">=</span> <span class="st">&#39;Clean_text_1&#39;</span></span></code></pre></div>
</div>
<div class="cell code" data-execution_count="40" id="FQ14E_CrEPsb">
<div class="sourceCode" id="cb121"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb121-1"><a href="#cb121-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim.parsing.preprocessing <span class="im">import</span> remove_stopwords, preprocess_string</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="41" id="yWP5R2twC4AG">
<div class="sourceCode" id="cb122"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> super_normalization(text):</span>
<span id="cb122-2"><a href="#cb122-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="st">&quot; &quot;</span>.join(preprocess_string(text))</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="42" id="ezzB_yaLCvbC">
<div class="sourceCode" id="cb123"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1" aria-hidden="true" tabindex="-1"></a>df_transformers[LABEL_KEY] <span class="op">=</span> df_transformers.Clean_text.<span class="bu">apply</span>(super_normalization)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="43" data-colab="{&quot;height&quot;:389,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="jhazpwX_DUgj" data-outputId="7240b4aa-0cff-437a-ba3c-f661ab03610f">
<div class="sourceCode" id="cb124"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1" aria-hidden="true" tabindex="-1"></a>df_transformers.head()</span></code></pre></div>
<div class="output execute_result" data-execution_count="43">

  <div id="df-62751ae4-facf-4ada-846f-78481e59fa5d">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>label</th>
      <th>tweet</th>
      <th>length</th>
      <th>Words</th>
      <th>Words_normalized</th>
      <th>Words_normalized_no_user</th>
      <th>Words_normalized_no_user_fixed</th>
      <th>Clean_text</th>
      <th>Clean_text_1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>@user when a father is dysfunctional and is s...</td>
      <td>102</td>
      <td>[user, when, a, father, is, dysfunctional, and...</td>
      <td>[user, when, a, father, is, dysfunctional, and...</td>
      <td>[when, a, father, is, dysfunctional, and, is, ...</td>
      <td>[when, a, father, is, dysfunctional, and, is, ...</td>
      <td>when a father is dysfunctional and is so selfi...</td>
      <td>father dysfunct selfish drag kid dysfunct</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>0</td>
      <td>@user @user thanks for #lyft credit i can't us...</td>
      <td>122</td>
      <td>[user, user, thanks, for, lyft, credit, i, ca,...</td>
      <td>[user, user, thanks, for, lyft, credit, i, ca,...</td>
      <td>[thanks, for, lyft, credit, i, ca, n't, use, c...</td>
      <td>[thanks, for, lyft, credit, i, can't, use, cau...</td>
      <td>thanks for lyft credit i can't use cause they ...</td>
      <td>thank lyft credit us caus offer wheelchair van...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>0</td>
      <td>bihday your majesty</td>
      <td>21</td>
      <td>[bihday, your, majesty]</td>
      <td>[bihday, your, majesty]</td>
      <td>[bihday, your, majesty]</td>
      <td>[bihday, your]</td>
      <td>bihday your</td>
      <td>bihdai</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>0</td>
      <td>#model   i love u take with u all the time in ...</td>
      <td>86</td>
      <td>[model, i, love, u, take, with, u, all, the, t...</td>
      <td>[model, i, love, u, take, with, u, all, the, t...</td>
      <td>[model, i, love, u, take, with, u, all, the, t...</td>
      <td>[model, i, love, u, take, with, u, all, the, t...</td>
      <td>model i love u take with u all the time in</td>
      <td>model love time</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>0</td>
      <td>factsguide: society now    #motivation</td>
      <td>39</td>
      <td>[factsguide, society, now, motivation]</td>
      <td>[factsguide, society, now, motivation]</td>
      <td>[factsguide, society, now, motivation]</td>
      <td>[factsguide, society, now]</td>
      <td>factsguide society now</td>
      <td>factsguid societi</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-62751ae4-facf-4ada-846f-78481e59fa5d')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-62751ae4-facf-4ada-846f-78481e59fa5d button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-62751ae4-facf-4ada-846f-78481e59fa5d');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
<div class="cell code" data-execution_count="44" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="keBWlKWKCjLk" data-outputId="8b1f9290-1507-497a-e80a-90041c56d392">
<div class="sourceCode" id="cb125"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1" aria-hidden="true" tabindex="-1"></a>SELECTION_KEY <span class="op">=</span> <span class="st">&quot;Clean_text&quot;</span></span>
<span id="cb125-2"><a href="#cb125-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-3"><a href="#cb125-3" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(df_transformers[SELECTION_KEY], df_transformers.label, test_size<span class="op">=</span><span class="fl">0.20</span>, random_state<span class="op">=</span><span class="dv">13</span>, stratify<span class="op">=</span>df_transformers.label)</span>
<span id="cb125-4"><a href="#cb125-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-5"><a href="#cb125-5" aria-hidden="true" tabindex="-1"></a>X_train, X_valid, y_train, y_valid <span class="op">=</span> train_test_split(X_train, y_train, test_size<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">13</span>, stratify<span class="op">=</span>y_train)</span>
<span id="cb125-6"><a href="#cb125-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb125-7"><a href="#cb125-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X_train.shape, X_test.shape)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>(23012,) (6393,)
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="45" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="m8jCMoHrE4F1" data-outputId="f61b150d-70a1-47a2-ff2d-c09094eb4976">
<div class="sourceCode" id="cb127"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1" aria-hidden="true" tabindex="-1"></a>df_transformers.label.value_counts()</span>
<span id="cb127-2"><a href="#cb127-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb127-3"><a href="#cb127-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 0 = positive</span></span>
<span id="cb127-4"><a href="#cb127-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 1 = negative</span></span></code></pre></div>
<div class="output execute_result" data-execution_count="45">
<pre><code>0    29720
1     2242
Name: label, dtype: int64</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="46" id="uxSyPZ-LkF1B">
<div class="sourceCode" id="cb129"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1" aria-hidden="true" tabindex="-1"></a>res_transformers <span class="op">=</span> {}</span></code></pre></div>
</div>
<section id="twitter-roberta-base-for-sentiment-analysis" class="cell markdown" id="m014nTeTj6dc">
<h1>Twitter-roBERTa-base for Sentiment Analysis</h1>
</section>
<div class="cell code" id="CW59SXTZkC7j">
<div class="sourceCode" id="cb130"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1" aria-hidden="true" tabindex="-1"></a>name_1 <span class="op">=</span> <span class="st">&quot;Twitter-roBERTa-base for Sentiment Analysis&quot;</span></span></code></pre></div>
</div>
<div class="cell markdown" id="jm5DO6WbFSJs">
<p><a href="https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment?text=model+i+love+u+take+with+u+all+the+time+in" class="uri">https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment?text=model+i+love+u+take+with+u+all+the+time+in</a></p>
</div>
<div class="cell code" data-execution_count="47" id="p-3-1IE6GOox">
<div class="sourceCode" id="cb131"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb131-1"><a href="#cb131-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoModelForSequenceClassification</span>
<span id="cb131-2"><a href="#cb131-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> TFAutoModelForSequenceClassification</span>
<span id="cb131-3"><a href="#cb131-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb131-4"><a href="#cb131-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb131-5"><a href="#cb131-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.special <span class="im">import</span> softmax</span>
<span id="cb131-6"><a href="#cb131-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> Counter</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;height&quot;:266,&quot;referenced_widgets&quot;:[&quot;02c3a590f9e74390a15b88f2ff9535e8&quot;,&quot;930575a7b79b4aa29c01c0f2299f99f8&quot;,&quot;1e71b9d62ea6466984e4a6c5734cd3ba&quot;,&quot;63f1ea2292f74f7188c65f46acdf3887&quot;,&quot;ac9ce7fe98ce4f62b20daaa3a5cc9d90&quot;,&quot;b2019d93d2d24b098bde40a9876897a7&quot;,&quot;55e00945e89a49c9bff5a72fa4916688&quot;,&quot;598dd30eb4c4429f871eddc89d682ef0&quot;,&quot;b53d07f182d14dddb47bcdfedf8c368b&quot;,&quot;c329e98d061a4dc081d268521f6c86b4&quot;,&quot;f8f5f0566b8d46c99639b18756999e47&quot;,&quot;fb1ec0e989e44d31a940e5e227fb001c&quot;,&quot;6d6f02b39bce464093f0796a5ab0d322&quot;,&quot;1bae26db4b204c34b50fc88834ed9b4f&quot;,&quot;9e4bc2c7465d469da44ddde0794eb5b5&quot;,&quot;cd324a8252254ed4a682564a08e0fa49&quot;,&quot;881a3bf3969742abbbd313650e5255cf&quot;,&quot;618b53b19f034a218732b10ad6b8d715&quot;,&quot;3d555524ea1b42c9ba8f473c6d018c30&quot;,&quot;35669f1e5c254d43a5a7d580f991af3d&quot;,&quot;d9aa4f12bd2f4f40a75a13fa63786eb1&quot;,&quot;426c9decaf2b4654821813afacdfcdd1&quot;,&quot;b963f85cf1ab4ca5898fce2fc833a2a0&quot;,&quot;019f1aa85afc4b8e8eefb989cb51b316&quot;,&quot;223ccf30bd5a4838bbd3a878d52f2c5f&quot;,&quot;d7e35b47116c48a5b1b619ca2138aa89&quot;,&quot;3b413a23e25b46f9895b8ada6263ddb2&quot;,&quot;e8af29e8992c42c393e30ff8a9e062de&quot;,&quot;6136476a36ab4db584fd225b13d11f59&quot;,&quot;a0fa324e55e4412580ebf8032ae98ca2&quot;,&quot;09b556f1f1aa40f7a22194dff005af46&quot;,&quot;f131652e20134c42891e567f7664ab3b&quot;,&quot;140fcf40286046e99bda4dacd3c5d87d&quot;,&quot;0a0a7cfd55994f0f9751bc7bbd7a5fcd&quot;,&quot;087583d157244012837021754318c1f2&quot;,&quot;a69b52a8d9884d33a63f90e84301c53e&quot;,&quot;f2184639cdf141c289969a5380ce9b9c&quot;,&quot;84f429a6db564e67b44e99213db989a3&quot;,&quot;8655b737ec974df685ce14ffcc81bce0&quot;,&quot;cf75eedba484495b9e56b2deec91ebfe&quot;,&quot;55d0b83ce6c447079e319e89864a8e12&quot;,&quot;3963017effb84467a0f137acf5027ec6&quot;,&quot;86019b6e17664f549c5c61c0c1164cb0&quot;,&quot;be1e56810275419ba90539a14e010649&quot;,&quot;0bb7546039d64769ae5737b819fbb912&quot;,&quot;827921ca10bf4e8ea364420022deb47e&quot;,&quot;ff958f5cf9af478395c2071d9422cc1a&quot;,&quot;d1603ac34a3b4fc786b6f21ebddaf09f&quot;,&quot;9f61bc56918f47f1972e9c7d0aced35f&quot;,&quot;2aa6cea622ca445a949cfa4ed1b1f03f&quot;,&quot;c4e1170c45eb48e594dc7043125bad1c&quot;,&quot;2cfa6bcc112c464389fff9e07ecba7fe&quot;,&quot;d7810966adc34d979cdf3ce8374829b9&quot;,&quot;fb7d57cbedba4bbb989c1456c743f4c3&quot;,&quot;cf86ca81acd044739c10eb570286d77f&quot;],&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="-nJw9-k3FRf-" data-outputId="2ccea671-b94e-4205-ba48-dfc19ace5c8e">
<div class="sourceCode" id="cb132"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Labels: </span></span>
<span id="cb132-2"><a href="#cb132-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 0 -&gt; Negative; </span></span>
<span id="cb132-3"><a href="#cb132-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 1 -&gt; Neutral; </span></span>
<span id="cb132-4"><a href="#cb132-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 2 -&gt; Positive</span></span>
<span id="cb132-5"><a href="#cb132-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-6"><a href="#cb132-6" aria-hidden="true" tabindex="-1"></a>task<span class="op">=</span><span class="st">&#39;sentiment&#39;</span></span>
<span id="cb132-7"><a href="#cb132-7" aria-hidden="true" tabindex="-1"></a>MODEL <span class="op">=</span> <span class="ss">f&quot;cardiffnlp/twitter-roberta-base-</span><span class="sc">{</span>task<span class="sc">}</span><span class="ss">&quot;</span></span>
<span id="cb132-8"><a href="#cb132-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-9"><a href="#cb132-9" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(MODEL)</span>
<span id="cb132-10"><a href="#cb132-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb132-11"><a href="#cb132-11" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> TFAutoModelForSequenceClassification.from_pretrained(MODEL)</span></code></pre></div>
<div class="output display_data">
<div class="sourceCode" id="cb133"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb133-1"><a href="#cb133-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">,</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;02c3a590f9e74390a15b88f2ff9535e8&quot;</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb134"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb134-1"><a href="#cb134-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">,</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;fb1ec0e989e44d31a940e5e227fb001c&quot;</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb135"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb135-1"><a href="#cb135-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">,</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;b963f85cf1ab4ca5898fce2fc833a2a0&quot;</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb136"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb136-1"><a href="#cb136-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">,</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;0a0a7cfd55994f0f9751bc7bbd7a5fcd&quot;</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb137"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb137-1"><a href="#cb137-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">,</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;0bb7546039d64769ae5737b819fbb912&quot;</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output stream stderr">
<pre><code>All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.

All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="u3BvZbWFG0RX" data-outputId="6023d545-623e-4c16-b304-597bb6490654">
<div class="sourceCode" id="cb139"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1" aria-hidden="true" tabindex="-1"></a>model.summary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Model: &quot;tf_roberta_for_sequence_classification&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 roberta (TFRobertaMainLayer  multiple                 124055040 
 )                                                               
                                                                 
 classifier (TFRobertaClassi  multiple                 592899    
 ficationHead)                                                   
                                                                 
=================================================================
Total params: 124,647,939
Trainable params: 124,647,939
Non-trainable params: 0
_________________________________________________________________
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="LCxgwYcWkbFy" data-outputId="ef6707dc-87e4-43dd-ce29-2ff1c5c2217d">
<div class="sourceCode" id="cb141"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb141-1"><a href="#cb141-1" aria-hidden="true" tabindex="-1"></a>X_test.shape</span></code></pre></div>
<div class="output execute_result" data-execution_count="91">
<pre><code>(6393,)</code></pre>
</div>
</div>
<div class="cell code" id="7y6hNG6GHZqd">
<div class="sourceCode" id="cb143"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb143-1"><a href="#cb143-1" aria-hidden="true" tabindex="-1"></a>pred_sentences <span class="op">=</span> <span class="bu">list</span>(X_test.values)[<span class="dv">0</span>:<span class="dv">1000</span>]</span>
<span id="cb143-2"><a href="#cb143-2" aria-hidden="true" tabindex="-1"></a>y_test_transformer <span class="op">=</span> y_test[<span class="dv">0</span>:<span class="dv">1000</span>]</span></code></pre></div>
</div>
<div class="cell code" id="Rzh5wonFHcR9">
<div class="sourceCode" id="cb144"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1" aria-hidden="true" tabindex="-1"></a><span class="co">#longest has 42</span></span>
<span id="cb144-2"><a href="#cb144-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb144-3"><a href="#cb144-3" aria-hidden="true" tabindex="-1"></a>tf_batch <span class="op">=</span> tokenizer(pred_sentences, max_length<span class="op">=</span><span class="dv">30</span>, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>, return_tensors<span class="op">=</span><span class="st">&#39;tf&#39;</span>)</span></code></pre></div>
</div>
<div class="cell code" id="ddzDyg6vRX8n">
<div class="sourceCode" id="cb145"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb145-1"><a href="#cb145-1" aria-hidden="true" tabindex="-1"></a>tf_outputs <span class="op">=</span> model(tf_batch)</span>
<span id="cb145-2"><a href="#cb145-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb145-3"><a href="#cb145-3" aria-hidden="true" tabindex="-1"></a>tf_predictions <span class="op">=</span> tf.nn.softmax(</span>
<span id="cb145-4"><a href="#cb145-4" aria-hidden="true" tabindex="-1"></a>    tf_outputs[<span class="dv">0</span>], </span>
<span id="cb145-5"><a href="#cb145-5" aria-hidden="true" tabindex="-1"></a>    axis<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb145-6"><a href="#cb145-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="48" id="6s4Ru7iYU4Yq">
<div class="sourceCode" id="cb146"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb146-1"><a href="#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_labels(predictions):</span>
<span id="cb146-2"><a href="#cb146-2" aria-hidden="true" tabindex="-1"></a>  negative <span class="op">=</span> predictions[:, <span class="dv">0</span>]</span>
<span id="cb146-3"><a href="#cb146-3" aria-hidden="true" tabindex="-1"></a>  positive <span class="op">=</span> predictions[:, <span class="dv">2</span>]</span>
<span id="cb146-4"><a href="#cb146-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> np.column_stack((positive, negative))</span></code></pre></div>
</div>
<div class="cell code" id="JCzzKRaKVCv9">
<div class="sourceCode" id="cb147"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb147-1"><a href="#cb147-1" aria-hidden="true" tabindex="-1"></a>predictions_without_neutral <span class="op">=</span> get_labels(tf_predictions)</span>
<span id="cb147-2"><a href="#cb147-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb147-3"><a href="#cb147-3" aria-hidden="true" tabindex="-1"></a>predicted <span class="op">=</span> tf.argmax(</span>
<span id="cb147-4"><a href="#cb147-4" aria-hidden="true" tabindex="-1"></a>    predictions_without_neutral, </span>
<span id="cb147-5"><a href="#cb147-5" aria-hidden="true" tabindex="-1"></a>    axis<span class="op">=</span><span class="dv">1</span></span>
<span id="cb147-6"><a href="#cb147-6" aria-hidden="true" tabindex="-1"></a>).numpy()</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="CG5RjToTUi5-" data-outputId="d87e265d-e8cb-47ab-dd20-0c600ece0a78">
<div class="sourceCode" id="cb148"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb148-1"><a href="#cb148-1" aria-hidden="true" tabindex="-1"></a>predicted</span></code></pre></div>
<div class="output execute_result" data-execution_count="104">
<pre><code>array([0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1,
       0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,
       0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0,
       1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
       0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0,
       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,
       0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,
       1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,
       0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,
       1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,
       0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,
       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
       0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1,
       1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,
       0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,
       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0,
       1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,
       0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,
       1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0,
       0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,
       1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,
       0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,
       0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,
       1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
       1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,
       1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1,
       1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,
       1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,
       0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0,
       1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,
       0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,
       0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,
       1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0,
       1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
       1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,
       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,
       1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1,
       0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0,
       0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,
       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,
       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,
       0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,
       0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,
       1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,
       1, 0, 0, 0, 1, 0, 0, 0, 1, 1])</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="49" id="2nF1e_loQRFe">
<div class="sourceCode" id="cb150"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb150-1"><a href="#cb150-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_y_true_y_pred(y_test, y_predicted):</span>
<span id="cb150-2"><a href="#cb150-2" aria-hidden="true" tabindex="-1"></a>   f1 <span class="op">=</span> f1_score(y_true<span class="op">=</span>y_test, y_pred<span class="op">=</span>y_predicted)</span>
<span id="cb150-3"><a href="#cb150-3" aria-hidden="true" tabindex="-1"></a>   <span class="cf">return</span> f1</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;height&quot;:165,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="TOJII5ZHQbln" data-outputId="a19fbacb-b240-4521-dbc3-a5801b113410">
<div class="sourceCode" id="cb151"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1" aria-hidden="true" tabindex="-1"></a>f1_1 <span class="op">=</span> get_y_true_y_pred(y_test_transformer, predicted)</span></code></pre></div>
<div class="output error" data-ename="NameError" data-evalue="ignored">
<pre><code>---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
&lt;ipython-input-48-ff64bd578744&gt; in &lt;module&gt;()
----&gt; 1 f1_1 = get_y_true_y_pred(y_test_transformer, predicted)

NameError: name &#39;y_test_transformer&#39; is not defined
</code></pre>
</div>
</div>
<div class="cell code" id="fpoBhjBxkKE6">
<div class="sourceCode" id="cb153"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a>res_transformers[name_1] <span class="op">=</span> f1_1</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="102" id="Y5Mru03cnTC0">
<div class="sourceCode" id="cb154"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb154-1"><a href="#cb154-1" aria-hidden="true" tabindex="-1"></a>MAX_SEQUENCE_LENGTH <span class="op">=</span> <span class="dv">40</span></span>
<span id="cb154-2"><a href="#cb154-2" aria-hidden="true" tabindex="-1"></a>MODEL_NAME <span class="op">=</span> <span class="st">&#39;distilbert-base-uncased&#39;</span></span></code></pre></div>
</div>
<div class="cell code" data-execution_count="51" id="d9B2wI3cno8V">
<div class="sourceCode" id="cb155"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb155-1"><a href="#cb155-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> (</span>
<span id="cb155-2"><a href="#cb155-2" aria-hidden="true" tabindex="-1"></a>    DistilBertTokenizerFast,</span>
<span id="cb155-3"><a href="#cb155-3" aria-hidden="true" tabindex="-1"></a>    TFDistilBertModel,</span>
<span id="cb155-4"><a href="#cb155-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="52" data-colab="{&quot;height&quot;:145,&quot;referenced_widgets&quot;:[&quot;7725d714f6d644cdb041499e102a071c&quot;,&quot;d15e6cc180ca4de89887a0ea9d042645&quot;,&quot;23d140b0e59c418a918573ca2cd8aecf&quot;,&quot;9addf586c997455d8f6879289de8ae44&quot;,&quot;4b8200a621264e8d923eaa77d0a42bb1&quot;,&quot;c7a3dd9bd35f4b608a81c409a86579e7&quot;,&quot;7112274cb76d47038f31c71c491f3e83&quot;,&quot;33e0dc734d58474eac0da58f72e093a3&quot;,&quot;6f95cd5625184a5ca829dcfbbae67cef&quot;,&quot;ef34be61f9bf41a0b8abedd625f7a62a&quot;,&quot;e8819e61117f46af92296d5028701d5b&quot;,&quot;63f864be50af402bb429dadc14b818c1&quot;,&quot;194ad5f1f14441d49b4366d37e5feaf0&quot;,&quot;624d240f97db4d03bc8eb3d6f7f2a89c&quot;,&quot;5264ba64fc41481d96986401c8c92864&quot;,&quot;2c5d2538afc64229848dad5252224aa3&quot;,&quot;d15202f63b2e448e930e373675addda1&quot;,&quot;76c3edfdd18d4a25a0879dcb8a653626&quot;,&quot;0a60cdce2f1f4132b7b109b3719e6d6c&quot;,&quot;2122043580204a3f9d77605c0d86e403&quot;,&quot;ed591c4d9e5349418d095f9559bf9d80&quot;,&quot;53400a9cf89f48aea8d558e7fbeea868&quot;,&quot;6b841ba2f8fe45b49a10db7eefecb2b8&quot;,&quot;482d37b4021f4eb6afcc6da8b21df7b8&quot;,&quot;90bd3622ec954087b33f212a80196787&quot;,&quot;054250412c1141338dea10b48d483b5e&quot;,&quot;13ac65735dc34495ba518ee581aff902&quot;,&quot;fc628727d1dd423ba3ed04242fdb413b&quot;,&quot;dcb9547298ce47b1877783d9d1c9b388&quot;,&quot;4952ce6a271c4aefb0786b1965f32cca&quot;,&quot;2874f674a73a41838d44b92beac4a2ea&quot;,&quot;130648a817f04d0da59a4eacec4a5502&quot;,&quot;7e7473707dac466ea0fb727e8c319b1f&quot;,&quot;67ffd56ab4cc49bf9e2ea985fe780987&quot;,&quot;adef031b6b3841d9984e4d047ee6697f&quot;,&quot;c2542e68e1064146af18ab2c40de40d0&quot;,&quot;21556941a2e749748e81df9c9f3acf06&quot;,&quot;190d72930fa04410b2f843b6af13ac21&quot;,&quot;a9672bcf4ff445c6977129978f678aad&quot;,&quot;6e0e3a0bbd9f49c7a283e1cf0f626be5&quot;,&quot;74ed1a5723c041b3a065a51afba1e89a&quot;,&quot;474fdb27497144bea6c07cb9c63ce1c2&quot;,&quot;c60c8844817647ce8b9c7cd7f7440359&quot;,&quot;c5b1efec570d42e188f3649708b66bed&quot;],&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="UMXg8ObxnkA4" data-outputId="eef4ce69-0d35-4432-cbb9-a8896e9aaa42">
<div class="sourceCode" id="cb156"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb156-1"><a href="#cb156-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> DistilBertTokenizerFast.from_pretrained(MODEL_NAME)</span>
<span id="cb156-2"><a href="#cb156-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb156-3"><a href="#cb156-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb156-4"><a href="#cb156-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize(sentences, max_length<span class="op">=</span>MAX_SEQUENCE_LENGTH, padding<span class="op">=</span><span class="st">&#39;max_length&#39;</span>):</span>
<span id="cb156-5"><a href="#cb156-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer(</span>
<span id="cb156-6"><a href="#cb156-6" aria-hidden="true" tabindex="-1"></a>        sentences,</span>
<span id="cb156-7"><a href="#cb156-7" aria-hidden="true" tabindex="-1"></a>        truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb156-8"><a href="#cb156-8" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span>padding,</span>
<span id="cb156-9"><a href="#cb156-9" aria-hidden="true" tabindex="-1"></a>        max_length<span class="op">=</span>max_length,</span>
<span id="cb156-10"><a href="#cb156-10" aria-hidden="true" tabindex="-1"></a>        return_tensors<span class="op">=</span><span class="st">&quot;tf&quot;</span></span>
<span id="cb156-11"><a href="#cb156-11" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
<div class="output display_data">
<div class="sourceCode" id="cb157"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">,</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;7725d714f6d644cdb041499e102a071c&quot;</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb158"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb158-1"><a href="#cb158-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">,</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;63f864be50af402bb429dadc14b818c1&quot;</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb159"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb159-1"><a href="#cb159-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">,</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;6b841ba2f8fe45b49a10db7eefecb2b8&quot;</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb160"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb160-1"><a href="#cb160-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">,</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;67ffd56ab4cc49bf9e2ea985fe780987&quot;</span><span class="fu">}</span></span></code></pre></div>
</div>
</div>
<div class="cell code" data-execution_count="53" id="_nQF9N2yqaRG">
<div class="sourceCode" id="cb161"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb161-1"><a href="#cb161-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_raw_ds(input_df):</span>
<span id="cb161-2"><a href="#cb161-2" aria-hidden="true" tabindex="-1"></a>  copy_df <span class="op">=</span> input_df.copy()</span>
<span id="cb161-3"><a href="#cb161-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-4"><a href="#cb161-4" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(copy_df.label.value_counts())</span>
<span id="cb161-5"><a href="#cb161-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-6"><a href="#cb161-6" aria-hidden="true" tabindex="-1"></a>  SELECTION_KEY <span class="op">=</span> <span class="st">&quot;Clean_text&quot;</span></span>
<span id="cb161-7"><a href="#cb161-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-8"><a href="#cb161-8" aria-hidden="true" tabindex="-1"></a>  X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(copy_df[SELECTION_KEY], copy_df.label, test_size<span class="op">=</span><span class="fl">0.20</span>, random_state<span class="op">=</span><span class="dv">13</span>, stratify<span class="op">=</span>copy_df.label)</span>
<span id="cb161-9"><a href="#cb161-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-10"><a href="#cb161-10" aria-hidden="true" tabindex="-1"></a>  X_train, X_valid, y_train, y_valid <span class="op">=</span> train_test_split(X_train, y_train, test_size<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">13</span>, stratify<span class="op">=</span>y_train)</span>
<span id="cb161-11"><a href="#cb161-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb161-12"><a href="#cb161-12" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> X_train, y_train, X_valid, y_valid, X_test, y_test</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="54" id="8j8MnLoP1Uiy">
<div class="sourceCode" id="cb162"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb162-1"><a href="#cb162-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> balance(input_df):</span>
<span id="cb162-2"><a href="#cb162-2" aria-hidden="true" tabindex="-1"></a>  copy_df <span class="op">=</span> input_df.copy()</span>
<span id="cb162-3"><a href="#cb162-3" aria-hidden="true" tabindex="-1"></a>  positive <span class="op">=</span> copy_df[copy_df.label <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="cb162-4"><a href="#cb162-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-5"><a href="#cb162-5" aria-hidden="true" tabindex="-1"></a>  negative <span class="op">=</span> copy_df[copy_df.label <span class="op">==</span> <span class="dv">1</span>]</span>
<span id="cb162-6"><a href="#cb162-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-7"><a href="#cb162-7" aria-hidden="true" tabindex="-1"></a>  number_times <span class="op">=</span> <span class="bu">int</span>(<span class="bu">len</span>(positive) <span class="op">/</span> <span class="bu">len</span>(negative))</span>
<span id="cb162-8"><a href="#cb162-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb162-9"><a href="#cb162-9" aria-hidden="true" tabindex="-1"></a>  negative <span class="op">=</span> negative.sample(n<span class="op">=</span><span class="bu">len</span>(negative)<span class="op">*</span>number_times, replace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb162-10"><a href="#cb162-10" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb162-11"><a href="#cb162-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> pd.concat([negative, positive])</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="55" id="sTVgvclpqcfn">
<div class="sourceCode" id="cb163"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb163-1"><a href="#cb163-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_balanced_ds(input_df):</span>
<span id="cb163-2"><a href="#cb163-2" aria-hidden="true" tabindex="-1"></a>  SELECTION_KEY <span class="op">=</span> <span class="st">&quot;Clean_text&quot;</span></span>
<span id="cb163-3"><a href="#cb163-3" aria-hidden="true" tabindex="-1"></a>  copy_df <span class="op">=</span> input_df.copy()</span>
<span id="cb163-4"><a href="#cb163-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-5"><a href="#cb163-5" aria-hidden="true" tabindex="-1"></a>  X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(copy_df[SELECTION_KEY], copy_df.label, test_size<span class="op">=</span><span class="fl">0.20</span>, random_state<span class="op">=</span><span class="dv">13</span>, stratify<span class="op">=</span>copy_df.label)</span>
<span id="cb163-6"><a href="#cb163-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-7"><a href="#cb163-7" aria-hidden="true" tabindex="-1"></a>  x <span class="op">=</span> pd.DataFrame()</span>
<span id="cb163-8"><a href="#cb163-8" aria-hidden="true" tabindex="-1"></a>  x[SELECTION_KEY] <span class="op">=</span> X_train</span>
<span id="cb163-9"><a href="#cb163-9" aria-hidden="true" tabindex="-1"></a>  x[<span class="st">&#39;label&#39;</span>] <span class="op">=</span> y_train</span>
<span id="cb163-10"><a href="#cb163-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-11"><a href="#cb163-11" aria-hidden="true" tabindex="-1"></a>  copy_df <span class="op">=</span> balance(x)</span>
<span id="cb163-12"><a href="#cb163-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-13"><a href="#cb163-13" aria-hidden="true" tabindex="-1"></a>  X_train, X_valid, y_train, y_valid <span class="op">=</span> train_test_split(copy_df[SELECTION_KEY], copy_df.label, test_size<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">13</span>, stratify<span class="op">=</span>copy_df.label)</span>
<span id="cb163-14"><a href="#cb163-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb163-15"><a href="#cb163-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> X_train, y_train, X_valid, y_valid, X_test, y_test</span></code></pre></div>
</div>
<section id="gru-with-glove" class="cell markdown" id="t3HmwEMDZkEm">
<h1>GRU with Glove</h1>
</section>
<div class="cell code" id="gnKo1AUv9MDT">
<div class="sourceCode" id="cb164"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb164-1"><a href="#cb164-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim.downloader <span class="im">as</span> api</span>
<span id="cb164-2"><a href="#cb164-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gzip</span></code></pre></div>
</div>
<div class="cell code" id="J2seVKP7870P">
<div class="sourceCode" id="cb165"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb165-1"><a href="#cb165-1" aria-hidden="true" tabindex="-1"></a>NAME_OF_MODEL_FASTTEXT <span class="op">=</span> <span class="st">&quot;fasttext-wiki-news-subwords-300&quot;</span></span>
<span id="cb165-2"><a href="#cb165-2" aria-hidden="true" tabindex="-1"></a>NAME_OF_MODEL_GLOVE <span class="op">=</span> <span class="st">&quot;glove-twitter-200&quot;</span></span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="lbJVyP5g9N-t" data-outputId="283972b3-5800-4d72-bd78-244f966d3241">
<div class="sourceCode" id="cb166"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb166-1"><a href="#cb166-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> api.load(NAME_OF_MODEL_GLOVE)  <span class="co"># load glove vectors</span></span></code></pre></div>
<div class="output stream stdout">
<pre><code>[==================================================] 100.0% 758.5/758.5MB downloaded
</code></pre>
</div>
</div>
<div class="cell code" id="oL6ASYJy-LHH">
<div class="sourceCode" id="cb168"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb168-1"><a href="#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> load_model(File):</span>
<span id="cb168-2"><a href="#cb168-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Gensim library can load file which contains embedding. This method unzip file and created dictionary word to embedding vector.</span></span>
<span id="cb168-3"><a href="#cb168-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-4"><a href="#cb168-4" aria-hidden="true" tabindex="-1"></a><span class="co">    @param File: path to file</span></span>
<span id="cb168-5"><a href="#cb168-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb168-6"><a href="#cb168-6" aria-hidden="true" tabindex="-1"></a><span class="co">    @return: created embedding dictionary word to embedding vector</span></span>
<span id="cb168-7"><a href="#cb168-7" aria-hidden="true" tabindex="-1"></a><span class="co">    &quot;&quot;&quot;</span></span>
<span id="cb168-8"><a href="#cb168-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> gzip.<span class="bu">open</span>(File, <span class="st">&#39;r&#39;</span>) <span class="im">as</span> f:</span>
<span id="cb168-9"><a href="#cb168-9" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> {}</span>
<span id="cb168-10"><a href="#cb168-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> line <span class="kw">in</span> f:        </span>
<span id="cb168-11"><a href="#cb168-11" aria-hidden="true" tabindex="-1"></a>            splitLines <span class="op">=</span> line.split()</span>
<span id="cb168-12"><a href="#cb168-12" aria-hidden="true" tabindex="-1"></a>            word <span class="op">=</span> splitLines[<span class="dv">0</span>].decode(<span class="st">&quot;utf-8&quot;</span>)</span>
<span id="cb168-13"><a href="#cb168-13" aria-hidden="true" tabindex="-1"></a>            wordEmbedding <span class="op">=</span> np.array([<span class="bu">float</span>(value) <span class="cf">for</span> value <span class="kw">in</span> splitLines[<span class="dv">1</span>:]])</span>
<span id="cb168-14"><a href="#cb168-14" aria-hidden="true" tabindex="-1"></a>            model[word] <span class="op">=</span> wordEmbedding</span>
<span id="cb168-15"><a href="#cb168-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="bu">len</span>(model),<span class="st">&quot; words loaded!&quot;</span>)</span>
<span id="cb168-16"><a href="#cb168-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span></code></pre></div>
</div>
<div class="cell code" id="zIQ9V7ma-PXC">
<div class="sourceCode" id="cb169"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb169-1"><a href="#cb169-1" aria-hidden="true" tabindex="-1"></a>loaded_model_path <span class="op">=</span> api.load(NAME_OF_MODEL_GLOVE, return_path<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="IXNLEf8d-RE3" data-outputId="e015b1b9-6c9b-436d-bed4-572511a63134">
<div class="sourceCode" id="cb170"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb170-1"><a href="#cb170-1" aria-hidden="true" tabindex="-1"></a>embedding_dictionary <span class="op">=</span> load_model(loaded_model_path)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>1193515  words loaded!
</code></pre>
</div>
</div>
<div class="cell code" id="0rcRNjKL92lQ">
<div class="sourceCode" id="cb172"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb172-1"><a href="#cb172-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prepare_embeddings_matrix(input_dic, embedding_dimension, vocab):</span>
<span id="cb172-2"><a href="#cb172-2" aria-hidden="true" tabindex="-1"></a>    num_tokens <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb172-3"><a href="#cb172-3" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb172-4"><a href="#cb172-4" aria-hidden="true" tabindex="-1"></a>    num_tokens <span class="op">=</span> <span class="bu">len</span>(voc) <span class="op">+</span> <span class="dv">2</span></span>
<span id="cb172-5"><a href="#cb172-5" aria-hidden="true" tabindex="-1"></a>    hits <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb172-6"><a href="#cb172-6" aria-hidden="true" tabindex="-1"></a>    misses <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb172-7"><a href="#cb172-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb172-8"><a href="#cb172-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb172-9"><a href="#cb172-9" aria-hidden="true" tabindex="-1"></a>    embedding_matrix <span class="op">=</span> np.zeros((num_tokens, embedding_dimension))</span>
<span id="cb172-10"><a href="#cb172-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(vocab):</span>
<span id="cb172-11"><a href="#cb172-11" aria-hidden="true" tabindex="-1"></a>        embedding_vector <span class="op">=</span> <span class="va">None</span></span>
<span id="cb172-12"><a href="#cb172-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> word <span class="kw">in</span> input_dic:</span>
<span id="cb172-13"><a href="#cb172-13" aria-hidden="true" tabindex="-1"></a>            embedding_vector <span class="op">=</span> input_dic[word]</span>
<span id="cb172-14"><a href="#cb172-14" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb172-15"><a href="#cb172-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> embedding_vector <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb172-16"><a href="#cb172-16" aria-hidden="true" tabindex="-1"></a>            embedding_matrix[i] <span class="op">=</span> embedding_vector</span>
<span id="cb172-17"><a href="#cb172-17" aria-hidden="true" tabindex="-1"></a>            hits <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb172-18"><a href="#cb172-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb172-19"><a href="#cb172-19" aria-hidden="true" tabindex="-1"></a>            misses <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb172-20"><a href="#cb172-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb172-21"><a href="#cb172-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;Converted </span><span class="sc">%d</span><span class="st"> words (</span><span class="sc">%d</span><span class="st"> misses)&quot;</span> <span class="op">%</span> (hits, misses))</span>
<span id="cb172-22"><a href="#cb172-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> embedding_matrix, num_tokens</span></code></pre></div>
</div>
<section id="gru-lstm-b-and-fasttext" class="cell markdown" id="kYM3tduCBNpV">
<h1>GRU LSTM b and FASTTEXT</h1>
</section>
<div class="cell code" id="dhknQ9AAHMPU">
<div class="sourceCode" id="cb173"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb173-1"><a href="#cb173-1" aria-hidden="true" tabindex="-1"></a>name_gru_lstm_fasttext <span class="op">=</span> <span class="st">&quot;GRU LSTM b and FASTTEXT&quot;</span></span>
<span id="cb173-2"><a href="#cb173-2" aria-hidden="true" tabindex="-1"></a>train_data, train_label, validation_data, validation_label, test_data, test_label <span class="op">=</span> get_raw_ds(df_transformers.copy())</span>
<span id="cb173-3"><a href="#cb173-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb173-4"><a href="#cb173-4" aria-hidden="true" tabindex="-1"></a>loaded_model_path <span class="op">=</span> api.load(NAME_OF_MODEL_FASTTEXT, return_path<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb173-5"><a href="#cb173-5" aria-hidden="true" tabindex="-1"></a>embedding_dictionary <span class="op">=</span> load_model(loaded_model_path)</span></code></pre></div>
</div>
<div class="cell code" id="kJG5P7XsBQsk">
<div class="sourceCode" id="cb174"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb174-1"><a href="#cb174-1" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb174-2"><a href="#cb174-2" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb174-3"><a href="#cb174-3" aria-hidden="true" tabindex="-1"></a>sequence_length <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb174-4"><a href="#cb174-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-5"><a href="#cb174-5" aria-hidden="true" tabindex="-1"></a>vect_layer <span class="op">=</span> TextVectorization(max_tokens<span class="op">=</span>vocab_size, output_mode<span class="op">=</span><span class="st">&#39;int&#39;</span>, output_sequence_length<span class="op">=</span>sequence_length)</span>
<span id="cb174-6"><a href="#cb174-6" aria-hidden="true" tabindex="-1"></a>vect_layer.adapt(df.Clean_text.values)</span>
<span id="cb174-7"><a href="#cb174-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb174-8"><a href="#cb174-8" aria-hidden="true" tabindex="-1"></a>voc <span class="op">=</span> vect_layer.get_vocabulary()</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="VZH7a9JRBSPR" data-outputId="a00c61c1-6d6a-4c5f-f8a0-56c34343134d">
<div class="sourceCode" id="cb175"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb175-1"><a href="#cb175-1" aria-hidden="true" tabindex="-1"></a>embedding_matrix, num_tokens <span class="op">=</span> prepare_embeddings_matrix(embedding_dictionary, <span class="dv">300</span>, vect_layer.get_vocabulary())</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Converted 13536 words (6464 misses)
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;height&quot;:1000,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="YLcS1yrNBTYG" data-outputId="99f48ba5-cb85-424f-d303-36d5307b61e9">
<div class="sourceCode" id="cb177"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb177-1"><a href="#cb177-1" aria-hidden="true" tabindex="-1"></a>input_layer <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span>tf_string)</span>
<span id="cb177-2"><a href="#cb177-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-3"><a href="#cb177-3" aria-hidden="true" tabindex="-1"></a>x_v <span class="op">=</span> vect_layer(input_layer)</span>
<span id="cb177-4"><a href="#cb177-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-5"><a href="#cb177-5" aria-hidden="true" tabindex="-1"></a>emb <span class="op">=</span> keras.layers.Embedding(num_tokens, embedding_dim, embeddings_initializer<span class="op">=</span>keras.initializers.Constant(embedding_matrix), trainable<span class="op">=</span><span class="va">True</span>)(x_v)</span>
<span id="cb177-6"><a href="#cb177-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-7"><a href="#cb177-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Bidirectional(keras.layers.LSTM(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, return_sequences<span class="op">=</span><span class="va">True</span>, dropout<span class="op">=</span><span class="fl">0.2</span>, recurrent_dropout<span class="op">=</span><span class="fl">0.2</span>))(emb)</span>
<span id="cb177-8"><a href="#cb177-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.GRU(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, return_sequences<span class="op">=</span><span class="va">False</span>)(x)</span>
<span id="cb177-9"><a href="#cb177-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.keras.layers.BatchNormalization()(x)</span>
<span id="cb177-10"><a href="#cb177-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.2</span>)(x)</span>
<span id="cb177-11"><a href="#cb177-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">32</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb177-12"><a href="#cb177-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.3</span>)(x)</span>
<span id="cb177-13"><a href="#cb177-13" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">64</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb177-14"><a href="#cb177-14" aria-hidden="true" tabindex="-1"></a>output_layer <span class="op">=</span> keras.layers.Dense(<span class="dv">1</span>, <span class="st">&#39;sigmoid&#39;</span>)(x)</span>
<span id="cb177-15"><a href="#cb177-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-16"><a href="#cb177-16" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.Model(input_layer, output_layer)</span>
<span id="cb177-17"><a href="#cb177-17" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb177-18"><a href="#cb177-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-19"><a href="#cb177-19" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;rmsprop&#39;</span>, loss<span class="op">=</span>keras.losses.BinaryCrossentropy(), metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb177-20"><a href="#cb177-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-21"><a href="#cb177-21" aria-hidden="true" tabindex="-1"></a>es <span class="op">=</span> keras.callbacks.EarlyStopping(monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, min_delta<span class="op">=</span><span class="dv">0</span>, patience<span class="op">=</span><span class="dv">2</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb177-22"><a href="#cb177-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-23"><a href="#cb177-23" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb177-24"><a href="#cb177-24" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb177-25"><a href="#cb177-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-26"><a href="#cb177-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-27"><a href="#cb177-27" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(train_data.values, train_label.values, validation_data<span class="op">=</span>(validation_data.values, validation_label.values), callbacks<span class="op">=</span>[es], epochs<span class="op">=</span>epochs, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb177-28"><a href="#cb177-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb177-29"><a href="#cb177-29" aria-hidden="true" tabindex="-1"></a>show_history(history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_2 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Model: &quot;model_2&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_5 (InputLayer)        [(None, 1)]               0         
                                                                 
 text_vectorization_6 (TextV  (None, 30)               0         
 ectorization)                                                   
                                                                 
 embedding_4 (Embedding)     (None, 30, 300)           6000600   
                                                                 
 bidirectional_2 (Bidirectio  (None, 30, 128)          186880    
 nal)                                                            
                                                                 
 gru_2 (GRU)                 (None, 64)                37248     
                                                                 
 dense_8 (Dense)             (None, 128)               8320      
                                                                 
 batch_normalization_2 (Batc  (None, 128)              512       
 hNormalization)                                                 
                                                                 
 dropout_6 (Dropout)         (None, 128)               0         
                                                                 
 dense_9 (Dense)             (None, 64)                8256      
                                                                 
 dropout_7 (Dropout)         (None, 64)                0         
                                                                 
 dense_10 (Dense)            (None, 64)                4160      
                                                                 
 dropout_8 (Dropout)         (None, 64)                0         
                                                                 
 dense_11 (Dense)            (None, 1)                 65        
                                                                 
=================================================================
Total params: 6,246,041
Trainable params: 6,245,785
Non-trainable params: 256
_________________________________________________________________
Epoch 1/5
180/180 [==============================] - 90s 458ms/step - loss: 0.2471 - accuracy: 0.9295 - val_loss: 0.2784 - val_accuracy: 0.9300
Epoch 2/5
180/180 [==============================] - 83s 459ms/step - loss: 0.1489 - accuracy: 0.9493 - val_loss: 0.1744 - val_accuracy: 0.9300
Epoch 3/5
180/180 [==============================] - 83s 462ms/step - loss: 0.1043 - accuracy: 0.9665 - val_loss: 0.2098 - val_accuracy: 0.9300
Epoch 4/5
180/180 [==============================] - 82s 457ms/step - loss: 0.0782 - accuracy: 0.9745 - val_loss: 0.3542 - val_accuracy: 0.9464
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_5d270abade7b464aacf99cad3478e1a6/137815b08c1be5289ff1f2bda0fd08ce9b9a0459.png" /></p>
</div>
</div>
<div class="cell code" id="S4wQfUwkBVYS">
<div class="sourceCode" id="cb179"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb179-1"><a href="#cb179-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(test_data.values).ravel()</span>
<span id="cb179-2"><a href="#cb179-2" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> [<span class="dv">1</span> <span class="cf">if</span> x <span class="op">&gt;=</span> <span class="fl">0.5</span> <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> x <span class="kw">in</span> y_pred]</span>
<span id="cb179-3"><a href="#cb179-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb179-4"><a href="#cb179-4" aria-hidden="true" tabindex="-1"></a>f1_1 <span class="op">=</span> get_y_true_y_pred(test_label.values, y_pred)</span>
<span id="cb179-5"><a href="#cb179-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb179-6"><a href="#cb179-6" aria-hidden="true" tabindex="-1"></a>res_transformers[name_gru_lstm_fasttext] <span class="op">=</span> f1_1</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="ZAdVpIhWKCF5" data-outputId="9954c0aa-33b4-46c9-8dd3-6bbad6e0879d">
<div class="sourceCode" id="cb180"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb180-1"><a href="#cb180-1" aria-hidden="true" tabindex="-1"></a>Counter(y_pred)</span></code></pre></div>
<div class="output execute_result" data-execution_count="108">
<pre><code>Counter({0: 6393})</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="Y9OD2bubKl4T" data-outputId="21e5c6d6-62b0-4585-e126-389bcc8ef79b">
<div class="sourceCode" id="cb182"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb182-1"><a href="#cb182-1" aria-hidden="true" tabindex="-1"></a>test_label.value_counts()</span></code></pre></div>
<div class="output execute_result" data-execution_count="110">
<pre><code>0    5945
1     448
Name: label, dtype: int64</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="Hcd_EIarEnpJ" data-outputId="a6e2c7be-08b2-431a-acbc-1b5a482d99c6">
<div class="sourceCode" id="cb184"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb184-1"><a href="#cb184-1" aria-hidden="true" tabindex="-1"></a>res_transformers</span></code></pre></div>
<div class="output execute_result" data-execution_count="105">
<pre><code>{&#39;GRU LSTM b and FASTTEXT&#39;: 0.0,
 &#39;GRU LSTM b and GLOVE twitter&#39;: 0.6237762237762238}</code></pre>
</div>
</div>
<section id="gru-lstm-b-and-glove-twitter" class="cell markdown" id="ZYa_pCtT_w1e">
<h1>GRU LSTM b and GLOVE twitter</h1>
<blockquote>
<p>Odsazený blok</p>
</blockquote>
</section>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="FxNfhykB_1Rk" data-outputId="3d991128-3f35-4963-fa3e-38af2e1d320f">
<div class="sourceCode" id="cb186"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb186-1"><a href="#cb186-1" aria-hidden="true" tabindex="-1"></a>name_gru_glove_twitter <span class="op">=</span> <span class="st">&quot;GRU LSTM b and GLOVE twitter&quot;</span></span>
<span id="cb186-2"><a href="#cb186-2" aria-hidden="true" tabindex="-1"></a>train_data, train_label, validation_data, validation_label, test_data, test_label <span class="op">=</span> get_raw_ds(df_transformers.copy())</span>
<span id="cb186-3"><a href="#cb186-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb186-4"><a href="#cb186-4" aria-hidden="true" tabindex="-1"></a>loaded_model_path <span class="op">=</span> api.load(NAME_OF_MODEL_GLOVE, return_path<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb186-5"><a href="#cb186-5" aria-hidden="true" tabindex="-1"></a>embedding_dictionary <span class="op">=</span> load_model(loaded_model_path)</span>
<span id="cb186-6"><a href="#cb186-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb186-7"><a href="#cb186-7" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb186-8"><a href="#cb186-8" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb186-9"><a href="#cb186-9" aria-hidden="true" tabindex="-1"></a>sequence_length <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb186-10"><a href="#cb186-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb186-11"><a href="#cb186-11" aria-hidden="true" tabindex="-1"></a>vect_layer <span class="op">=</span> TextVectorization(max_tokens<span class="op">=</span>vocab_size, output_mode<span class="op">=</span><span class="st">&#39;int&#39;</span>, output_sequence_length<span class="op">=</span>sequence_length)</span>
<span id="cb186-12"><a href="#cb186-12" aria-hidden="true" tabindex="-1"></a>vect_layer.adapt(df.Clean_text.values)</span>
<span id="cb186-13"><a href="#cb186-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb186-14"><a href="#cb186-14" aria-hidden="true" tabindex="-1"></a>voc <span class="op">=</span> vect_layer.get_vocabulary()</span></code></pre></div>
<div class="output stream stdout">
<pre><code>0    29720
1     2242
Name: label, dtype: int64
1193515  words loaded!
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="bPIBHZkoAEQa" data-outputId="fcab4245-e56f-465e-9874-548b3fc73fc9">
<div class="sourceCode" id="cb188"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb188-1"><a href="#cb188-1" aria-hidden="true" tabindex="-1"></a>embedding_matrix, num_tokens <span class="op">=</span> prepare_embeddings_matrix(embedding_dictionary, <span class="dv">200</span>, vect_layer.get_vocabulary())</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Converted 15607 words (4393 misses)
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;height&quot;:1000,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="HWbUyUnV_90N" data-outputId="59f6a25d-1cdc-4a54-8fd5-ad4fc6e37a2b">
<div class="sourceCode" id="cb190"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb190-1"><a href="#cb190-1" aria-hidden="true" tabindex="-1"></a>input_layer <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span>tf_string)</span>
<span id="cb190-2"><a href="#cb190-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-3"><a href="#cb190-3" aria-hidden="true" tabindex="-1"></a>x_v <span class="op">=</span> vect_layer(input_layer)</span>
<span id="cb190-4"><a href="#cb190-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-5"><a href="#cb190-5" aria-hidden="true" tabindex="-1"></a>emb <span class="op">=</span> keras.layers.Embedding(num_tokens, embedding_dim, embeddings_initializer<span class="op">=</span>keras.initializers.Constant(embedding_matrix), trainable<span class="op">=</span><span class="va">True</span>)(x_v)</span>
<span id="cb190-6"><a href="#cb190-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-7"><a href="#cb190-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Bidirectional(keras.layers.LSTM(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, return_sequences<span class="op">=</span><span class="va">True</span>, dropout<span class="op">=</span><span class="fl">0.2</span>, recurrent_dropout<span class="op">=</span><span class="fl">0.2</span>))(emb)</span>
<span id="cb190-8"><a href="#cb190-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.GRU(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, return_sequences<span class="op">=</span><span class="va">False</span>)(x)</span>
<span id="cb190-9"><a href="#cb190-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.keras.layers.BatchNormalization()(x)</span>
<span id="cb190-10"><a href="#cb190-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.2</span>)(x)</span>
<span id="cb190-11"><a href="#cb190-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">64</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb190-12"><a href="#cb190-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.3</span>)(x)</span>
<span id="cb190-13"><a href="#cb190-13" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">16</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb190-14"><a href="#cb190-14" aria-hidden="true" tabindex="-1"></a>output_layer <span class="op">=</span> keras.layers.Dense(<span class="dv">1</span>, <span class="st">&#39;sigmoid&#39;</span>)(x)</span>
<span id="cb190-15"><a href="#cb190-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-16"><a href="#cb190-16" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.Model(input_layer, output_layer)</span>
<span id="cb190-17"><a href="#cb190-17" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb190-18"><a href="#cb190-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-19"><a href="#cb190-19" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;rmsprop&#39;</span>, loss<span class="op">=</span>keras.losses.BinaryCrossentropy(), metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb190-20"><a href="#cb190-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-21"><a href="#cb190-21" aria-hidden="true" tabindex="-1"></a>es <span class="op">=</span> keras.callbacks.EarlyStopping(monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, min_delta<span class="op">=</span><span class="dv">0</span>, patience<span class="op">=</span><span class="dv">2</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb190-22"><a href="#cb190-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-23"><a href="#cb190-23" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb190-24"><a href="#cb190-24" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb190-25"><a href="#cb190-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-26"><a href="#cb190-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-27"><a href="#cb190-27" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(train_data.values, train_label.values, validation_data<span class="op">=</span>(validation_data.values, validation_label.values), callbacks<span class="op">=</span>[es], epochs<span class="op">=</span>epochs, batch_size<span class="op">=</span>batch_size)</span>
<span id="cb190-28"><a href="#cb190-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb190-29"><a href="#cb190-29" aria-hidden="true" tabindex="-1"></a>show_history(history)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru_3 will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Model: &quot;model_3&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_6 (InputLayer)        [(None, 1)]               0         
                                                                 
 text_vectorization_7 (TextV  (None, 30)               0         
 ectorization)                                                   
                                                                 
 embedding_5 (Embedding)     (None, 30, 200)           4000400   
                                                                 
 bidirectional_3 (Bidirectio  (None, 30, 128)          135680    
 nal)                                                            
                                                                 
 gru_3 (GRU)                 (None, 64)                37248     
                                                                 
 batch_normalization_3 (Batc  (None, 64)               256       
 hNormalization)                                                 
                                                                 
 dropout_9 (Dropout)         (None, 64)                0         
                                                                 
 dense_12 (Dense)            (None, 64)                4160      
                                                                 
 dropout_10 (Dropout)        (None, 64)                0         
                                                                 
 dense_13 (Dense)            (None, 16)                1040      
                                                                 
 dense_14 (Dense)            (None, 1)                 17        
                                                                 
=================================================================
Total params: 4,178,801
Trainable params: 4,178,673
Non-trainable params: 128
_________________________________________________________________
Epoch 1/5
360/360 [==============================] - 171s 453ms/step - loss: 0.2828 - accuracy: 0.9276 - val_loss: 0.2797 - val_accuracy: 0.9300
Epoch 2/5
360/360 [==============================] - 161s 447ms/step - loss: 0.2690 - accuracy: 0.9298 - val_loss: 0.2550 - val_accuracy: 0.9300
Epoch 3/5
360/360 [==============================] - 159s 442ms/step - loss: 0.2637 - accuracy: 0.9298 - val_loss: 0.2427 - val_accuracy: 0.9300
Epoch 4/5
360/360 [==============================] - 158s 439ms/step - loss: 0.2591 - accuracy: 0.9298 - val_loss: 0.2485 - val_accuracy: 0.9300
Epoch 5/5
360/360 [==============================] - 157s 437ms/step - loss: 0.2549 - accuracy: 0.9298 - val_loss: 0.2440 - val_accuracy: 0.9300
</code></pre>
</div>
<div class="output display_data">
<p><img src="vertopal_5d270abade7b464aacf99cad3478e1a6/2446d3f1dfd9c9258064b5dd575c00ec46acf7d4.png" /></p>
</div>
</div>
<div class="cell code" id="54WWdNuiARbR">
<div class="sourceCode" id="cb192"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb192-1"><a href="#cb192-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(test_data.values).ravel()</span>
<span id="cb192-2"><a href="#cb192-2" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> [<span class="dv">1</span> <span class="cf">if</span> x <span class="op">&gt;=</span> <span class="fl">0.5</span> <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> x <span class="kw">in</span> y_pred]</span>
<span id="cb192-3"><a href="#cb192-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb192-4"><a href="#cb192-4" aria-hidden="true" tabindex="-1"></a>f1_1 <span class="op">=</span> get_y_true_y_pred(test_label.values, y_pred)</span>
<span id="cb192-5"><a href="#cb192-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb192-6"><a href="#cb192-6" aria-hidden="true" tabindex="-1"></a>res_transformers[name_gru_glove_twitter] <span class="op">=</span> f1_1</span></code></pre></div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="5U1LvJvaEeEj" data-outputId="39bf48aa-8d06-4637-bbdf-2eaa19250a14">
<div class="sourceCode" id="cb193"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb193-1"><a href="#cb193-1" aria-hidden="true" tabindex="-1"></a>res_transformers</span></code></pre></div>
<div class="output execute_result" data-execution_count="115">
<pre><code>{&#39;GRU LSTM b and FASTTEXT&#39;: 0.0, &#39;GRU LSTM b and GLOVE twitter&#39;: 0.0}</code></pre>
</div>
</div>
<section id="gru-lstm-b-and-glove" class="cell markdown" id="J0xm70II_sb2">
<h1>GRU LSTM b and GLOVE</h1>
</section>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="sxT2t848Z0J8" data-outputId="b3869c6c-00a7-45f6-b6e1-42a8435c2998">
<div class="sourceCode" id="cb195"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb195-1"><a href="#cb195-1" aria-hidden="true" tabindex="-1"></a>name_gru <span class="op">=</span> <span class="st">&quot;GRU with GLOVE&quot;</span></span>
<span id="cb195-2"><a href="#cb195-2" aria-hidden="true" tabindex="-1"></a>train_data, train_label, validation_data, validation_label, test_data, test_label <span class="op">=</span> get_raw_ds(df_transformers.copy())</span>
<span id="cb195-3"><a href="#cb195-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-4"><a href="#cb195-4" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb195-5"><a href="#cb195-5" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="dv">13000</span> </span>
<span id="cb195-6"><a href="#cb195-6" aria-hidden="true" tabindex="-1"></a>sequence_length <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb195-7"><a href="#cb195-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-8"><a href="#cb195-8" aria-hidden="true" tabindex="-1"></a>vect_layer <span class="op">=</span> TextVectorization(max_tokens<span class="op">=</span>vocab_size, output_mode<span class="op">=</span><span class="st">&#39;int&#39;</span>, output_sequence_length<span class="op">=</span>sequence_length)</span>
<span id="cb195-9"><a href="#cb195-9" aria-hidden="true" tabindex="-1"></a>vect_layer.adapt(df.Clean_text.values)</span>
<span id="cb195-10"><a href="#cb195-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-11"><a href="#cb195-11" aria-hidden="true" tabindex="-1"></a>voc <span class="op">=</span> vect_layer.get_vocabulary()</span>
<span id="cb195-12"><a href="#cb195-12" aria-hidden="true" tabindex="-1"></a>word_index <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(voc, <span class="bu">range</span>(<span class="bu">len</span>(voc))))</span>
<span id="cb195-13"><a href="#cb195-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-14"><a href="#cb195-14" aria-hidden="true" tabindex="-1"></a>num_tokens <span class="op">=</span> <span class="bu">len</span>(voc) <span class="op">+</span> <span class="dv">2</span></span>
<span id="cb195-15"><a href="#cb195-15" aria-hidden="true" tabindex="-1"></a>hits <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb195-16"><a href="#cb195-16" aria-hidden="true" tabindex="-1"></a>misses <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb195-17"><a href="#cb195-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb195-18"><a href="#cb195-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare embedding matrix</span></span>
<span id="cb195-19"><a href="#cb195-19" aria-hidden="true" tabindex="-1"></a>embedding_matrix <span class="op">=</span> np.zeros((num_tokens, embedding_dim))</span>
<span id="cb195-20"><a href="#cb195-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, i <span class="kw">in</span> word_index.items():</span>
<span id="cb195-21"><a href="#cb195-21" aria-hidden="true" tabindex="-1"></a>    embedding_vector <span class="op">=</span> embeddings_index.get(word)</span>
<span id="cb195-22"><a href="#cb195-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> embedding_vector <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb195-23"><a href="#cb195-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Words not found in embedding index will be all-zeros.</span></span>
<span id="cb195-24"><a href="#cb195-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># This includes the representation for &quot;padding&quot; and &quot;OOV&quot;</span></span>
<span id="cb195-25"><a href="#cb195-25" aria-hidden="true" tabindex="-1"></a>        embedding_matrix[i] <span class="op">=</span> embedding_vector</span>
<span id="cb195-26"><a href="#cb195-26" aria-hidden="true" tabindex="-1"></a>        hits <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb195-27"><a href="#cb195-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb195-28"><a href="#cb195-28" aria-hidden="true" tabindex="-1"></a>        misses <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb195-29"><a href="#cb195-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Converted </span><span class="sc">%d</span><span class="st"> words (</span><span class="sc">%d</span><span class="st"> misses)&quot;</span> <span class="op">%</span> (hits, misses))</span></code></pre></div>
<div class="output stream stdout">
<pre><code>0    29720
1     2242
Name: label, dtype: int64
Converted 10586 words (2414 misses)
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="en8YV7XbeZ_2" data-outputId="38ffcb55-31e5-404a-a345-8a039772afa1">
<div class="sourceCode" id="cb197"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb197-1"><a href="#cb197-1" aria-hidden="true" tabindex="-1"></a>embedding_matrix.shape</span></code></pre></div>
<div class="output execute_result" data-execution_count="71">
<pre><code>(13002, 50)</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;height&quot;:1000,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="BXzuCTE2eVFR" data-outputId="5f3a939c-708e-4888-e501-abd11f678912">
<div class="sourceCode" id="cb199"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb199-1"><a href="#cb199-1" aria-hidden="true" tabindex="-1"></a>input_layer <span class="op">=</span> keras.layers.Input(shape<span class="op">=</span>(<span class="dv">1</span>,), dtype<span class="op">=</span>tf_string)</span>
<span id="cb199-2"><a href="#cb199-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-3"><a href="#cb199-3" aria-hidden="true" tabindex="-1"></a>x_v <span class="op">=</span> vect_layer(input_layer)</span>
<span id="cb199-4"><a href="#cb199-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-5"><a href="#cb199-5" aria-hidden="true" tabindex="-1"></a>emb <span class="op">=</span> keras.layers.Embedding(num_tokens, embedding_dim, embeddings_initializer<span class="op">=</span>keras.initializers.Constant(embedding_matrix), trainable<span class="op">=</span><span class="va">True</span>)(x_v)</span>
<span id="cb199-6"><a href="#cb199-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-7"><a href="#cb199-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Bidirectional(keras.layers.LSTM(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, return_sequences<span class="op">=</span><span class="va">True</span>, dropout<span class="op">=</span><span class="fl">0.2</span>, recurrent_dropout<span class="op">=</span><span class="fl">0.2</span>))(emb)</span>
<span id="cb199-8"><a href="#cb199-8" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.GRU(<span class="dv">64</span>, activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>, return_sequences<span class="op">=</span><span class="va">False</span>)(x)</span>
<span id="cb199-9"><a href="#cb199-9" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">128</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb199-10"><a href="#cb199-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> tf.keras.layers.BatchNormalization()(x)</span>
<span id="cb199-11"><a href="#cb199-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.2</span>)(x)</span>
<span id="cb199-12"><a href="#cb199-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">64</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb199-13"><a href="#cb199-13" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.3</span>)(x)</span>
<span id="cb199-14"><a href="#cb199-14" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dense(<span class="dv">64</span>, <span class="st">&#39;relu&#39;</span>)(x)</span>
<span id="cb199-15"><a href="#cb199-15" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> keras.layers.Dropout(<span class="fl">0.2</span>)(x)</span>
<span id="cb199-16"><a href="#cb199-16" aria-hidden="true" tabindex="-1"></a>output_layer <span class="op">=</span> keras.layers.Dense(<span class="dv">1</span>, <span class="st">&#39;sigmoid&#39;</span>)(x)</span>
<span id="cb199-17"><a href="#cb199-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-18"><a href="#cb199-18" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.Model(input_layer, output_layer)</span>
<span id="cb199-19"><a href="#cb199-19" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb199-20"><a href="#cb199-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-21"><a href="#cb199-21" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">&#39;rmsprop&#39;</span>, loss<span class="op">=</span>keras.losses.BinaryCrossentropy(), metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb199-22"><a href="#cb199-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-23"><a href="#cb199-23" aria-hidden="true" tabindex="-1"></a>es <span class="op">=</span> keras.callbacks.EarlyStopping(monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, min_delta<span class="op">=</span><span class="dv">0</span>, patience<span class="op">=</span><span class="dv">5</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb199-24"><a href="#cb199-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-25"><a href="#cb199-25" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb199-26"><a href="#cb199-26" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb199-27"><a href="#cb199-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-28"><a href="#cb199-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb199-29"><a href="#cb199-29" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(train_data.values, train_label.values, validation_data<span class="op">=</span>(validation_data.values, validation_label.values), callbacks<span class="op">=</span>[es], epochs<span class="op">=</span>epochs, batch_size<span class="op">=</span>batch_size)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn&#39;t meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.
Model: &quot;model&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1)]               0         
                                                                 
 text_vectorization_2 (TextV  (None, 30)               0         
 ectorization)                                                   
                                                                 
 embedding (Embedding)       (None, 30, 50)            650100    
                                                                 
 bidirectional (Bidirectiona  (None, 30, 128)          58880     
 l)                                                              
                                                                 
 gru (GRU)                   (None, 64)                37248     
                                                                 
 dense (Dense)               (None, 128)               8320      
                                                                 
 batch_normalization (BatchN  (None, 128)              512       
 ormalization)                                                   
                                                                 
 dropout (Dropout)           (None, 128)               0         
                                                                 
 dense_1 (Dense)             (None, 64)                8256      
                                                                 
 dropout_1 (Dropout)         (None, 64)                0         
                                                                 
 dense_2 (Dense)             (None, 64)                4160      
                                                                 
 dropout_2 (Dropout)         (None, 64)                0         
                                                                 
 dense_3 (Dense)             (None, 1)                 65        
                                                                 
=================================================================
Total params: 767,541
Trainable params: 767,285
Non-trainable params: 256
_________________________________________________________________
Epoch 1/5
720/720 [==============================] - 342s 459ms/step - loss: 0.2463 - accuracy: 0.9291 - val_loss: 0.2474 - val_accuracy: 0.9347
Epoch 2/5
720/720 [==============================] - 313s 435ms/step - loss: 0.2181 - accuracy: 0.9317 - val_loss: 0.2395 - val_accuracy: 0.9300
Epoch 3/5
312/720 [============&gt;.................] - ETA: 2:56 - loss: 0.2233 - accuracy: 0.9281</code></pre>
</div>
<div class="output error" data-ename="KeyboardInterrupt" data-evalue="ignored">
<pre><code>---------------------------------------------------------------------------
KeyboardInterrupt                         Traceback (most recent call last)
&lt;ipython-input-72-a8d76ccd3230&gt; in &lt;module&gt;()
     27 
     28 
---&gt; 29 history = model.fit(train_data.values, train_label.values, validation_data=(validation_data.values, validation_label.values), callbacks=[es], epochs=epochs, batch_size=batch_size)

/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py in error_handler(*args, **kwargs)
     62     filtered_tb = None
     63     try:
---&gt; 64       return fn(*args, **kwargs)
     65     except Exception as e:  # pylint: disable=broad-except
     66       filtered_tb = _process_traceback_frames(e.__traceback__)

/usr/local/lib/python3.7/dist-packages/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1382                 _r=1):
   1383               callbacks.on_train_batch_begin(step)
-&gt; 1384               tmp_logs = self.train_function(iterator)
   1385               if data_handler.should_sync:
   1386                 context.async_wait()

/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs)
    148     filtered_tb = None
    149     try:
--&gt; 150       return fn(*args, **kwargs)
    151     except Exception as e:
    152       filtered_tb = _process_traceback_frames(e.__traceback__)

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    913 
    914       with OptionalXlaContext(self._jit_compile):
--&gt; 915         result = self._call(*args, **kwds)
    916 
    917       new_tracing_count = self.experimental_get_tracing_count()

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    945       # In this case we have created variables on the first call, so we run the
    946       # defunned version which is guaranteed to never create variables.
--&gt; 947       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
    948     elif self._stateful_fn is not None:
    949       # Release the lock early so that multiple threads can perform the call

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   2955        filtered_flat_args) = self._maybe_define_function(args, kwargs)
   2956     return graph_function._call_flat(
-&gt; 2957         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
   2958 
   2959   @property

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1852       # No tape is watching; skip to running the function.
   1853       return self._build_call_outputs(self._inference_function.call(
-&gt; 1854           ctx, args, cancellation_manager=cancellation_manager))
   1855     forward_backward = self._select_forward_and_backward_functions(
   1856         args,

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    502               inputs=args,
    503               attrs=attrs,
--&gt; 504               ctx=ctx)
    505         else:
    506           outputs = execute.execute_with_cancellation(

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     53     ctx.ensure_initialized()
     54     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---&gt; 55                                         inputs, attrs, num_outputs)
     56   except core._NotOkStatusException as e:
     57     if name is not None:

KeyboardInterrupt: 
</code></pre>
</div>
</div>
<div class="cell code" data-colab="{&quot;height&quot;:297,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="-I940a9QfK9D" data-outputId="8c4dc9b0-096d-4419-ed75-f37982b176a3">
<div class="sourceCode" id="cb202"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb202-1"><a href="#cb202-1" aria-hidden="true" tabindex="-1"></a>show_history(history)</span></code></pre></div>
<div class="output display_data">
<p><img src="vertopal_5d270abade7b464aacf99cad3478e1a6/3644d8816f14fd147423732de3cf03f92573c81a.png" /></p>
</div>
</div>
<div class="cell code" id="qpysCd2ZgMB5">
<div class="sourceCode" id="cb203"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb203-1"><a href="#cb203-1" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(test_data.values).ravel()</span>
<span id="cb203-2"><a href="#cb203-2" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> [<span class="dv">1</span> <span class="cf">if</span> x <span class="op">&gt;=</span> <span class="fl">0.5</span> <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> x <span class="kw">in</span> y_pred]</span>
<span id="cb203-3"><a href="#cb203-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb203-4"><a href="#cb203-4" aria-hidden="true" tabindex="-1"></a>f1_1 <span class="op">=</span> get_y_true_y_pred(test_label.values, y_pred)</span>
<span id="cb203-5"><a href="#cb203-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb203-6"><a href="#cb203-6" aria-hidden="true" tabindex="-1"></a>res_transformers[name_gru] <span class="op">=</span> f1_1</span></code></pre></div>
</div>
<section id="learning-distilbert---trainable" class="cell markdown" id="97S7J935mKph">
<h1>Learning DistilBERT - trainable</h1>
</section>
<div class="cell code" data-execution_count="58" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="2Q7_IrDts9XC" data-outputId="2c4bedeb-b77a-40f9-9803-41ca0f607663">
<div class="sourceCode" id="cb204"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb204-1"><a href="#cb204-1" aria-hidden="true" tabindex="-1"></a>name_3 <span class="op">=</span> <span class="st">&quot;Learning BERT - trainable&quot;</span></span>
<span id="cb204-2"><a href="#cb204-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-3"><a href="#cb204-3" aria-hidden="true" tabindex="-1"></a>NUM_EPOCHS <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb204-4"><a href="#cb204-4" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb204-5"><a href="#cb204-5" aria-hidden="true" tabindex="-1"></a>LEARNING_RATE <span class="op">=</span> <span class="fl">5e-5</span></span>
<span id="cb204-6"><a href="#cb204-6" aria-hidden="true" tabindex="-1"></a>EARLY_STOP_PATIENCE <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb204-7"><a href="#cb204-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-8"><a href="#cb204-8" aria-hidden="true" tabindex="-1"></a>train_data, train_label, validation_data, validation_label, test_data, test_label <span class="op">=</span> get_raw_ds(df_transformers.copy())</span>
<span id="cb204-9"><a href="#cb204-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-10"><a href="#cb204-10" aria-hidden="true" tabindex="-1"></a>base <span class="op">=</span> TFDistilBertModel.from_pretrained(</span>
<span id="cb204-11"><a href="#cb204-11" aria-hidden="true" tabindex="-1"></a>    MODEL_NAME,</span>
<span id="cb204-12"><a href="#cb204-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb204-13"><a href="#cb204-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-14"><a href="#cb204-14" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tf.keras.layers.Input(shape<span class="op">=</span>(MAX_SEQUENCE_LENGTH,), dtype<span class="op">=</span>tf.int32, name<span class="op">=</span><span class="st">&#39;input_ids&#39;</span>)</span>
<span id="cb204-15"><a href="#cb204-15" aria-hidden="true" tabindex="-1"></a>attention_mask <span class="op">=</span> tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype<span class="op">=</span>tf.int32, name<span class="op">=</span><span class="st">&#39;attention_mask&#39;</span>)</span>
<span id="cb204-16"><a href="#cb204-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-17"><a href="#cb204-17" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> base([input_ids, attention_mask]).last_hidden_state[:, <span class="dv">0</span>, :]</span>
<span id="cb204-18"><a href="#cb204-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-19"><a href="#cb204-19" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> tf.keras.layers.Dropout(</span>
<span id="cb204-20"><a href="#cb204-20" aria-hidden="true" tabindex="-1"></a>    rate<span class="op">=</span><span class="fl">0.15</span>,</span>
<span id="cb204-21"><a href="#cb204-21" aria-hidden="true" tabindex="-1"></a>)(output)</span>
<span id="cb204-22"><a href="#cb204-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb204-23"><a href="#cb204-23" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> tf.keras.layers.Dense(</span>
<span id="cb204-24"><a href="#cb204-24" aria-hidden="true" tabindex="-1"></a>    units<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb204-25"><a href="#cb204-25" aria-hidden="true" tabindex="-1"></a>    activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>,</span>
<span id="cb204-26"><a href="#cb204-26" aria-hidden="true" tabindex="-1"></a>)(output)</span>
<span id="cb204-27"><a href="#cb204-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-28"><a href="#cb204-28" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> tf.keras.layers.BatchNormalization()(output)</span>
<span id="cb204-29"><a href="#cb204-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-30"><a href="#cb204-30" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> tf.keras.layers.Dense(</span>
<span id="cb204-31"><a href="#cb204-31" aria-hidden="true" tabindex="-1"></a>    units<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb204-32"><a href="#cb204-32" aria-hidden="true" tabindex="-1"></a>    activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>,</span>
<span id="cb204-33"><a href="#cb204-33" aria-hidden="true" tabindex="-1"></a>)(output)</span>
<span id="cb204-34"><a href="#cb204-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-35"><a href="#cb204-35" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> tf.keras.layers.BatchNormalization()(output)</span>
<span id="cb204-36"><a href="#cb204-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-37"><a href="#cb204-37" aria-hidden="true" tabindex="-1"></a>output_layer <span class="op">=</span> tf.keras.layers.Dense(</span>
<span id="cb204-38"><a href="#cb204-38" aria-hidden="true" tabindex="-1"></a>    units<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb204-39"><a href="#cb204-39" aria-hidden="true" tabindex="-1"></a>    activation<span class="op">=</span><span class="st">&#39;sigmoid&#39;</span></span>
<span id="cb204-40"><a href="#cb204-40" aria-hidden="true" tabindex="-1"></a>)(output)</span>
<span id="cb204-41"><a href="#cb204-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-42"><a href="#cb204-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-43"><a href="#cb204-43" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>[input_ids, attention_mask], outputs<span class="op">=</span>output_layer)</span>
<span id="cb204-44"><a href="#cb204-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-45"><a href="#cb204-45" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb204-46"><a href="#cb204-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-47"><a href="#cb204-47" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>tf.keras.optimizers.Adam(learning_rate<span class="op">=</span>LEARNING_RATE), loss<span class="op">=</span>keras.losses.BinaryCrossentropy(), metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb204-48"><a href="#cb204-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb204-49"><a href="#cb204-49" aria-hidden="true" tabindex="-1"></a>es <span class="op">=</span> keras.callbacks.EarlyStopping(monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, min_delta<span class="op">=</span><span class="dv">0</span>, patience<span class="op">=</span>EARLY_STOP_PATIENCE, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>0    29720
1     2242
Name: label, dtype: int64
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: [&#39;vocab_transform&#39;, &#39;vocab_projector&#39;, &#39;vocab_layer_norm&#39;, &#39;activation_13&#39;]
- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Model: &quot;model_2&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_ids (InputLayer)         [(None, 40)]         0           []                               
                                                                                                  
 attention_mask (InputLayer)    [(None, 40)]         0           []                               
                                                                                                  
 tf_distil_bert_model_2 (TFDist  TFBaseModelOutput(l  66362880   [&#39;input_ids[0][0]&#39;,              
 ilBertModel)                   ast_hidden_state=(N               &#39;attention_mask[0][0]&#39;]         
                                one, 40, 768),                                                    
                                 hidden_states=None                                               
                                , attentions=None)                                                
                                                                                                  
 tf.__operators__.getitem_2 (Sl  (None, 768)         0           [&#39;tf_distil_bert_model_2[0][0]&#39;] 
 icingOpLambda)                                                                                   
                                                                                                  
 dropout_59 (Dropout)           (None, 768)          0           [&#39;tf.__operators__.getitem_2[0][0
                                                                 ]&#39;]                              
                                                                                                  
 dense_6 (Dense)                (None, 64)           49216       [&#39;dropout_59[0][0]&#39;]             
                                                                                                  
 batch_normalization_4 (BatchNo  (None, 64)          256         [&#39;dense_6[0][0]&#39;]                
 rmalization)                                                                                     
                                                                                                  
 dense_7 (Dense)                (None, 64)           4160        [&#39;batch_normalization_4[0][0]&#39;]  
                                                                                                  
 batch_normalization_5 (BatchNo  (None, 64)          256         [&#39;dense_7[0][0]&#39;]                
 rmalization)                                                                                     
                                                                                                  
 dense_8 (Dense)                (None, 1)            65          [&#39;batch_normalization_5[0][0]&#39;]  
                                                                                                  
==================================================================================================
Total params: 66,416,833
Trainable params: 66,416,577
Non-trainable params: 256
__________________________________________________________________________________________________
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="59" id="QBXfHKzis9XD">
<div class="sourceCode" id="cb208"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb208-1"><a href="#cb208-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> tf.data.Dataset.from_tensor_slices((</span>
<span id="cb208-2"><a href="#cb208-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">dict</span>(tokenize(<span class="bu">list</span>(train_data))),</span>
<span id="cb208-3"><a href="#cb208-3" aria-hidden="true" tabindex="-1"></a>    train_label</span>
<span id="cb208-4"><a href="#cb208-4" aria-hidden="true" tabindex="-1"></a>)).batch(BATCH_SIZE).prefetch(<span class="dv">1</span>)</span>
<span id="cb208-5"><a href="#cb208-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-6"><a href="#cb208-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-7"><a href="#cb208-7" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> tf.data.Dataset.from_tensor_slices((</span>
<span id="cb208-8"><a href="#cb208-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">dict</span>(tokenize(<span class="bu">list</span>(validation_data))),</span>
<span id="cb208-9"><a href="#cb208-9" aria-hidden="true" tabindex="-1"></a>    validation_label</span>
<span id="cb208-10"><a href="#cb208-10" aria-hidden="true" tabindex="-1"></a>)).batch(BATCH_SIZE).prefetch(<span class="dv">1</span>)</span>
<span id="cb208-11"><a href="#cb208-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb208-12"><a href="#cb208-12" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> tf.data.Dataset.from_tensor_slices((</span>
<span id="cb208-13"><a href="#cb208-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">dict</span>(tokenize(<span class="bu">list</span>(test_data))),</span>
<span id="cb208-14"><a href="#cb208-14" aria-hidden="true" tabindex="-1"></a>    test_label</span>
<span id="cb208-15"><a href="#cb208-15" aria-hidden="true" tabindex="-1"></a>)).batch(<span class="dv">1</span>).prefetch(<span class="dv">1</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="60" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="GlOpkYr0s9XE" data-outputId="6bdcc0d2-365c-4214-cd65-e76e12a3fb66">
<div class="sourceCode" id="cb209"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb209-1"><a href="#cb209-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(</span>
<span id="cb209-2"><a href="#cb209-2" aria-hidden="true" tabindex="-1"></a>    X,</span>
<span id="cb209-3"><a href="#cb209-3" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>NUM_EPOCHS,</span>
<span id="cb209-4"><a href="#cb209-4" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>V,</span>
<span id="cb209-5"><a href="#cb209-5" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb209-6"><a href="#cb209-6" aria-hidden="true" tabindex="-1"></a>      es</span>
<span id="cb209-7"><a href="#cb209-7" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb209-8"><a href="#cb209-8" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/10
720/720 [==============================] - 221s 291ms/step - loss: 0.5739 - accuracy: 0.7794 - val_loss: 0.3575 - val_accuracy: 0.9562
Epoch 2/10
720/720 [==============================] - 207s 288ms/step - loss: 0.3769 - accuracy: 0.9319 - val_loss: 0.2855 - val_accuracy: 0.9562
Epoch 3/10
720/720 [==============================] - 207s 287ms/step - loss: 0.2560 - accuracy: 0.9624 - val_loss: 0.3972 - val_accuracy: 0.9515
Epoch 4/10
720/720 [==============================] - 207s 287ms/step - loss: 0.1840 - accuracy: 0.9738 - val_loss: 0.2895 - val_accuracy: 0.9609
Epoch 5/10
720/720 [==============================] - 208s 289ms/step - loss: 0.1140 - accuracy: 0.9857 - val_loss: 0.3298 - val_accuracy: 0.9605
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="61" data-colab="{&quot;height&quot;:297,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="Gn_JVD8O136w" data-outputId="e5190f82-a349-4cb5-9ed9-fa1154a9146b">
<div class="sourceCode" id="cb211"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb211-1"><a href="#cb211-1" aria-hidden="true" tabindex="-1"></a>show_history(history)</span></code></pre></div>
<div class="output display_data">
<p><img src="vertopal_5d270abade7b464aacf99cad3478e1a6/0bb8e300acdb0d6961bf4bcb74066cd974732396.png" /></p>
</div>
</div>
<div class="cell code" data-execution_count="62" id="kLC-k3XIs9XE">
<div class="sourceCode" id="cb212"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb212-1"><a href="#cb212-1" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> model.predict(T)</span>
<span id="cb212-2"><a href="#cb212-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb212-3"><a href="#cb212-3" aria-hidden="true" tabindex="-1"></a>predicted <span class="op">=</span> [<span class="dv">1</span> <span class="cf">if</span> x <span class="op">&gt;=</span> <span class="fl">0.5</span> <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> x <span class="kw">in</span> labels.ravel()]</span>
<span id="cb212-4"><a href="#cb212-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb212-5"><a href="#cb212-5" aria-hidden="true" tabindex="-1"></a>f1_1 <span class="op">=</span> get_y_true_y_pred(test_label, predicted)</span>
<span id="cb212-6"><a href="#cb212-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb212-7"><a href="#cb212-7" aria-hidden="true" tabindex="-1"></a>res_transformers[name_3] <span class="op">=</span> f1_1</span></code></pre></div>
</div>
<section id="bert-trainable" class="cell markdown" id="pgrpiIXfvoPj">
<h1>Bert trainable</h1>
</section>
<div class="cell code" data-execution_count="64" id="ZmDKdupuwK0B">
<div class="sourceCode" id="cb213"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb213-1"><a href="#cb213-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> TFAutoModel</span>
<span id="cb213-2"><a href="#cb213-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="67" id="hE-4rfR7wDOC">
<div class="sourceCode" id="cb214"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb214-1"><a href="#cb214-1" aria-hidden="true" tabindex="-1"></a>BERT_MODEL_NAME <span class="op">=</span> <span class="st">&quot;bert-base-uncased&quot;</span></span></code></pre></div>
</div>
<div class="cell code" data-execution_count="68" data-colab="{&quot;height&quot;:145,&quot;referenced_widgets&quot;:[&quot;cb801a108fba4fb39545b05ad235bd2a&quot;,&quot;845cdb7fbeb3407ab6b26cf5222092bd&quot;,&quot;48bd62cfd9bf420a9b5ea0664a6c1f80&quot;,&quot;d9ef31de77b34a6f91699e82c702b3cc&quot;,&quot;e2a103ceac114c6e858423ff81390e70&quot;,&quot;4829419baea644ceb3584cff9a6b5f06&quot;,&quot;50ff15be23e84d52a05245b63a3dde96&quot;,&quot;7e0161eca8c84a3d9126e0a3f994b188&quot;,&quot;27901beb2513493e9b2f53d1072a49e6&quot;,&quot;e61993940c794448821c0ebdef638028&quot;,&quot;8f5b736fe1774e0bbdec59313b714192&quot;,&quot;3388de62d02c44aba6f70a9d8e12850c&quot;,&quot;57987e3895cc4fa9a966318a2a2eb20d&quot;,&quot;f02e33215af94324accc6e30578075ea&quot;,&quot;ffb9c6198022490c9d3a348a6bdcef16&quot;,&quot;d653a62a76d24dc59159fac05aa5172c&quot;,&quot;74c46af72e4b431d97e9612308f56579&quot;,&quot;843dbf528de94c6c88c3d4c47100d6cc&quot;,&quot;489877d98ad349cfac41573f97ea84e8&quot;,&quot;b4de6a5496f541d693c0108df97b81f9&quot;,&quot;c838fdecdeac41b7929c687b66581e2c&quot;,&quot;e914e2904e3946118690861d101ef48c&quot;,&quot;34de187ae4164221b816102329ad0471&quot;,&quot;d255798b6230420985dea8d0804e379b&quot;,&quot;a7c4af57faae484ca9486da00a20e0b7&quot;,&quot;4121cfde2b934fbfbb34bdd95cfb117a&quot;,&quot;ae66e8f7be42458584f58a441915a98a&quot;,&quot;cb34d9dfdb344bfe9cd99d1eeff44fe0&quot;,&quot;9332d017fa1447a3bcfe69f89336710d&quot;,&quot;15131385591e4f89a9e00a65b461e9fc&quot;,&quot;bb1b638dad0e467881afda384a5e9174&quot;,&quot;303e1bb7b4d047399fa840c8d41990f7&quot;,&quot;ebc909d5e1214016b7c76f891ab95d9e&quot;,&quot;4ac5bd3128254aedb0c792f29a763765&quot;,&quot;0d68c100379c4c4cabc13cf4571e3a55&quot;,&quot;2fb8e684c4764d8ebc7fb9468010544a&quot;,&quot;92abba112bd247439406c2f8d69cc21d&quot;,&quot;c0ee38f2b0f74fafac1c3b18b73d994a&quot;,&quot;c14e3f6af7424e17b70d2c803457e028&quot;,&quot;2b5f76639fe14656af9202c4130e82a4&quot;,&quot;8417193e5b464897a3a92fc3a0f800ae&quot;,&quot;3aa5b61da5464fbc8c71a064f01d0d9f&quot;,&quot;049d2c6265ae400db7950e3d5d9c5022&quot;,&quot;4a3133b964f94591bc4168d0a9e9ff54&quot;],&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="cZWShip6wCJN" data-outputId="46534d34-8a5a-4b9a-f42a-124432a3f3f4">
<div class="sourceCode" id="cb215"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb215-1"><a href="#cb215-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(BERT_MODEL_NAME)</span>
<span id="cb215-2"><a href="#cb215-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb215-3"><a href="#cb215-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb215-4"><a href="#cb215-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize(sentences, max_length<span class="op">=</span>MAX_SEQUENCE_LENGTH, padding<span class="op">=</span><span class="st">&#39;max_length&#39;</span>):</span>
<span id="cb215-5"><a href="#cb215-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer(</span>
<span id="cb215-6"><a href="#cb215-6" aria-hidden="true" tabindex="-1"></a>        sentences,</span>
<span id="cb215-7"><a href="#cb215-7" aria-hidden="true" tabindex="-1"></a>        truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb215-8"><a href="#cb215-8" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span>padding,</span>
<span id="cb215-9"><a href="#cb215-9" aria-hidden="true" tabindex="-1"></a>        max_length<span class="op">=</span>max_length,</span>
<span id="cb215-10"><a href="#cb215-10" aria-hidden="true" tabindex="-1"></a>        return_tensors<span class="op">=</span><span class="st">&quot;tf&quot;</span></span>
<span id="cb215-11"><a href="#cb215-11" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
<div class="output display_data">
<div class="sourceCode" id="cb216"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb216-1"><a href="#cb216-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">,</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;cb801a108fba4fb39545b05ad235bd2a&quot;</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb217"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb217-1"><a href="#cb217-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">,</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;3388de62d02c44aba6f70a9d8e12850c&quot;</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb218"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb218-1"><a href="#cb218-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">,</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;34de187ae4164221b816102329ad0471&quot;</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb219"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb219-1"><a href="#cb219-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">,</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;4ac5bd3128254aedb0c792f29a763765&quot;</span><span class="fu">}</span></span></code></pre></div>
</div>
</div>
<div class="cell code" data-execution_count="69" data-colab="{&quot;height&quot;:971,&quot;referenced_widgets&quot;:[&quot;18ad687895494cd0a9ed956840930efd&quot;,&quot;0ea2da81f093421398be2ae353990d6f&quot;,&quot;ad2c41413730485b95cc02cb3660b98f&quot;,&quot;49cefe5defc1408f8d963e2e3d01a337&quot;,&quot;7d0ea23c65564df59ca8b663557ed930&quot;,&quot;5ac3f4b832e44ce9ba1cf930627ae6d5&quot;,&quot;77c58bc04bc54333873b995040549bed&quot;,&quot;3a3cdb39492548368b7da38995c481ab&quot;,&quot;68c88c0e96b9484588841cbc47fd2162&quot;,&quot;0904773af4354cc9b3ade044ae4bc16a&quot;,&quot;2d2e959f4a574d92ae24ce6cb0dec20a&quot;],&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="FBSRHbH8vqxw" data-outputId="6c6386a1-1e46-48ca-f9e5-775814ab431d">
<div class="sourceCode" id="cb220"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb220-1"><a href="#cb220-1" aria-hidden="true" tabindex="-1"></a>name_4 <span class="op">=</span> <span class="st">&quot;Learning BERT (no distil) - trainable&quot;</span></span>
<span id="cb220-2"><a href="#cb220-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-3"><a href="#cb220-3" aria-hidden="true" tabindex="-1"></a>NUM_EPOCHS <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb220-4"><a href="#cb220-4" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb220-5"><a href="#cb220-5" aria-hidden="true" tabindex="-1"></a>LEARNING_RATE <span class="op">=</span> <span class="fl">5e-5</span></span>
<span id="cb220-6"><a href="#cb220-6" aria-hidden="true" tabindex="-1"></a>EARLY_STOP_PATIENCE <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb220-7"><a href="#cb220-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-8"><a href="#cb220-8" aria-hidden="true" tabindex="-1"></a>train_data, train_label, validation_data, validation_label, test_data, test_label <span class="op">=</span> get_raw_ds(df_transformers.copy())</span>
<span id="cb220-9"><a href="#cb220-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-10"><a href="#cb220-10" aria-hidden="true" tabindex="-1"></a>base <span class="op">=</span> TFAutoModel.from_pretrained(</span>
<span id="cb220-11"><a href="#cb220-11" aria-hidden="true" tabindex="-1"></a>    BERT_MODEL_NAME,</span>
<span id="cb220-12"><a href="#cb220-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb220-13"><a href="#cb220-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-14"><a href="#cb220-14" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tf.keras.layers.Input(shape<span class="op">=</span>(MAX_SEQUENCE_LENGTH,), dtype<span class="op">=</span>tf.int32, name<span class="op">=</span><span class="st">&#39;input_ids&#39;</span>)</span>
<span id="cb220-15"><a href="#cb220-15" aria-hidden="true" tabindex="-1"></a>attention_mask <span class="op">=</span> tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype<span class="op">=</span>tf.int32, name<span class="op">=</span><span class="st">&#39;attention_mask&#39;</span>)</span>
<span id="cb220-16"><a href="#cb220-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-17"><a href="#cb220-17" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> base([input_ids, attention_mask]).last_hidden_state[:, <span class="dv">0</span>, :]</span>
<span id="cb220-18"><a href="#cb220-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-19"><a href="#cb220-19" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> tf.keras.layers.Dropout(</span>
<span id="cb220-20"><a href="#cb220-20" aria-hidden="true" tabindex="-1"></a>    rate<span class="op">=</span><span class="fl">0.15</span>,</span>
<span id="cb220-21"><a href="#cb220-21" aria-hidden="true" tabindex="-1"></a>)(output)</span>
<span id="cb220-22"><a href="#cb220-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb220-23"><a href="#cb220-23" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> tf.keras.layers.Dense(</span>
<span id="cb220-24"><a href="#cb220-24" aria-hidden="true" tabindex="-1"></a>    units<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb220-25"><a href="#cb220-25" aria-hidden="true" tabindex="-1"></a>    activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>,</span>
<span id="cb220-26"><a href="#cb220-26" aria-hidden="true" tabindex="-1"></a>)(output)</span>
<span id="cb220-27"><a href="#cb220-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-28"><a href="#cb220-28" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> tf.keras.layers.BatchNormalization()(output)</span>
<span id="cb220-29"><a href="#cb220-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-30"><a href="#cb220-30" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> tf.keras.layers.Dense(</span>
<span id="cb220-31"><a href="#cb220-31" aria-hidden="true" tabindex="-1"></a>    units<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb220-32"><a href="#cb220-32" aria-hidden="true" tabindex="-1"></a>    activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>,</span>
<span id="cb220-33"><a href="#cb220-33" aria-hidden="true" tabindex="-1"></a>)(output)</span>
<span id="cb220-34"><a href="#cb220-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-35"><a href="#cb220-35" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> tf.keras.layers.BatchNormalization()(output)</span>
<span id="cb220-36"><a href="#cb220-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-37"><a href="#cb220-37" aria-hidden="true" tabindex="-1"></a>output_layer <span class="op">=</span> tf.keras.layers.Dense(</span>
<span id="cb220-38"><a href="#cb220-38" aria-hidden="true" tabindex="-1"></a>    units<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb220-39"><a href="#cb220-39" aria-hidden="true" tabindex="-1"></a>    activation<span class="op">=</span><span class="st">&#39;sigmoid&#39;</span></span>
<span id="cb220-40"><a href="#cb220-40" aria-hidden="true" tabindex="-1"></a>)(output)</span>
<span id="cb220-41"><a href="#cb220-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-42"><a href="#cb220-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-43"><a href="#cb220-43" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>[input_ids, attention_mask], outputs<span class="op">=</span>output_layer)</span>
<span id="cb220-44"><a href="#cb220-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-45"><a href="#cb220-45" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb220-46"><a href="#cb220-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-47"><a href="#cb220-47" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>tf.keras.optimizers.Adam(learning_rate<span class="op">=</span>LEARNING_RATE), loss<span class="op">=</span>keras.losses.BinaryCrossentropy(), metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb220-48"><a href="#cb220-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb220-49"><a href="#cb220-49" aria-hidden="true" tabindex="-1"></a>es <span class="op">=</span> keras.callbacks.EarlyStopping(monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, min_delta<span class="op">=</span><span class="dv">0</span>, patience<span class="op">=</span>EARLY_STOP_PATIENCE, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>0    29720
1     2242
Name: label, dtype: int64
</code></pre>
</div>
<div class="output display_data">
<div class="sourceCode" id="cb222"><pre class="sourceCode json"><code class="sourceCode json"><span id="cb222-1"><a href="#cb222-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">&quot;version_major&quot;</span><span class="fu">:</span><span class="dv">2</span><span class="fu">,</span><span class="dt">&quot;version_minor&quot;</span><span class="fu">:</span><span class="dv">0</span><span class="fu">,</span><span class="dt">&quot;model_id&quot;</span><span class="fu">:</span><span class="st">&quot;18ad687895494cd0a9ed956840930efd&quot;</span><span class="fu">}</span></span></code></pre></div>
</div>
<div class="output stream stderr">
<pre><code>Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: [&#39;nsp___cls&#39;, &#39;mlm___cls&#39;]
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Model: &quot;model_3&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_ids (InputLayer)         [(None, 40)]         0           []                               
                                                                                                  
 attention_mask (InputLayer)    [(None, 40)]         0           []                               
                                                                                                  
 tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   [&#39;input_ids[0][0]&#39;,              
                                thPoolingAndCrossAt               &#39;attention_mask[0][0]&#39;]         
                                tentions(last_hidde                                               
                                n_state=(None, 40,                                                
                                768),                                                             
                                 pooler_output=(Non                                               
                                e, 768),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 tf.__operators__.getitem_3 (Sl  (None, 768)         0           [&#39;tf_bert_model[0][0]&#39;]          
 icingOpLambda)                                                                                   
                                                                                                  
 dropout_97 (Dropout)           (None, 768)          0           [&#39;tf.__operators__.getitem_3[0][0
                                                                 ]&#39;]                              
                                                                                                  
 dense_9 (Dense)                (None, 64)           49216       [&#39;dropout_97[0][0]&#39;]             
                                                                                                  
 batch_normalization_6 (BatchNo  (None, 64)          256         [&#39;dense_9[0][0]&#39;]                
 rmalization)                                                                                     
                                                                                                  
 dense_10 (Dense)               (None, 64)           4160        [&#39;batch_normalization_6[0][0]&#39;]  
                                                                                                  
 batch_normalization_7 (BatchNo  (None, 64)          256         [&#39;dense_10[0][0]&#39;]               
 rmalization)                                                                                     
                                                                                                  
 dense_11 (Dense)               (None, 1)            65          [&#39;batch_normalization_7[0][0]&#39;]  
                                                                                                  
==================================================================================================
Total params: 109,536,193
Trainable params: 109,535,937
Non-trainable params: 256
__________________________________________________________________________________________________
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="70" id="po8enzz8vwl5">
<div class="sourceCode" id="cb225"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb225-1"><a href="#cb225-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> tf.data.Dataset.from_tensor_slices((</span>
<span id="cb225-2"><a href="#cb225-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">dict</span>(tokenize(<span class="bu">list</span>(train_data))),</span>
<span id="cb225-3"><a href="#cb225-3" aria-hidden="true" tabindex="-1"></a>    train_label</span>
<span id="cb225-4"><a href="#cb225-4" aria-hidden="true" tabindex="-1"></a>)).batch(BATCH_SIZE).prefetch(<span class="dv">1</span>)</span>
<span id="cb225-5"><a href="#cb225-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb225-6"><a href="#cb225-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb225-7"><a href="#cb225-7" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> tf.data.Dataset.from_tensor_slices((</span>
<span id="cb225-8"><a href="#cb225-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">dict</span>(tokenize(<span class="bu">list</span>(validation_data))),</span>
<span id="cb225-9"><a href="#cb225-9" aria-hidden="true" tabindex="-1"></a>    validation_label</span>
<span id="cb225-10"><a href="#cb225-10" aria-hidden="true" tabindex="-1"></a>)).batch(BATCH_SIZE).prefetch(<span class="dv">1</span>)</span>
<span id="cb225-11"><a href="#cb225-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb225-12"><a href="#cb225-12" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> tf.data.Dataset.from_tensor_slices((</span>
<span id="cb225-13"><a href="#cb225-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">dict</span>(tokenize(<span class="bu">list</span>(test_data))),</span>
<span id="cb225-14"><a href="#cb225-14" aria-hidden="true" tabindex="-1"></a>    test_label</span>
<span id="cb225-15"><a href="#cb225-15" aria-hidden="true" tabindex="-1"></a>)).batch(<span class="dv">1</span>).prefetch(<span class="dv">1</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="71" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="GVHibj-jvzzy" data-outputId="4bc84037-bf7d-4f97-9734-d316f0e91740">
<div class="sourceCode" id="cb226"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb226-1"><a href="#cb226-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(</span>
<span id="cb226-2"><a href="#cb226-2" aria-hidden="true" tabindex="-1"></a>    X,</span>
<span id="cb226-3"><a href="#cb226-3" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span>NUM_EPOCHS,</span>
<span id="cb226-4"><a href="#cb226-4" aria-hidden="true" tabindex="-1"></a>    validation_data<span class="op">=</span>V,</span>
<span id="cb226-5"><a href="#cb226-5" aria-hidden="true" tabindex="-1"></a>    callbacks<span class="op">=</span>[</span>
<span id="cb226-6"><a href="#cb226-6" aria-hidden="true" tabindex="-1"></a>      es</span>
<span id="cb226-7"><a href="#cb226-7" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb226-8"><a href="#cb226-8" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Epoch 1/10
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys [&#39;token_type_ids&#39;] which did not match any model input. They will be ignored by the model.
  inputs = self._flatten_to_reference_inputs(inputs)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>WARNING:tensorflow:Gradients do not exist for variables [&#39;tf_bert_model/bert/pooler/dense/kernel:0&#39;, &#39;tf_bert_model/bert/pooler/dense/bias:0&#39;] when minimizing the loss. If you&#39;re using `model.compile()`, did you forget to provide a `loss`argument?
WARNING:tensorflow:Gradients do not exist for variables [&#39;tf_bert_model/bert/pooler/dense/kernel:0&#39;, &#39;tf_bert_model/bert/pooler/dense/bias:0&#39;] when minimizing the loss. If you&#39;re using `model.compile()`, did you forget to provide a `loss`argument?
720/720 [==============================] - 421s 557ms/step - loss: 0.5883 - accuracy: 0.7623 - val_loss: 0.6690 - val_accuracy: 0.4638
Epoch 2/10
720/720 [==============================] - 401s 557ms/step - loss: 0.4207 - accuracy: 0.8958 - val_loss: 0.2533 - val_accuracy: 0.9574
Epoch 3/10
720/720 [==============================] - 396s 550ms/step - loss: 0.2931 - accuracy: 0.9535 - val_loss: 0.1966 - val_accuracy: 0.9609
Epoch 4/10
720/720 [==============================] - 399s 553ms/step - loss: 0.2074 - accuracy: 0.9704 - val_loss: 0.2821 - val_accuracy: 0.9609
Epoch 5/10
720/720 [==============================] - 399s 554ms/step - loss: 0.2689 - accuracy: 0.9393 - val_loss: 0.7152 - val_accuracy: 0.6433
Epoch 6/10
720/720 [==============================] - 398s 553ms/step - loss: 0.2444 - accuracy: 0.9452 - val_loss: 0.2655 - val_accuracy: 0.9300
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="72" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="Ex9b7miev2Q6" data-outputId="273e70b5-81c5-47d3-97b7-64bf3b073b8c">
<div class="sourceCode" id="cb230"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb230-1"><a href="#cb230-1" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> model.predict(T)</span>
<span id="cb230-2"><a href="#cb230-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb230-3"><a href="#cb230-3" aria-hidden="true" tabindex="-1"></a>predicted <span class="op">=</span> [<span class="dv">1</span> <span class="cf">if</span> x <span class="op">&gt;=</span> <span class="fl">0.5</span> <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> x <span class="kw">in</span> labels.ravel()]</span>
<span id="cb230-4"><a href="#cb230-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb230-5"><a href="#cb230-5" aria-hidden="true" tabindex="-1"></a>f1_1 <span class="op">=</span> get_y_true_y_pred(test_label, predicted)</span>
<span id="cb230-6"><a href="#cb230-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb230-7"><a href="#cb230-7" aria-hidden="true" tabindex="-1"></a>res_transformers[name_4] <span class="op">=</span> f1_1</span></code></pre></div>
<div class="output stream stderr">
<pre><code>/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys [&#39;token_type_ids&#39;] which did not match any model input. They will be ignored by the model.
  inputs = self._flatten_to_reference_inputs(inputs)
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="73" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="FEr_lHjE8sza" data-outputId="59fcc686-ed15-4929-f29f-9d2451d3981e">
<div class="sourceCode" id="cb232"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb232-1"><a href="#cb232-1" aria-hidden="true" tabindex="-1"></a>res_transformers</span></code></pre></div>
<div class="output execute_result" data-execution_count="73">
<pre><code>{&#39;Learning BERT (no distil) - trainable&#39;: 0.6083086053412462,
 &#39;Learning BERT - trainable&#39;: 0.6719636776390465}</code></pre>
</div>
</div>
<section id="bert-trainable-oversampled" class="cell markdown" id="G_PYyqlozNmH">
<h1>Bert trainable oversampled</h1>
</section>
<div class="cell code" data-execution_count="106" id="bN7j-hydzNAz">
<div class="sourceCode" id="cb234"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb234-1"><a href="#cb234-1" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(BERT_MODEL_NAME)</span>
<span id="cb234-2"><a href="#cb234-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb234-3"><a href="#cb234-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb234-4"><a href="#cb234-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize(sentences, max_length<span class="op">=</span>MAX_SEQUENCE_LENGTH, padding<span class="op">=</span><span class="st">&#39;max_length&#39;</span>):</span>
<span id="cb234-5"><a href="#cb234-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tokenizer(</span>
<span id="cb234-6"><a href="#cb234-6" aria-hidden="true" tabindex="-1"></a>        sentences,</span>
<span id="cb234-7"><a href="#cb234-7" aria-hidden="true" tabindex="-1"></a>        truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb234-8"><a href="#cb234-8" aria-hidden="true" tabindex="-1"></a>        padding<span class="op">=</span>padding,</span>
<span id="cb234-9"><a href="#cb234-9" aria-hidden="true" tabindex="-1"></a>        max_length<span class="op">=</span>max_length,</span>
<span id="cb234-10"><a href="#cb234-10" aria-hidden="true" tabindex="-1"></a>        return_tensors<span class="op">=</span><span class="st">&quot;tf&quot;</span></span>
<span id="cb234-11"><a href="#cb234-11" aria-hidden="true" tabindex="-1"></a>    )</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="107" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="0q1c0KIGzXEI" data-outputId="13f8e462-50fd-4887-94eb-32429f4cc425">
<div class="sourceCode" id="cb235"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb235-1"><a href="#cb235-1" aria-hidden="true" tabindex="-1"></a>name_5 <span class="op">=</span> <span class="st">&quot;Learning BERT (no distil) - trainable oversampled&quot;</span></span>
<span id="cb235-2"><a href="#cb235-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb235-3"><a href="#cb235-3" aria-hidden="true" tabindex="-1"></a>NUM_EPOCHS <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb235-4"><a href="#cb235-4" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb235-5"><a href="#cb235-5" aria-hidden="true" tabindex="-1"></a>LEARNING_RATE <span class="op">=</span> <span class="fl">5e-5</span></span>
<span id="cb235-6"><a href="#cb235-6" aria-hidden="true" tabindex="-1"></a>EARLY_STOP_PATIENCE <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb235-7"><a href="#cb235-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb235-8"><a href="#cb235-8" aria-hidden="true" tabindex="-1"></a>train_data, train_label, validation_data, validation_label, test_data, test_label <span class="op">=</span> get_balanced_ds(df_transformers.copy())</span>
<span id="cb235-9"><a href="#cb235-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb235-10"><a href="#cb235-10" aria-hidden="true" tabindex="-1"></a>base <span class="op">=</span> TFAutoModel.from_pretrained(</span>
<span id="cb235-11"><a href="#cb235-11" aria-hidden="true" tabindex="-1"></a>    BERT_MODEL_NAME,</span>
<span id="cb235-12"><a href="#cb235-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb235-13"><a href="#cb235-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb235-14"><a href="#cb235-14" aria-hidden="true" tabindex="-1"></a>input_ids <span class="op">=</span> tf.keras.layers.Input(shape<span class="op">=</span>(MAX_SEQUENCE_LENGTH,), dtype<span class="op">=</span>tf.int32, name<span class="op">=</span><span class="st">&#39;input_ids&#39;</span>)</span>
<span id="cb235-15"><a href="#cb235-15" aria-hidden="true" tabindex="-1"></a>attention_mask <span class="op">=</span> tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype<span class="op">=</span>tf.int32, name<span class="op">=</span><span class="st">&#39;attention_mask&#39;</span>)</span>
<span id="cb235-16"><a href="#cb235-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb235-17"><a href="#cb235-17" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> base([input_ids, attention_mask]).last_hidden_state[:, <span class="dv">0</span>, :]</span>
<span id="cb235-18"><a href="#cb235-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb235-19"><a href="#cb235-19" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> tf.keras.layers.Dropout(</span>
<span id="cb235-20"><a href="#cb235-20" aria-hidden="true" tabindex="-1"></a>    rate<span class="op">=</span><span class="fl">0.15</span>,</span>
<span id="cb235-21"><a href="#cb235-21" aria-hidden="true" tabindex="-1"></a>)(output)</span>
<span id="cb235-22"><a href="#cb235-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb235-23"><a href="#cb235-23" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> tf.keras.layers.Dense(</span>
<span id="cb235-24"><a href="#cb235-24" aria-hidden="true" tabindex="-1"></a>    units<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb235-25"><a href="#cb235-25" aria-hidden="true" tabindex="-1"></a>    activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>,</span>
<span id="cb235-26"><a href="#cb235-26" aria-hidden="true" tabindex="-1"></a>)(output)</span>
<span id="cb235-27"><a href="#cb235-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb235-28"><a href="#cb235-28" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> tf.keras.layers.BatchNormalization()(output)</span>
<span id="cb235-29"><a href="#cb235-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb235-30"><a href="#cb235-30" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> tf.keras.layers.Dense(</span>
<span id="cb235-31"><a href="#cb235-31" aria-hidden="true" tabindex="-1"></a>    units<span class="op">=</span><span class="dv">64</span>,</span>
<span id="cb235-32"><a href="#cb235-32" aria-hidden="true" tabindex="-1"></a>    activation<span class="op">=</span><span class="st">&#39;relu&#39;</span>,</span>
<span id="cb235-33"><a href="#cb235-33" aria-hidden="true" tabindex="-1"></a>)(output)</span>
<span id="cb235-34"><a href="#cb235-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb235-35"><a href="#cb235-35" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> tf.keras.layers.BatchNormalization()(output)</span>
<span id="cb235-36"><a href="#cb235-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb235-37"><a href="#cb235-37" aria-hidden="true" tabindex="-1"></a>output_layer <span class="op">=</span> tf.keras.layers.Dense(</span>
<span id="cb235-38"><a href="#cb235-38" aria-hidden="true" tabindex="-1"></a>    units<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb235-39"><a href="#cb235-39" aria-hidden="true" tabindex="-1"></a>    activation<span class="op">=</span><span class="st">&#39;sigmoid&#39;</span></span>
<span id="cb235-40"><a href="#cb235-40" aria-hidden="true" tabindex="-1"></a>)(output)</span>
<span id="cb235-41"><a href="#cb235-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb235-42"><a href="#cb235-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb235-43"><a href="#cb235-43" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> keras.Model(inputs<span class="op">=</span>[input_ids, attention_mask], outputs<span class="op">=</span>output_layer)</span>
<span id="cb235-44"><a href="#cb235-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb235-45"><a href="#cb235-45" aria-hidden="true" tabindex="-1"></a>model.summary()</span>
<span id="cb235-46"><a href="#cb235-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb235-47"><a href="#cb235-47" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">compile</span>(optimizer<span class="op">=</span>tf.keras.optimizers.Adam(learning_rate<span class="op">=</span>LEARNING_RATE), loss<span class="op">=</span>keras.losses.BinaryCrossentropy(), metrics<span class="op">=</span>[<span class="st">&#39;accuracy&#39;</span>])</span>
<span id="cb235-48"><a href="#cb235-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb235-49"><a href="#cb235-49" aria-hidden="true" tabindex="-1"></a>es <span class="op">=</span> keras.callbacks.EarlyStopping(monitor<span class="op">=</span><span class="st">&#39;val_loss&#39;</span>, min_delta<span class="op">=</span><span class="dv">0</span>, patience<span class="op">=</span>EARLY_STOP_PATIENCE, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: [&#39;nsp___cls&#39;, &#39;mlm___cls&#39;]
- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>Model: &quot;model_10&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_ids (InputLayer)         [(None, 40)]         0           []                               
                                                                                                  
 attention_mask (InputLayer)    [(None, 40)]         0           []                               
                                                                                                  
 tf_bert_model_7 (TFBertModel)  TFBaseModelOutputWi  109482240   [&#39;input_ids[0][0]&#39;,              
                                thPoolingAndCrossAt               &#39;attention_mask[0][0]&#39;]         
                                tentions(last_hidde                                               
                                n_state=(None, 40,                                                
                                768),                                                             
                                 pooler_output=(Non                                               
                                e, 768),                                                          
                                 past_key_values=No                                               
                                ne, hidden_states=N                                               
                                one, attentions=Non                                               
                                e, cross_attentions                                               
                                =None)                                                            
                                                                                                  
 tf.__operators__.getitem_10 (S  (None, 768)         0           [&#39;tf_bert_model_7[0][0]&#39;]        
 licingOpLambda)                                                                                  
                                                                                                  
 dropout_363 (Dropout)          (None, 768)          0           [&#39;tf.__operators__.getitem_10[0][
                                                                 0]&#39;]                             
                                                                                                  
 dense_30 (Dense)               (None, 64)           49216       [&#39;dropout_363[0][0]&#39;]            
                                                                                                  
 batch_normalization_20 (BatchN  (None, 64)          256         [&#39;dense_30[0][0]&#39;]               
 ormalization)                                                                                    
                                                                                                  
 dense_31 (Dense)               (None, 64)           4160        [&#39;batch_normalization_20[0][0]&#39;] 
                                                                                                  
 batch_normalization_21 (BatchN  (None, 64)          256         [&#39;dense_31[0][0]&#39;]               
 ormalization)                                                                                    
                                                                                                  
 dense_32 (Dense)               (None, 1)            65          [&#39;batch_normalization_21[0][0]&#39;] 
                                                                                                  
==================================================================================================
Total params: 109,536,193
Trainable params: 109,535,937
Non-trainable params: 256
__________________________________________________________________________________________________
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="108" id="Qsu3fFVczbCg">
<div class="sourceCode" id="cb238"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb238-1"><a href="#cb238-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> tf.data.Dataset.from_tensor_slices((</span>
<span id="cb238-2"><a href="#cb238-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">dict</span>(tokenize(<span class="bu">list</span>(train_data))),</span>
<span id="cb238-3"><a href="#cb238-3" aria-hidden="true" tabindex="-1"></a>    train_label</span>
<span id="cb238-4"><a href="#cb238-4" aria-hidden="true" tabindex="-1"></a>)).batch(BATCH_SIZE).prefetch(<span class="dv">1</span>)</span>
<span id="cb238-5"><a href="#cb238-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb238-6"><a href="#cb238-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb238-7"><a href="#cb238-7" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> tf.data.Dataset.from_tensor_slices((</span>
<span id="cb238-8"><a href="#cb238-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">dict</span>(tokenize(<span class="bu">list</span>(validation_data))),</span>
<span id="cb238-9"><a href="#cb238-9" aria-hidden="true" tabindex="-1"></a>    validation_label</span>
<span id="cb238-10"><a href="#cb238-10" aria-hidden="true" tabindex="-1"></a>)).batch(BATCH_SIZE).prefetch(<span class="dv">1</span>)</span>
<span id="cb238-11"><a href="#cb238-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb238-12"><a href="#cb238-12" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> tf.data.Dataset.from_tensor_slices((</span>
<span id="cb238-13"><a href="#cb238-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">dict</span>(tokenize(<span class="bu">list</span>(test_data))),</span>
<span id="cb238-14"><a href="#cb238-14" aria-hidden="true" tabindex="-1"></a>    test_label</span>
<span id="cb238-15"><a href="#cb238-15" aria-hidden="true" tabindex="-1"></a>)).batch(<span class="dv">1</span>).prefetch(<span class="dv">1</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="109" id="dXvWfqFNK_UK">
<div class="sourceCode" id="cb239"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb239-1"><a href="#cb239-1" aria-hidden="true" tabindex="-1"></a>combined_dataset <span class="op">=</span> X.concatenate(V)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="110" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="EznPrGjszclh" data-outputId="40bc11b7-0536-4264-d67f-617d90c72955">
<div class="sourceCode" id="cb240"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb240-1"><a href="#cb240-1" aria-hidden="true" tabindex="-1"></a>history <span class="op">=</span> model.fit(</span>
<span id="cb240-2"><a href="#cb240-2" aria-hidden="true" tabindex="-1"></a>    combined_dataset,</span>
<span id="cb240-3"><a href="#cb240-3" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">1</span></span>
<span id="cb240-4"><a href="#cb240-4" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<div class="output stream stderr">
<pre><code>/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys [&#39;token_type_ids&#39;] which did not match any model input. They will be ignored by the model.
  inputs = self._flatten_to_reference_inputs(inputs)
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>WARNING:tensorflow:Gradients do not exist for variables [&#39;tf_bert_model_7/bert/pooler/dense/kernel:0&#39;, &#39;tf_bert_model_7/bert/pooler/dense/bias:0&#39;] when minimizing the loss. If you&#39;re using `model.compile()`, did you forget to provide a `loss`argument?
WARNING:tensorflow:Gradients do not exist for variables [&#39;tf_bert_model_7/bert/pooler/dense/kernel:0&#39;, &#39;tf_bert_model_7/bert/pooler/dense/bias:0&#39;] when minimizing the loss. If you&#39;re using `model.compile()`, did you forget to provide a `loss`argument?
1473/1473 [==============================] - 832s 551ms/step - loss: 0.1376 - accuracy: 0.9482
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="111" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="56r3dTyJzfVP" data-outputId="541c6de6-6e33-45c9-d289-a2af204eca60">
<div class="sourceCode" id="cb243"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb243-1"><a href="#cb243-1" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> model.predict(T)</span>
<span id="cb243-2"><a href="#cb243-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb243-3"><a href="#cb243-3" aria-hidden="true" tabindex="-1"></a>predicted <span class="op">=</span> [<span class="dv">1</span> <span class="cf">if</span> x <span class="op">&gt;=</span> <span class="fl">0.5</span> <span class="cf">else</span> <span class="dv">0</span> <span class="cf">for</span> x <span class="kw">in</span> labels.ravel()]</span>
<span id="cb243-4"><a href="#cb243-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb243-5"><a href="#cb243-5" aria-hidden="true" tabindex="-1"></a>f1_1 <span class="op">=</span> get_y_true_y_pred(test_label, predicted)</span>
<span id="cb243-6"><a href="#cb243-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb243-7"><a href="#cb243-7" aria-hidden="true" tabindex="-1"></a>res_transformers[name_5] <span class="op">=</span> f1_1</span></code></pre></div>
<div class="output stream stderr">
<pre><code>/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys [&#39;token_type_ids&#39;] which did not match any model input. They will be ignored by the model.
  inputs = self._flatten_to_reference_inputs(inputs)
</code></pre>
</div>
</div>
<div class="cell code" data-execution_count="112" data-colab="{&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="N1ke6Q2_9TDL" data-outputId="f13e2d5b-348b-4dd1-ac2d-772b8c2137e6">
<div class="sourceCode" id="cb245"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb245-1"><a href="#cb245-1" aria-hidden="true" tabindex="-1"></a>res_transformers</span></code></pre></div>
<div class="output execute_result" data-execution_count="112">
<pre><code>{&#39;Learning BERT (no distil) - trainable&#39;: 0.6083086053412462,
 &#39;Learning BERT (no distil) - trainable oversampled&#39;: 0.7224880382775121,
 &#39;Learning BERT (no distil) - trainable oversampledtrain+valid&#39;: 0.7631901840490797,
 &#39;Learning BERT (no distil) - trainable oversampledtrain+valid+1&#39;: 0.7212765957446808,
 &#39;Learning BERT (no distil) - trainable oversampledtrain+valid+2&#39;: 0.7228070175438597,
 &#39;Learning BERT (no distil) - trainable oversampledtrain+valid+3&#39;: 0.723699421965318,
 &#39;Learning BERT - trainable&#39;: 0.6719636776390465}</code></pre>
</div>
</div>
<section id="results" class="cell markdown" id="gudzZpzfkQ1N">
<h1>Results</h1>
</section>
<div class="cell code" data-execution_count="115" id="y4aCZkF0kNjj">
<div class="sourceCode" id="cb247"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb247-1"><a href="#cb247-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame.from_dict(res_transformers, orient<span class="op">=</span><span class="st">&quot;index&quot;</span>)</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="116" id="8imuWFpbKoyR">
<div class="sourceCode" id="cb248"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb248-1"><a href="#cb248-1" aria-hidden="true" tabindex="-1"></a>df.columns<span class="op">=</span>[<span class="st">&#39;f1&#39;</span>]</span></code></pre></div>
</div>
<div class="cell code" data-execution_count="120" data-colab="{&quot;height&quot;:269,&quot;base_uri&quot;:&quot;https://localhost:8080/&quot;}" id="LyzRqwrZlndE" data-outputId="a8c40520-d9fd-467e-b213-0de95c29e5dd">
<div class="sourceCode" id="cb249"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb249-1"><a href="#cb249-1" aria-hidden="true" tabindex="-1"></a>df.sort_values(by<span class="op">=</span><span class="st">&#39;f1&#39;</span>, ascending<span class="op">=</span><span class="va">False</span>)</span></code></pre></div>
<div class="output execute_result" data-execution_count="120">

  <div id="df-d7701b2f-a8d2-450f-bb3b-6f07e90b3222">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>f1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Learning BERT (no distil) - trainable oversampledtrain+valid</th>
      <td>0.763190</td>
    </tr>
    <tr>
      <th>Learning BERT (no distil) - trainable oversampledtrain+valid+3</th>
      <td>0.723699</td>
    </tr>
    <tr>
      <th>Learning BERT (no distil) - trainable oversampledtrain+valid+2</th>
      <td>0.722807</td>
    </tr>
    <tr>
      <th>Learning BERT (no distil) - trainable oversampled</th>
      <td>0.722488</td>
    </tr>
    <tr>
      <th>Learning BERT (no distil) - trainable oversampledtrain+valid+1</th>
      <td>0.721277</td>
    </tr>
    <tr>
      <th>Learning BERT - trainable</th>
      <td>0.671964</td>
    </tr>
    <tr>
      <th>Learning BERT (no distil) - trainable</th>
      <td>0.608309</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-d7701b2f-a8d2-450f-bb3b-6f07e90b3222')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">
        
  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>
      
  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-d7701b2f-a8d2-450f-bb3b-6f07e90b3222 button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-d7701b2f-a8d2-450f-bb3b-6f07e90b3222');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>
  
</div>
</div>
</body>
</html>
